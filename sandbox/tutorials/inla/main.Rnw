\documentclass[a4paper,11pt]{report}
\usepackage[scale={0.8,0.9},centering,includeheadfoot]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{block}
\usepackage{my-showkeys}
\usepackage{amsmath}
\usepackage{verbatim}

\SweaveOpts{engine=R,prefix.string=Sweave/sweave,keep.source=TRUE,split=FALSE,eps=TRUE,pdf=TRUE}

\newcommand{\tv}{\texttt}
\def\Fig#1{Figure~\ref{#1}}
\def\Tab#1{Table~\ref{#1}}
\def\Thm#1{Theorem~\ref{#1}}
\def\Cor#1{Corrolary~\ref{#1}}
\def\Sec#1{Section~\ref{#1}}
\def\eref#1{(\ref{#1})}
\def\Eref#1{Eq.~(\ref{#1})}
\def\miscinfo#1#2{{\footnotesize\indent\textsc{#1: }\ignorespaces #2}}   
\def\R{\mathbb{R}}
\def\mm#1{\ensuremath{\boldsymbol{#1}}} % version: amsmath
\def\bs#1{\ensuremath{\boldsymbol{#1}}} % version: amsmath
\def\Var{\text{Var}}
\def\Corr{\text{Corr}}
\def\E{\text{E}}
\def\Prec{\text{Prec}}
\def\Trace{\text{Trace}}

\usepackage[colon,longnamesfirst]{natbib}

\begin{document}
\bibliographystyle{apalike}

\title{A tutorial for \tv{R-INLA}}
\author{H. Rue \& T.\ G.\ Martins\\
 Department of Mathematical Sciences\\
 Norwegian University of Science and Technology\\
 N-7491 Trondheim, Norway}
\maketitle

<<echo=FALSE>>=
options(width=60, prompt = " ", continue = " ")
@
<<echo=FALSE, cache=TRUE>>=
library(INLA)
library(xtable)
@
%%


\chapter{Introduction to \tv{R-INLA}}
\label{intro:0}

This chapter gives a brief introduction to \tv{R-INLA}; answering
questions like ``What is it?'', ``What models can I use it for?'',
``How do I install it?'', ``How can I get help?'' and so on.

\section{What it is?}
\label{intro:1}

\tv{R-INLA} is an implementation in \tv{R}\footnote{Strictly speaking,
    it is an \tv{R}-interface towards the \tv{inla-program} written in
    \tv{C}.} of the \tv{INLA} approach to do \emph{approximate
    Bayesian inference} for a class of \emph{latent Gaussian models}.

\emph{Latent Gaussian models} is an important class of statistical
models, and in our opinion, the most important and most widely used
model class in (applied) statistics. The details are given in
\Sec{intro:2}. This class includes most/many of dynamic linear models,
stochastic volatility models, generalized linear (mixed) models,
generalized additive (mixed) models, spline smoothing models,
semiparametric regression models, space-varying (semiparametric)
regression models, disease mapping models, Log-Gaussian Cox-processes,
model-based geostatistics and spatio-temporal models.

\emph{Approximate Bayesian inference} means that we do not target to
do exact inference, meaning that there is not button to tune on in
order to make the results converge towards the true ones. This
strategy is very different from the one using MCMC, say, where as the
number of samples tends to infinity the results get increasingly more
accurate. Also we do not aim to compute all possible marginals, but
only the marginals for the parameters in the model and linear
combinations thereof. These are in most cases the target for the
statistical analysis. It turns out, that in most cases, we can compute
approximated marginals for the model parameters both much much quicker
and more accurate, compared to common MCMC alternatives. The main
reasons to these achievements, are as follows.
\begin{enumerate}
\item We make use of the latent Gaussian field by using Laplace
    approximations.
\item We make use of the conditional independence properties of the
    latent Gaussian field by using numerical algorithms for sparse
    (symmetric and positive definite) matrices.
\item We make use of the low dimension of the hyperparameters by using
    \emph{integrated nested} Laplace approximations.
\end{enumerate}
For details about the \tv{INLA}-approach itself, then \cite{art375} is
a teaser, \cite{art451} is the main reference,
\cite{col30,col33,art517} are book-chapters about the \tv{INLA}
approach with applications, \cite{tech106} gives details of new
extensions in \tv{INLA}, and \cite{tech104} discuss some extensions to
latent near-Gaussian models.

\section{What models can it be used for?}
\label{intro:2}

The \tv{INLA} approach is targeted towards the class of \emph{latent
    Gaussian models} (LGM). At the first stage, there are the
conditional independent observations $\mm{y}$,
\begin{displaymath}
    y_{i} \mid \ldots \;\sim\; \pi(y_{i} \mid \eta_{i}, \mm{\theta}_{1})
\end{displaymath}
where $\eta_{i}$ is the linear predictor for $y_{i}$ and
$\mm{\theta}_{1}$ are hyperparameters in the likelihood model
$\pi(y_{i} \mid \eta_{i}, \mm{\theta}_{1})$. The likelihood model
could be Gaussian, Poisson or some other model. 

At the second stage, we have a model for the linear predictors
$\mm{\eta}$, as a linear combinations of various Gaussian effects,
\begin{displaymath}
    \eta_{i} = \sum_{j} \beta_{j} z_{ji} + \sum_{k}
    w_{ki}f_{k}(c_{ki}; \mm{\theta}_{2}).
\end{displaymath}
Here, $\mm{z}$'s are ``fixed effects'' with Gaussian coefficients
$\mm{\beta}$, $\mm{w}$ are fixed weights and $\mm{f}$ are some
Gaussian ``random effects''\footnote{We will use the terms ``fixed
    effects'' and ``random effects'' throughout this tutorial, even
    though that in a Bayesian context they are all ``random''.} The
terms $f_{k}(c_{ki}\; \mm{\theta}_{2})$ are general Gaussian ``random
effects'' models, for which the covariate $c_{ki}$ extract the
component of it that goes into the $i$th linear predictor. Again,
$\mm{\theta}_{2}$ are hyperparameters in these general Gaussian
models. An important assumption, is that the joint density for $\mm{x}
= (\mm{\eta}, \mm{\beta}, \mm{f})$ is Gaussian with density
\begin{displaymath}
    \pi(\mm{x} \mid \mm{\theta}_{2}) \propto |\mm{Q}(\mm{\theta}_{2})|^{1/2}
    \exp\left(
      -\frac{1}{2} \mm{x}^{T}\mm{Q}(\mm{\theta}_{2}) \mm{x} +
      b(\mm{\theta_{2}})^{T} \mm{x}\right)
\end{displaymath}
where $\mm{Q}$ is the precision matrix and $\mm{b}$ is the linear
term, both which can depend on $\mm{\theta}_{2}$.

As the third stage, we have the joint prior $\pi(\mm{\theta})$ for the hyperparameters
$\mm{\theta} = (\mm{\theta}_{1}, \mm{\theta}_{2})$.

The target for the Bayesian analysis is to compute the posterior
marginals for each $\theta_{i}$, $\eta_{i}$, $\beta_{i}$ and $f_{ki}$
given the observations $y$.
    
\section{How do I install it?}

The \tv{R-INLA} package is available from the web-site 
\begin{verbatim}
    www.r-inla.org
\end{verbatim}
which also contains all other available information.

Before installing \tv{R-INLA} you might want to install the
packages \tv{R-INLA} depends on, these are
\begin{itemize}
\item pixmap
\item sp
\end{itemize}
and optionally install suggested (not required) packages 
\begin{itemize}
\item numDeriv
\item Rgraphviz
\item graph
\item fields
\item rgl
\item mvtnorm
\item multicore
\end{itemize}
There are two ways to install \tv{R-INLA}.
\begin{enumerate}
\item Type the following command in \tv{R}
<<eval=FALSE>>=
source("http://www.math.ntnu.no/inla/givemeINLA.R")
@     
%%
which will download and install the package. 
\item You can also install the package manually. If you are using
    Linux or Mac, then
 download
\begin{verbatim}
http://www.math.ntnu.no/inla/binaries/INLA.tgz
\end{verbatim}
and install the package from within \tv{R} using
<<eval=FALSE>>=
install.packages("INLA.tgz", repos=NULL, type="source")
@ 
%%
If you are using Windows, then download
\begin{verbatim}
http://www.math.ntnu.no/inla/binaries/INLA.zip
\end{verbatim}
Start \tv{R} and select the \emph{Packages menu}, then \emph{Install
    package from local zip file}, then find and highlight the location
of the zip file and click on open.
\end{enumerate}

It is important to keep your package up-to-date. Here you have two
options. 
\begin{enumerate}
\item There is a more stable version that is updated once-in-a-while,
    and you can upgrade it using
<<eval=FALSE>>=
inla.upgrade()
@     
%%
\item There is a testing version that is updated frequently and you
    can upgrade to it, using
<<eval=FALSE>>=
inla.upgrade(testing=TRUE)
@ 
%%
and this version will contain the most recent version of the package.
The testing-version is assumed for all examples at the
\verb|www.r-inla.org| page. 
\end{enumerate}
If you have the testing-version installed, then you can ``downgrade''
to the most recent stable version doing \verb|inla.upgrade()| and
\verb|inla.upgrade(testing=TRUE)| will take you back to the more
recent testing-version. 

You can check which version you have installed using
<<>>=
inla.version()
@ 
%%
The ``build date'' shows when the package was compiled and build, the
``INLA hgid'' is the version and date of the \tv{R}-code and 
``INLA-program hgid'' is the version and date of the inla-program that
do all the calculations. The version and date for the inla-program is
often the same as for the \tv{R}-code, but it can be slightly older. 

If you are interested in recent changes in the package then
<<eval=FALSE>>=
inla.changelog()
@ 
%%
will place your browser at the list of the most recent changes. You can also visit
\begin{verbatim}
    inla.googlecode.com
\end{verbatim}
to view or download the complete source-code. 



\section{How do I use it?}

If you have installed the \tv{R-INLA} package then
<<eval=FALSE>>=
library(INLA)
@ 
%%
will load the library and you are ready to go. 

If you are familiar with the \verb|lm()| function in \tv{R}, then you
can use the \verb|inla()| function instead of the \verb|lm()| function
to do a Bayesian analysis. Here is a small example
<<ex1,cache=TRUE>>=
n = 100
x = rnorm(n)
y = 1 + x + rnorm(n, sd = 0.1)
formula = y ~ 1 + x
result = inla(formula, data = data.frame(y,x))
@
%%
The \verb|result|-object contains all the results from the Bayesian analysis.
You can get a summary with
<<>>=
summary(result)
@ 
%%
and posterior summary statistics for the fixed effects (the
\tv{Intercept} and the covariate \tv{x}) and the observational
precision (for the Gaussian observations) are shown. These summary
statistics are derived from the corresponding posterior marginals;
which can be plotted using
<<eval=FALSE>>=
plot(result)
@ 
%%
and the results are shown in~\Fig{fig:11}.
<<results=tex,echo=FALSE>>=  
figs = plot(result, single=TRUE, postscript=TRUE, prefix="Sweave/ex1/figure-")
cat("\\begin{figure}[tbp]\n")
cat("\n")
cat("\\centering\n")
for(i in 1:length(figs)) {
    cat("\\includegraphics[width=7cm,height=7cm,angle=-90]{", gsub("[.]eps$","",figs[i]), "}\n", sep="")  
}  
cat("\\caption{The posterior marginals for the fixed effects for the intercept and covariate \tv{x}, and the precision for the Gaussian observations.}\n")
cat("\\label{fig:11}\n")
cat("\\end{figure}\n")
@ 
%%


\section{How can I get help?}
\label{sec:gethelp}

There are two options to get help from the \verb|inla-developers| and
other users. You can either sign up and post your questions to the
\verb|R-inla discussion group|, see
\begin{verbatim}
http://www.r-inla.org/comments-1
\end{verbatim}
which is a public discussion group. If you want to questions
privately, you can email \verb|help@r-inla.org| or
\verb|hrue@r-inla.org|.



\section{How do I report an error or a new feature request?}

A request for a new feature can be posted to either the discussion
group or to \verb|help@r-inla.org|; see Section~\ref{sec:gethelp}.

An error is easiest reported to \verb|help@r-inla.org| or
\verb|hrue@r-inla.org|. However, please make sure that you run with
the latest version of the software, i.e.\ please do 
<<eval=FALSE>>=
inla.upgrade(testing=TRUE)
@ 
%%
and rerun your example. If the error is still there, please make us
aware of it.


In order to reproduce the error, we need to rerun your model/code
locally, hence we will need a copy of the R-code and datasets used to
generate the error. If the error occurs inside the \verb|inla()|-call,
then you may do as follows; rerun with
<<eval=FALSE>>=
inla( "<your stuff here>",  keep=TRUE, inla.call="")
@ 
%%
which will just generate the internal files, but no inference will be
done.  In the current (seen from within \verb|R|) directory/folder,
look for a new directory/folder named \verb|inla.model| and just send
us that directory/folder. If you have one of these from before, then
the new models are created as \verb|inla.model-1|,
\verb|inla.model-2|, etc. In this case, send us the most recent
one. With this option, your data stays almost secret, in the sense
that they are there, but in a form that makes it very hard to extract
any useful information about unless knowing how the model was
defined. However, it contains in most cases what we need to rerun the
model and check what is going on.


\chapter{Fixed effect models: Bayesian GLM}

This chapter discuss classical fixed effects models in a Bayesian
context, and is essentially a Bayesian version of \verb|lm()| and
\verb|glm()|\footnote{Due to implementation issues, large scale (pure)
    fixed effects models in \tv{R-INLA} may scale badly and run slower
    than their frequentisitic alternatives.}.

We will now define Bayesian generalized linear models (BGLMs) to
perform approximate fully Bayesian inference with INLA. These models
can be written using a hierarchical representation where the first
stage is formed by a likelihood function with conditional independence
properties given the ($n_d \times 1$) vector of linear predictors
$\bs{\eta}$ and the ($m \times 1$) vector of possible hyperparameters
$\bs{\theta}$
\begin{list}{\labelitemi}{\leftmargin=5em}
\item[\textbf{Stage 1.}] $\bs{y}|\bs{\eta},\bs{\theta} \sim
    p(\bs{y}|\bs{\eta},\bs{\theta}) = \prod _{i=1}^{n_d} p(y_i|g^{-1}
    (\eta _i), \bs{\theta})$,
\end{list}
where $\bs{y}$ is the ($n_d \times 1$) vector of data points and
$g^{-1}(\cdot)$ is the inverse of the link function that connects the
conditional mean of $y_i$ to the linear predictor $\eta _i$.  The
second stage is formed by the systematic part of the model and states
that the covariates are linearly related to the linear predictor. In
matrix notation,
\begin{list}{\labelitemi}{\leftmargin=5em}
\item[\textbf{Stage 2.}] $\bs{\eta} = \bs{X} \bs{\beta}, \quad
    \bs{\beta} \sim N(\bs{\mu}, \text{diag}(\bs{\tau}^{-1}))$,
\end{list}
where $\bs{X}$ is a ($n_d \times p$) design matrix which have the $p$
covariate vectors as its columns and $\bs{\beta}$ is a ($p \times 1$)
vector of regression coefficients. INLA attributes independent
Gaussian priors with mean $\mu _i$ and precision $\tau_i$ for each of
the regressor coefficients $\beta_i$, $i = 1,...,p$.  The hierarchical
model is then completed with an appropriate prior distribution for a
possible vector of hyperparameters $\bs{\theta}$
\begin{list}{\labelitemi}{\leftmargin=5em}
  \item[\textbf{Stage 3.}] $\bs{\theta} \sim \pi(\bs{\theta})$.
\end{list}

\section{Specifying the linear predictor}

As mentioned in Section \ref{bglm:sec:intro}, in generalized linear
models, each data point is connected, through a link function, to a
linear predictor. In matrix notation
\begin{equation}
    \bs{\eta} = \bs{X}\bs{\beta}.
    \label{bglm:eq:linear_pred}
\end{equation}

Similar to linear models in \tv{R} (like \tv{lm()} and \tv{glm()}),
the model is defined using a \emph{formula}; see
\verb|?formula|.\footnote{One exception is that the specification
    ``\texttt{y $\;\mbox{$\sim$}\;$ .}'' is not allowed.}
That
    is, the linear predictor can be specified using the formula
    notation
\begin{center}
    \texttt{formula = y \;\mbox{$\sim$}\; a + b + a:b + c*d}
\end{center}
where \texttt{y} is the response and \texttt{a}, \texttt{b},
\texttt{c} and \texttt{d} are covariates.  The meaning of the symbols
\texttt{+}, \texttt{:} and \texttt{*} used  are as follows.
\begin{description}
\item[``+''] combine main effects, as in \tv{a + b}
\item[``:''] for interaction terms, as in \tv{a:b}
\item[``*''] for both main effects and interaction terms, so \tv{c*d = c+d+c:d}
\end{description}
In mathematical language, the model would read
\begin{displaymath}
    \eta_{i} = \mu 
    + \beta_{a}a_{i}
    + \beta_{b}b_{i}
    + \beta_{ab}a_{i}b_{i}
    + \beta_{c}c_{i}
    + \beta_{d}d_{i}
    + \beta_{cd}c_{i}d_{i}, \qquad i=1, \ldots, n
\end{displaymath}
where $\eta_{i}$ is the linear predictor for $y_{i}$ with a not yet
defined likelihood.

Note that an intercept $\mu$ is automatically added to the model,
unless you say that you do not want it by adding term $-1$ or $0$:
\begin{center}
    \texttt{y \;\mbox{$\sim$}\; -1 + ....}\\
    \texttt{y \;\mbox{$\sim$}\; 0 + ....}
\end{center}
Similarly, adding term $1$ makes the intercept appear explicitly
\begin{center}
    \texttt{y \;\mbox{$\sim$}\; 1 + ....}
\end{center}
A small example will illustrate these features.
<<>>=
n = 10
x = rnorm(n)
z = rnorm(n)
y = 1 + x + z + x*z + rnorm(n)

formula = y ~ x*z
r = inla(formula, data = data.frame(y,x,z))
r$summary.fixed
@ 
%%
Note that we need to pass a \verb|data.frame| or a \verb|list|
containing the variables

It is also possible to use the design matrix $\bs{X}$ explicitly as in
\begin{center}
    \texttt{formula = y \;\mbox{$\sim$}\; X}
\end{center}
An alternative specification of the last example, is
<<>>=
formula = y ~ X
X = cbind(x=x, z=z, "x:z"=x*z)
r = inla(formula, data = list(y=y,X=X))
r$summary.fixed
@ 
%%
\tv{R-INLA} adds prefix ``X'' to the fixed effects names, to make the
connection to the design-matrix $X$ explicit.

\subsection{Prior for the fixed effects}

As mentioned in Section \ref{bglm:sec:intro}, INLA attributes
independent Gaussian priors for each fixed effect of the
model. The mean and precision of each of the priors can be controlled
using the argument \texttt{control.fixed} of the \texttt{inla}
function.  The argument \texttt{control.fixed} needs to be a list, and
the arguments relevant for the prior specification of the fixed
effects are:
\begin{itemize}
\item \texttt{mean.intercept}: prior mean for the intercept. Default value is 
    \Sexpr{inla.set.control.fixed.default()$mean.intercept}
\item \texttt{prec.intercept}: precision for the intercept. Default
    value is
    \Sexpr{inla.set.control.fixed.default()$prec.intercept}\footnote{Note
        that the this prior is improper!}
\item \texttt{mean}: prior mean for all fixed effects except the
    intercept.  Default is
    \Sexpr{inla.set.control.fixed.default()$mean}.  Alternatively, a
    named list with specific means where \texttt{name = default}
    applies to unmatched names. For example
    \begin{center}
        \texttt{control.fixed = list(mean = list(a = 1, b = 2, default = 0))} 
    \end{center}
    assign \texttt{mean = 1} to fixed effect \texttt{a}, \texttt{mean
        = 2} to effect \texttt{b} and \texttt{mean = 0} to all others.
\item \texttt{prec}: Similar to the \texttt{mean} argument above,
    \texttt{prec} set the precision for all fixed effects except the
    intercept.  Default is
    \Sexpr{inla.set.control.fixed.default()$prec}.  Alternatively, a
    named list with specific precisions where \texttt{name = default}
    applies to unmatched names.
\end{itemize}
As an example, assume we have a model with an intercept and four fixed
effects \tv{a}, \tv{b}, \tv{c} and \tv{d} and we want the intercept to
have mean $0$ and precision $10$, the fixed effect \tv{b} to have mean
$3$ and precision $5$ and the remaining to have mean $10$ and
precision $20$. Then the prior specification in this case would be
<<eval=FALSE,echo=TRUE>>=
control.fixed = list(mean.intercept = 0,
                     prec.intercept = 10,
                     mean = list(b = 3, default = 10),
                     prec = list(b = 5, default = 20))
inla(..., control.fixed = control.fixed)
@ 
%%

\section{Looking at the results}

The result from an \tv{inla()}-call returns an object with class
\tv{inla}
<<>>=
class(r)
@ 
%%
and with a lot of contents:
<<>>=
names(r)
@ 
%%
Not all of the contents are of interest for the typical user, but some
of the objects are, which we now will discuss. 

With a result-object you can get a summary using
<<>>=
summary(r)
@ 
%%
which shows the call itself, time usage (broken into preprocessing the
model and data, doing the computations, and reading the results back
and postprocess them), (statistical) summary for the fixed effects
(and random effects when they are included), summary for the
hyperparameters $\theta$, an estimate for the effective number of
parameters in the model, and number of observations pr effective
number of parameters, and the estimate for the (log-)marginal
likelihood.

\subsection{Posterior marginals}

The main task for the inference is to compute posterior
marginals. These marginals are stored in the object
\verb|marginals.fixed|
<<>>=
str(r$marginals.fixed)
@ 
%%
each element in the list contains a two-column matrix with the
density-values $(x_{i}, y_{i})$, so
<<>>=
head(r$marginals.fixed[["Xx"]])
@ 
%%
and we can plot this marginal
\begin{center} 
<<eval=TRUE,fig=TRUE>>=
marg = r$marginals.fixed[["Xx"]]
plot(marg)
@ 
%%
\end{center}
or make it more smooth
\begin{center} 
<<eval=TRUE,fig=TRUE>>=
marg = r$marginals.fixed[["Xx"]]
plot(inla.smarginal(marg), type="l")
@ 
%%
\end{center}
where \verb|inla.smarginal| interpolates the points using splines (in
log-scale); see \verb|?inla.smarginal|.

The posterior marginals for the hyperparameters $\theta$, which is
this simple regression example is the precision for the Gaussian
observations, is found similarly as
\verb|marginals.hyper|
<<>>=
str(r$marginals.hyper)
@ 
%%

From a marginal, we can also compute expectations, like the mean
<<eval=TRUE>>=
inla.emarginal(function(x) x, marg)
@ 
%%
or the variance
<<eval=TRUE>>=
e = inla.emarginal(function(x) c(x,x^2), marg)
print(e[2] - e[1]^2)
@ 
%%
or quantiles
<<>>=
inla.qmarginal(0.5, marg)
@ 
%%
or produce samples
<<>>=
inla.rmarginal(5, marg)
@ 
%%
or compute marginals for a transformed variable
<<fig=TRUE,echo=TRUE>>=
exp.marg = inla.tmarginal(function(x) exp(x), marg)
plot(inla.smarginal(exp.marg), type="l")
@ 
%%

\subsection{Posterior summary}

If only summary-statistics of the posterior marginals are required, then
<<>>=
str(r$summary.fixed)
str(r$summary.hyper)
@ 
%%
provide summaries including the expectation, standard deviations and
quantiles. For example
<<>>=
r$summary.fixed[, "mean"]
r$summary.hyper[, "mean"]
@ 
%%
provide the posterior means. 

Note that the posterior means reported may deviate, a little, from
those computed from the marginal itself, like
<<>>=
e.1 = r$summary.fixed["Xx", "mean"]
e.2 = inla.emarginal(function(x) x, r$marginals.fixed[["Xx"]])
print(e.1 - e.2)
@ 
%%
as those results reported in \verb|r$summary.fixed| are computed with a
different (and higher) resolution of the posterior marginal.



\chapter{Likelihood models}



\section{Available models}

\section{Setting priors for hyperparameters}

\section{\tv{NA} is the response}



\chapter{Random effects models}

\section{Available models}

\section{General options}



\chapter{Options for the \tv{inla}-call}

\section{General options}
\section{Specific options: \tv{control.xxx}}



\chapter{Extensions and other cool features}

\section{Using more than one type of likelihood models}

\section{Feature: \tv{copy}}

\section{Feature: \tv{replicate}}

\section{Feature: \tv{group}}

\section{Feature: \tv{A-matrix}}

\section{Feature: \tv{linear combinations}}

\section{Defining your own prior distribution for hyperparameters}

Besides the list of available prior functions for hyperparameters, compare \ref{sec:}, it is even possible to define user-specific prior distributions. This might be need on one side to incorporate expert-motivated prior distributions or to use prior distributions that are non-standard.

Hence, there are three ways to specify prior distributions for hyperparameter in INLA:
\begin{enumerate}
	\item Use an available prior distribution.
	\item Define your own prior distribution function using R-like (not equal) syntax as expression.
	\item Assign a table of x and corresponding y values which represent your prior distribution.
\end{enumerate}

In the following we will provide more details regarding 2.) and 3.), afterward we will present an example illustrating (and comparing) the three different possibilities using the log-gamma distribution.

\subsection{“Expression”: a do-it-yourself prior}

A user can specify a new prior distribution by defining an expression for the log-density of any (univariate) prior $\log\pi(\theta)$, as a function of the corresponding $\theta$. Here, it is important to be aware that $\theta$ is defined on the internal scale.

The expression is evaluated using the
\texttt{muparser}-library\footnote{See
    \texttt{http://muparser.sourceforge.net/} for more documentation},
with some local configuration changes to make it more
``\texttt{R}''-like in style. The format is
\begin{quote}
    \texttt{expression: <statement>; <statement>; ...; return(<value>)}
\end{quote}
where ``\texttt{<statement>}'' is any regular statement (more below)
and value returned, ``\texttt{<value>}'' is the value for the
log-density of the prior, evaluated at the current value for $\theta$.

Here, is an example defining the log-gamma distribution:
\begin{verbatim}
prior.expression = "expression:
            a = 1;
            b = 0.1;
            precision = exp(log_precision);
            logdens = log(b^a) - lgamma(a)
                      + (a-1)*log_precision - b*precision;
            log_jacobian = log_precision;
            return(logdens + log_jacobian);"
\end{verbatim}


Some syntax specific notes include:
\begin{enumerate}
\item \verb|return (x)| (with a space before ``(.)'') is NOT allowed,
    it must be \verb|return(x)|.
\item A ``;'' is needed to terminate each expression.
\item You can use ``\verb|_|'' in variable-names, like
    \verb|log_precision = <whatever>|; see the following example.
\end{enumerate}

Known function that can be used within the \texttt{expression} statement are:
\begin{itemize}
\item common math functions, such as ``$\exp(\cdot)$'', ``$\sin(\cdot)$'', \ldots.
\item \verb|gamma(x)| is the Gamma-function and \verb|lgamma(x)| is
    its $\log$ (see \verb|?gamma| in \verb|R|).
\item \verb|pi| is $\pi$
\item $x^y$ is expressed as either \verb|x^y| or \verb|pow(x;y)|
\end{itemize}

\subsection{“Table”: provide a suitable set of support points}

Instead of defining a prior distribution function, it is possible to provide a table of suitable support values $x$ (internal scale) and the corresponding log-density values $y$. INLA fits a spline through the provided points and continues with this in the succeeding computations. Note, the transformation into a functional form is not required.

The input-format for the table is a string, which starts with \texttt{table: } and is then followed by a block of $x$-values and  a block of the corresponding $y$-values, which represent the values of the log-density evaluated on $x$. Thus
\begin{quote}
    ``\texttt{table: x\_1 ... x\_n y\_1 ... y\_n}''
\end{quote}

\subsection{Example: The log-gamma distribution}

We illustrate all three different ways of defining a prior distribution, here for residual precision, of a normal likelihood. To show that the three definitions lead to the same result we inspect the log-marginal likelihood.

<<echo=FALSE, eval=TRUE>>=
set.seed(1352)
@
<<echo=TRUE, eval=TRUE, keep.source=T>>=
## the loggamma-prior 
prior.function = function(log_precision) {
    a = 1;
    b = 0.1;
    precision = exp(log_precision);
    logdens = log(b^a) - lgamma(a) + (a-1)*log_precision - b*precision;
    log_jacobian = log_precision;
    return(logdens + log_jacobian)
}

## implementing the loggamma-prior using "expression:"
prior.expression = "expression:
            a = 1;
            b = 0.1;
            precision = exp(log_precision);
            logdens = log(b^a) - lgamma(a)
                      + (a-1)*log_precision - b*precision;
            log_jacobian = log_precision;
            return(logdens + log_jacobian);"

## use suitable support points x
lprec = seq(-10, 10, len=100)
## link the x and corresponding y values into a string which begins with "table:""
prior.table = inla.paste(c("table:", cbind(lprec, prior.function(lprec))))

# simulate some data
n = 50
y = rnorm(n)

## 1. use the built-in loggamma prior
r1 = inla(y~1,data = data.frame(y),
        control.family = list(hyper =
            list(prec = list(prior = "loggamma", param = c(1, 0.1)))))

## 2. use the definition using expression
r2 = inla(y~1,
        data = data.frame(y),
        control.family = list(hyper =
            list(prec = list(prior = prior.expression))))

## 3. use a table of x and y values representing the loggamma prior
r3 = inla(y~1,
        data = data.frame(y),
        verbose=T,
        control.family = list(hyper =
            list(prec = list(prior = prior.table))))

## and we verify that we get the same result using the log-marginal likelihood
c(r1$mlik[1], r2$mlik[1], r3$mlik[1])
@


\chapter{Troubleshooting and FAQs}





\chapter{}



%%
\clearpage




\chapter{Bayesian generalized linear model}

\section{Introduction}\label{bglm:sec:intro}

In this chapter, we cover how to define Bayesian generalized linear
models (BGLMs) to perform approximate fully Bayesian inference with
INLA. These models can be written using a hierarchical representation
where the first stage is formed by a likelihood function with
conditional independence properties given the ($n_d \times 1$) vector
of linear predictors $\bs{\eta}$ and the ($m \times 1$) vector of
possible hyperparameters $\bs{\theta}$
\begin{list}{\labelitemi}{\leftmargin=5em}
\item[\textbf{Stage 1.}] $\bs{y}|\bs{\eta},\bs{\theta} \sim
    p(\bs{y}|\bs{\eta},\bs{\theta}) = \prod _{i=1}^{n_d} p(y_i|g^{-1}
    (\eta _i), \bs{\theta})$,
\end{list}
where $\bs{y}$ is the ($n_d \times 1$) vector of data points and
$g^{-1}(\cdot)$ is the inverse of the link function that connects the
conditional mean of $y_i$ to the linear predictor $\eta _i$.  The
second stage is formed by the systematic part of the model and states
that the covariates are linearly related to the linear predictor. In
matrix notation,
\begin{list}{\labelitemi}{\leftmargin=5em}
\item[\textbf{Stage 2.}] $\bs{\eta} = \bs{X} \bs{\beta}, \quad
    \bs{\beta} \sim N(\bs{\mu}, diag(\bs{\tau}^{-1}))$,
\end{list}
where $\bs{X}$ is a ($n_d \times p$) design matrix which have the $p$
covariate vectors as its columns and $\bs{\beta}$ is a ($p \times 1$)
vector of regression coefficients. INLA attributes independent
Gaussian priors with mean $\mu _i$ and precision $\tau_i$ for each of
the regressor coefficients $\beta_i$, $i = 1,...,p$.  The hierarchical
model is then completed with an appropriate prior distribution for a
possible vector of hyperparameters $\bs{\theta}$
\begin{list}{\labelitemi}{\leftmargin=5em}
  \item[\textbf{Stage 3.}] $\bs{\theta} \sim \pi(\bs{\theta})$.
\end{list}

\section{Latent field}

\subsection{Specifying the linear predictor}

As mentioned in Section \ref{bglm:sec:intro}, in generalized linear
models, each data point is connected, through a link function, to a
linear predictor. In matrix notation
\begin{equation}
    \bs{\eta} = \bs{X}\bs{\beta}.
    \label{bglm:eq:linear_pred}
\end{equation}

The specification of the linear predictor (\ref{bglm:eq:linear_pred})
in INLA follows the same pattern found in \texttt{lm} and \texttt{glm}
functions available in \texttt{R}.  That is, the linear predictor can
be specified using the standard
\begin{equation}
    \texttt{formula = y ~ a + b + a:b + c*d}
    \label{bglm:eq:formulaspec}
\end{equation}
where \texttt{y} is the response and \texttt{a}, \texttt{b},
\texttt{c} and \texttt{d} are covariates.  Table
\ref{bglm:tab:lmsymbols} display the meaning of the symbols
\texttt{+}, \texttt{:} and \texttt{*} used in
(\ref{bglm:eq:formulaspec}).

\begin{table}[ht!]
    \centering
    \begin{tabular}{|c|l|}
        \hline
        + & to combine main effects, as in a + b \\
        \hline
        : & for interaction terms, as in a:b; \\
        \hline
        * & for both main effects and interaction terms, so c*d = c+d+c:d \\
        \hline
    \end{tabular}
    \caption{Meaning of the symbols used in the formula for specification of fixed effects.}
    \label{bglm:tab:lmsymbols}
\end{table}

Instead of using the formula specification
(\ref{bglm:eq:formulaspec}), it is also possible to use the design
matrix $\bs{X}$ explicitly as in
\begin{equation}
    \texttt{formula = y ~ X}
    \label{bglm:eq:designspec}
\end{equation}
where in this case the design matrix $\bs{X}$ is used as input for the 
\texttt{inla} function together with the response vector $\bs{y}$ in 
a list object of the form
\begin{center}
    \texttt{inla(..., data = list(y=y, X=X), ...)}
\end{center}

More details about the two forms (\ref{bglm:eq:formulaspec}) and
(\ref{bglm:eq:designspec}) of specifying fixed effects in INLA can be
found on the examples of Section \ref{bglm:sec:examples}.

\subsection{Prior for the fixed effects}

As mentioned in Section \ref{bglm:sec:intro}, INLA attributes
independent Gaussian priors for each one of the fixed effects of the
model. The mean and precision of each of the priors can be controlled
using the argument \texttt{control.fixed} of the \texttt{inla}
function.  The argument \texttt{control.fixed} needs to be a list, and
the arguments relevant for the prior specification of the fixed
effects are:
\begin{itemize}
\item \texttt{mean.intercept}: prior mean for the intercept. Default value is 0.
\item \texttt{prec.intercept}: precision for the intercept. Default value is 0.
\item \texttt{mean}: prior mean for all fixed effects except the intercept.
    Alternatively, a named list with specific means where \texttt{name = default} 
    applies to unmatched names. For example 
\texttt{control.fixed = list(mean = list(a = 1, b = 2, default = 0))} assign
\texttt{mean = 1} to fixed effect \texttt{a}, \texttt{mean = 2} to effect 
\texttt{b} and \texttt{mean = 0} to all others.
\item \texttt{prec}: Similar to the \texttt{mean} argument above, \texttt{prec} set the 
    precision for all fixed effects except the intercept. Alternatively, a named list with 
    specific precisions where \texttt{name = default} applies to unmatched names.
\end{itemize}

As an example, assume we have a model with an intercept and four fixed
effects \tv{a}, \tv{b}, \tv{c} and \tv{d} and we want the intercept to
have mean $0$ and precision $10$, the fixed effect \tv{b} to have mean
$3$ and precision $5$ and the remaining to have mean $10$ and
precision $20$. Then the prior specification in this case would be

\begin{verbatim}
control.fixed = list(mean.intercept = 0,
                     prec.intercept = 10,
                     mean = list(b = 3, default = 10),
                     prec = list(b = 5, default = 20))

result = inla(..., control.fixed = control.fixed, ...)
\end{verbatim}

\section{Likelihood function}

For BGLMs, the likelihood function belong to the exponential family,
as for example the Gaussian and Poisson case. When defining a
likelihood function for use with INLA, some aspects deserves
attention.
\begin{enumerate}
\item \textbf{Parameterization}: For example, in the Gaussian case, the
    parameterization used by INLA is mean $\mu$, precision $\tau$, and
    a possible (known) scale parameter $s$ for each observation, so
    that for data point $y_i$, we have a mean $\mu_i$ and variance
    $\sigma_i = \frac{1}{s_i\tau}$

\item \textbf{Link function}: Check how the linear predictor is
    connected to a specific parameter of the likelihood function. In
    the Gaussian case, the linear predictor $\eta$ is connected to the
    mean $\mu$ through an identity link function, while for the
    Poisson case, the linear predictor is connected to the mean
    $\lambda$ through $\eta = \log (\lambda)$.

\item \label{bglm:enum:hyper} \textbf{Hyperparameters}: Every unknown
    likelihood parameter that is not connected to the linear predictor
    is considered to be a hyperparameter. For the Gaussian likelihood,
    the precision parameter $\tau$ is considered a
    hyperparameter. Every hyperparameter has an internal
    parameterization. For example, the internal parameterization of the
    precision parameter $\tau$ in the Gaussian case is $\theta = \log
    (\tau)$, and the prior specification is done using the
    $\theta$-parameterization.

\item \textbf{Prior for hyperparameters}: As mentioned on item
    \ref{bglm:enum:hyper}, the priors for the hyperparameters are
    defined using the internal parameterization. For example, in the
    Gaussian case the prior is defined for $\theta$, where $\theta =
    \log (\tau)$.

    The prior specification for the hyperparameters of the likelihood
    function should be defined using the \texttt{control.data}
    argument of the \tv{inla} function, using the following format

    \begin{center}
        \tv{inla(..., control.data = list(hyper = hyper.lik), ...)} 
    \end{center}
    
    where \texttt{hyper.lik} is a list with one component for each
    hyperparameter in the likelihood, and each component on this list
    should be a list with information about the prior distribution for
    that specific hyperparameter.
 
    For the Gaussian case, we would have:
    
\begin{verbatim}
  hyper.lik = list(prec = prec) # There is only one
                                # hyperparameter

  prec = list(prior = "loggamma", # name of the prior
              param = c(2,0.01),  # parameters of the prior
              initial = log(200)) # initial value for 
                                  # optmization algorithm.
\end{verbatim}
    
    This might seem a little complicated now but will become clearer
    when we go through several examples later in Section
    \ref{bglm:sec:examples}.
    
\end{enumerate}

\section{Goodness of fit and predictive measures}

\section{\tv{inla()} function and the resulting object}

Up to this point we have learned how to include fixed effects in the
linear predictor, how to chose the family of the likelihood function,
and how to set up priors for the fixed effects and the hyperparameters
in the model. Now we just need to join all this together and use it as
arguments for the \tv{inla()} function:

\begin{verbatim}
## calling the inla function
result = inla(# formula for the linear predictor
              formula,
              # prior for the fixed effects
              control.fixed,
              # dataset
              data,
              # family distribution
              family,
              # prior for hyperparameters in the likelihood function
              control.data)
\end{verbatim}

After you have run your model with INLA using the above function,
the resulting object will be stored in the \tv{result} variable. This
will be a S3 object with several attributes and we will now highlight
the most important ones.

With \tv{result\$summary.fixed} one can access the summary matrix for
the fixed effects which contains by default the mean, standard
deviation, and the $(0.025, 0.5, 0.975)$ quantiles. Similarly, we use
\tv{result\$summary.hyper} to obtain the summary matrix for the
hyperparameters in the model.

In some applications we need to go beyond point estimates and have
access to the posterior distributions of the fixed effects and
hyperparameters.  We can access those with
\tv{result\$marginals.fixed} and \tv{result\$marginals.hyper},
respectively. Those objects are a named list with one element for each
parameter, and each of these elements are a matrix with two columns
where the first column hold the x-axis and the second column hold the
y-axis of the marginal posterior distribution. As an example, if we
fit a model where the linear predictor is defined by
\begin{center}
\tv{formula = y ~ 1 + a + b}
\end{center}
we can expect something similar to the following structure for 
\tv{result\$marginals.fixed}:

\small{
\begin{verbatim}
$`(Intercept)`
                x            y
 [1,] -35.2578230 1.672335e-08
 [2,] -31.2495132 6.823312e-08
         ...         ...
[80,] 108.5801057 2.072500e-08
[81,] 112.7888309 4.417685e-09

$a
                  x            y
 [1,] -0.7176915644 1.999485e-06
 [2,] -0.6841669015 8.158126e-06
          ...          ...
[80,]  0.4853387317 2.477937e-06
[81,]  0.5205396278 5.281901e-07

$b
                 x            y
 [1,] -0.688182689 2.624530e-06
 [2,] -0.662642127 1.070838e-05
           ...         ...
[80,]  0.228338654 3.252553e-06
[81,]  0.255156244 6.933052e-07
\end{verbatim}
}
\noindent That is, in this example, \tv{result\$marginals.fixed}
returns a named list with elements \tv{(Intercept)}, \tv{a} and
\tv{b}, each of which is a matrix with two columns, as expected.


\section{Examples}\label{bglm:sec:examples}

\subsection{Carbohydrate data - Normal linear model}

\subsubsection*{Example description}

The data displayed in table \ref{tab:bayesglm:carbdata} was taken from
\cite{dobson1983introduction}.  The data show responses, percentages
of total calories obtained from complex carbohydrates, for twenty male
insulin-dependent diabetics who had been on a high-carbohydrate diet
for six months. Compliance with the regime was thought to be related
to age (in years), body weight (relative to 'ideal' weight for height)
and other components of the diet, such as the percentage of calories
as protein. These other variables are treated as explanatory
variables.

\begin{table}[ht!]
    \centering
    \begin{tabular}{c|c|c|c}
        \hline
        Carbohydrate & Age & Weight & Protein \\
        \hline
        33 & 33 & 100 & 14 \\
        40 & 47 & 92  & 15 \\
        37 & 49 & 135 & 18 \\
        27 & 35 & 144 & 12 \\
        30 & 46 & 140 & 15 \\
        43 & 52 & 101 & 15 \\ 
        34 & 62 & 95  & 14 \\
        48 & 23 & 101 & 17 \\ 
        30 & 32 & 98  & 15 \\
        38 & 42 & 105 & 14 \\
        50 & 31 & 108 & 17 \\
        51 & 61 & 85  & 19 \\
        30 & 63 & 130 & 19 \\
        36 & 40 & 127 & 20 \\
        41 & 50 & 109 & 15 \\
        42 & 64 & 107 & 16 \\
        46 & 56 & 117 & 18 \\
        24 & 61 & 100 & 13 \\
        35 & 48 & 118 & 18 \\
        37 & 28 & 102 & 14 \\
        \hline
    \end{tabular}
    \caption{Carbohydrate, age, relative weight and protein for twenty 
        male insulin-dependent diabetics; for units, see text.}
    \label{tab:bayesglm:carbdata}
\end{table}

\subsubsection*{Model definition}

A possible model to analyze this data is

\begin{enumerate}
\item $y_i|\eta_i, \tau \sim N(\eta_i, \tau^{-1})$, $i=1,...,20$. 
\item $\eta_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3}$
\item $\tau \sim Gamma(2, 0.01)$, $p(\beta_j) \propto 1$, $j=0,1,2,3$
\end{enumerate}
in which carbohydrate $y$ is linearly related to age $x_1$, relative weight $x_2$
and protein $x_3$. 


\subsubsection*{INLA code}


{\small\bibliography{thiago,hrue}}

\end{document}


There is a dedicated web-site for \tv{R-INLA}
\begin{verbatim}
    http://www.r-inla.org
\end{verbatim}
and for the source-code itself
\begin{verbatim}
    http://inla.googlecode.com
\end{verbatim}
