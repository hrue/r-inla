\documentclass[a4paper,11pt]{report}
\usepackage[scale={0.8,0.9},centering,includeheadfoot]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{block}
\usepackage{my-showkeys}
\usepackage{amsmath}
\usepackage{verbatim}



\newcommand{\tv}{\texttt}
\def\Fig#1{Figure~\ref{#1}}
\def\Tab#1{Table~\ref{#1}}
\def\Thm#1{Theorem~\ref{#1}}
\def\Cor#1{Corrolary~\ref{#1}}
\def\Sec#1{Section~\ref{#1}}
\def\eref#1{(\ref{#1})}
\def\Eref#1{Eq.~(\ref{#1})}
\def\miscinfo#1#2{{\footnotesize\indent\textsc{#1: }\ignorespaces #2}}   
\def\R{\mathbb{R}}
\def\mm#1{\ensuremath{\boldsymbol{#1}}} % version: amsmath
\def\bs#1{\ensuremath{\boldsymbol{#1}}} % version: amsmath
\def\Var{\text{Var}}
\def\Corr{\text{Corr}}
\def\E{\text{E}}
\def\Prec{\text{Prec}}
\def\Trace{\text{Trace}}

\usepackage[colon,longnamesfirst]{natbib}

\usepackage{Sweave}
\begin{document}
\bibliographystyle{apalike}

\title{A tutorial for \tv{R-INLA}}
\author{H. Rue \& T.\ G.\ Martins\\
 Department of Mathematical Sciences\\
 Norwegian University of Science and Technology\\
 N-7491 Trondheim, Norway}
\maketitle




\chapter{Introduction to \tv{R-INLA}}
\label{intro:0}

\section{What it is?}
\label{intro:1}

\tv{R-INLA} is an implementation in \tv{R}\footnote{Strictly speaking,
    it is an \tv{R}-interface towards the \tv{inla-program} written in
    \tv{C}.} of the \tv{INLA} approach to do \emph{approximate
    Bayesian inference} for a class of \emph{latent Gaussian models}.

\emph{Latent Gaussian models} is an important class of statistical
models, and in our opinion, the most important and most widely used
model class in (applied) statistics. The details are given in
\Sec{intro:2}. This class include most/many of dynamic linear models,
stochastic volatility models, generalised linear (mixed) models,
generalised additive (mixed) models, spline smoothing models,
semiparametric regression models, space-varying (semiparametric)
regression models, disease mapping models, Log-Gaussian Cox-processes,
model-based geostatistics and spatio-temporal models.

\emph{Approximate Bayesian inference} means that we do not target to
do exact inference, meaning that there is not button to tune on in
order to make the results converge towards the true ones. This
strategy is very different from the one using MCMC, say, where as the
number of samples tends to infinity the results gets increasingly more
accurate. Also we do not aim to compute all possible marginals, but
only the marginals for the parameters in the model and linear
combinations thereof. These are in most cases the target for the
statistical analysis. It turns out, that in most cases, we can compute
approximated marginals for the model parameters both much much quicker
and more accurate, compared to common MCMC alternatives. The main
reasons to these achivements, are as follows.
\begin{enumerate}
\item We make use of the latent Gaussian field by using Laplace
    approximations.
\item We make use of the conditional independence properties of the
    latent Gaussian field by using numerical algorithms for sparse
    (symmetric and positive definite) matrices.
\item We make use of the low dimension of the hyperparameters by using
    \emph{integrated nested} Laplace approximations.
\end{enumerate}
For details about the \tv{INLA}-approach itself, then \cite{art375} is
a teaser, \cite{art451} is the main reference,
\cite{col30,col33,art517} are book-chapters about the \tv{INLA}
approach with applications, \cite{tech106} gives details of new
extentions in \tv{INLA}, and \cite{tech104} discuss some extentions to
latent near-Gaussian models.

\section{What models can it be used for?}
\label{intro:2}

The \tv{INLA} approach is targeted towards the class of \emph{latent
    Gaussian models} (LGM). At the first stage, there are the
conditional independent observations $\mm{y}$,
\begin{displaymath}
    y_{i} \mid \ldots \;\sim\; \pi(y_{i} \mid \eta_{i}, \mm{\theta}_{1})
\end{displaymath}
where $\eta_{i}$ is the linear predictor for $y_{i}$ and
$\mm{\theta}_{1}$ are hyperparmaters in the likelihood model
$\pi(y_{i} \mid \eta_{i}, \mm{\theta}_{1})$. The likelihood model
could be Gaussian, Poisson or some other model. 

At the second stage, we have a model for the linear predictors
$\mm{\eta}$, as a linear combinations of various Gaussian effects,
\begin{displaymath}
    \eta_{i} = \sum_{j} \beta_{j} z_{ji} + \sum_{k}
    w_{ki}f_{k}(c_{ki}; \mm{\theta}_{2}).
\end{displaymath}
Here, $\mm{z}$'s are ``fixed effects'' with Gaussian coefficients
$\mm{\beta}$, $\mm{w}$ are fixed weights and $\mm{f}$ are some
Gaussian ``random effects''\footnote{We will use the terms ``fixed
    effects'' and ``random effects'' throughout this tutorial, even
    though that in a Bayesian context they are all ``random''.} The
terms $f_{k}(c_{ki}\; \mm{\theta}_{2})$ are general Gaussian ``random
effects'' models, for which the covariate $c_{ki}$ extract the
component of it that goes into the $i$th linear predictor. Again,
$\mm{\theta}_{2}$ are hyperparmaters in these general Gaussian
models. An important assumption, is that the joint density for $\mm{x}
= (\mm{\eta}, \mm{\beta}, \mm{f})$ is Gaussian with density
\begin{displaymath}
    \pi(\mm{x} \mid \mm{\theta}_{2}) \propto |\mm{Q}(\mm{\theta}_{2})|^{1/2}
    \exp\left(
      -\frac{1}{2} \mm{x}^{T}\mm{Q}(\mm{\theta}_{2}) \mm{x} +
      b(\mm{\theta_{2}})^{T} \mm{x}\right)
\end{displaymath}
where $\mm{Q}$ is the precision matrix and $\mm{b}$ is the linear
term, both which can depend on $\mm{\theta}_{2}$.

As the third stage, we have the joint prior for the hyperparmaters
$\mm{\theta} = (\mm{\theta}_{1}, \mm{\theta}_{2})$,
$\pi(\mm{\theta})$.

The target for the Bayesian analysis is to compute the posterior
marginals for each $\theta_{i}$, $\eta_{i}$, $\beta_{i}$ and $f_{ki}$
given the observations $y$.
    
\section{How do I install it?}

The \tv{R-INLA} package is available from the web-site 
\begin{verbatim}
    www.r-inla.org
\end{verbatim}
which also contains all other available information.

Before installing \tv{R-INLA} you might want to install the dependent
packages \tv{R-INLA} depends on, these are
\begin{itemize}
\item pixmap
\item sp
\end{itemize}
and optionally install suggested (not required) packages 
\begin{itemize}
\item numDeriv
\item Rgraphviz
\item graph
\item fields
\item rgl
\item mvtnorm
\item multicore
\end{itemize}
There are two ways to install \tv{R-INLA}.
\begin{enumerate}
\item Type the following command in \tv{R}
\begin{Schunk}
\begin{Sinput}
> source("http://www.math.ntnu.no/inla/givemeINLA.R")
\end{Sinput}
\end{Schunk}
which will download and install the package. 
\item You can also install the package manually. If you are using
    Linux or mach, then 
 download
\begin{verbatim}
http://www.math.ntnu.no/inla/binaries/INLA.tgz
\end{verbatim}
and install the package from within \tv{R} using
\begin{Schunk}
\begin{Sinput}
> install.packages("INLA.tgz", repos=NULL, type="source")
\end{Sinput}
\end{Schunk}
If you are using Windows, then download
\begin{verbatim}
http://www.math.ntnu.no/inla/binaries/INLA.zip
\end{verbatim}
Start \tv{R} and select the \emph{Packages menu}, then \emph{Install
    package from local zip file}, then find and highlight the location
of the zip file and click on open.
\end{enumerate}

It is important to keep your package up-to-date. Here you have two
options. 
\begin{enumerate}
\item There is a more stable version that is updated once-in-a-while,
    and you can upgrade it using
\begin{Schunk}
\begin{Sinput}
> inla.upgrade()
\end{Sinput}
\end{Schunk}
\item There is a testing version that is updated frequently and you
    can upgrade to it, using
\begin{Schunk}
\begin{Sinput}
> inla.upgrade(testing=TRUE)
\end{Sinput}
\end{Schunk}
%%
and this version will contain the most recent version of the package.
The testing-version is assumed for all examples at the
\verb|www.r-inla.org| page.
\end{enumerate}
If you have the testing-version installed, then you can ``downgrade''
to the most recent stable version doing \verb|inla.upgrade()| and
\verb|inla.upgrade(testing=TRUE)| will take you back to the more
recent testing-version. 

You can check which version you have installed using
\begin{Schunk}
\begin{Sinput}
> inla.version()
\end{Sinput}
\begin{Soutput}
	INLA build date .........: Fri Nov 23 18:44:00 CET 2012
	INLA hgid ...............: hgid: 6b73f3e09c6e  date: Fri Nov 23 18:43:19 2012 +0100
	INLA-program hgid .......: hgid: d61398f7f40e  date: Thu Nov 22 10:35:58 2012 +0100
	Maintainers .............: Havard Rue <hrue@math.ntnu.no>
	                         : Finn Lindgren <finn.lindgren@gmail.com>
	                         : Daniel Simpson <dp.simpson@gmail.com>
	Web-page ................: http://www.r-inla.org
	Email support ...........: help@r-inla.org
	                         : r-inla-discussion-group@googlegroups.com
	Source-code .............: http://inla.googlecode.com
\end{Soutput}
\end{Schunk}
The ``build date'' is when this packages was compiled and build, the
``INLA hgid'' is the version and date of the \tv{R}-code and 
``INLA-program hgid'' is the version and date of the inla-program that
do all the calculations. The version and date for the inla-program is
often the same as for the \tv{R}-code, but it can be slightly older. 

If you are interested in recent changes in the package then
\begin{Schunk}
\begin{Sinput}
> inla.changelog()
\end{Sinput}
\end{Schunk}
will place your browser at the list of the most recent changes. You can also visit
\begin{verbatim}
    inla.googlecode.com
\end{verbatim}
to view or download the complete source-code. 



\section{How do I use it?}

If you have installed the \tv{R-INLA} package then
\begin{Schunk}
\begin{Sinput}
> library(INLA)
\end{Sinput}
\end{Schunk}
will load the library and you are ready to go. 

If you are familiar with the \verb|lm()| function in \tv{R}, then you
can use \verb|inla()| function instead of the \verb|lm()| function to
do a Bayesian analysis. Here is a small example
\begin{Schunk}
\begin{Sinput}
> n = 100
> x = rnorm(n)
> y = 1 + x + rnorm(n, sd = 0.1)
> formula = y ~ 1 + x
> result = inla(formula, data = data.frame(y,x))
\end{Sinput}
\end{Schunk}
The \verb|result|-object contains all the results from the Bayesian analysis.
You can get a summary with
\begin{Schunk}
\begin{Sinput}
> summary(result)
\end{Sinput}
\begin{Soutput}
Call:
"inla(formula = formula, data = data.frame(y, x))"

Time used:
 Pre-processing    Running inla Post-processing           Total 
  0.17673659325   0.05490756035   0.05068182945   0.28232598305 

Fixed effects:
                   mean            sd   0.025quant    0.5quant  0.975quant
(Intercept) 1.005698523 0.01093528125 0.9842266159 1.005698434 1.027177538
x           1.023955214 0.01322397701 0.9979893491 1.023955108 1.049929668
                        kld
(Intercept) 9.860761315e-32
x           2.465190329e-32

The model has no random effects

Model hyperparameters:
                                        mean   sd     0.025quant 0.5quant
Precision for the Gaussian observations  84.50  11.98  63.07      83.80  
                                        0.975quant
Precision for the Gaussian observations 110.03    

Expected number of effective parameters(std dev): 2.139(0.01567)
Number of equivalent replicates : 46.76 

Marginal Likelihood:  62.05 
\end{Soutput}
\end{Schunk}
and posterior summary statistics for the fixed effects (the
\tv{Intercept} and the covariate \tv{x}) and the observational
precision (for the Gaussian observations) are shown. These summary
statistics are derived from the corresponding posterior marginals;
which can be plotted using
\begin{Schunk}
\begin{Sinput}
> plot(result)
\end{Sinput}
\end{Schunk}
and the results are shown in~\Fig{fig:11}.
\begin{figure}[tbp]\centering\includegraphics[width=7cm,height=7cm,angle=-90]{Sweave/ex1/figure-1}
\includegraphics[width=7cm,height=7cm,angle=-90]{Sweave/ex1/figure-2}
\includegraphics[width=7cm,height=7cm,angle=-90]{Sweave/ex1/figure-3}\caption{The posterior marginals for the fixed effects for the intercept and covariate 	v{x}, and the precision for the Gaussian observations.}\label{fig:11}\end{figure}


\section{How can I get help?}

\section{How do I report an error or a new feature request?}



\chapter{``Fixed effect'' models: Bayesian GLM}

\section{The formula spesification}

\section{Setting priors for ``fixed effects''}


\chapter{Likelihood models}

\section{Available models}

\section{Setting priors for hyperparameters}

\section{Using more than one type of likelihood models}

\section{\tv{NA} is the reponse}



\chapter{}



%%
\clearpage




\chapter{Bayesian generalized linear model}

\section{Introduction}\label{bglm:sec:intro}

In this chapter, we cover how to define Bayesian generalized linear
models (BGLMs) to perform approximate fully Bayesian inference with
INLA. These models can be written using a hierarchical representation
where the first stage is formed by a likelihood function with
conditional independence properties given the ($n_d \times 1$) vector
of linear predictors $\bs{\eta}$ and the ($m \times 1$) vector of
possible hyperparameters $\bs{\theta}$
\begin{list}{\labelitemi}{\leftmargin=5em}
\item[\textbf{Stage 1.}] $\bs{y}|\bs{\eta},\bs{\theta} \sim
    p(\bs{y}|\bs{\eta},\bs{\theta}) = \prod _{i=1}^{n_d} p(y_i|g^{-1}
    (\eta _i), \bs{\theta})$,
\end{list}
where $\bs{y}$ is the ($n_d \times 1$) vector of data points and
$g^{-1}(\cdot)$ is the inverse of the link function that connects the
conditional mean of $y_i$ to the linear predictor $\eta _i$.  The
second stage is formed by the systematic part of the model and states
that the covariates are linearly related to the linear predictor. In
matrix notation,
\begin{list}{\labelitemi}{\leftmargin=5em}
\item[\textbf{Stage 2.}] $\bs{\eta} = \bs{X} \bs{\beta}, \quad
    \bs{\beta} \sim N(\bs{\mu}, diag(\bs{\tau}^{-1}))$,
\end{list}
where $\bs{X}$ is a ($n_d \times p$) design matrix which have the $p$
covariate vectors as its columns and $\bs{\beta}$ is a ($p \times 1$)
vector of regression coefficients. INLA attributes independent
Gaussian priors with mean $\mu _i$ and precision $\tau_i$ for each of
the regressor coefficients $\beta_i$, $i = 1,...,p$.  The hierarchical
model is then completed with an appropriate prior distribution for a
possible vector of hyperparameters $\bs{\theta}$
\begin{list}{\labelitemi}{\leftmargin=5em}
  \item[\textbf{Stage 3.}] $\bs{\theta} \sim \pi(\bs{\theta})$.
\end{list}

\section{Latent field}

\subsection{Specifying the linear predictor}

As mentioned in Section \ref{bglm:sec:intro}, in generalized linear
models, each data point is connected, through a link function, to a
linear predictor. In matrix notation
\begin{equation}
    \bs{\eta} = \bs{X}\bs{\beta}.
    \label{bglm:eq:linear_pred}
\end{equation}

The specification of the linear predictor (\ref{bglm:eq:linear_pred})
in INLA follow the same pattern found in \texttt{lm} and \texttt{glm}
functions available in \texttt{R}.  That is, the linear predictor can
be specified using the standard
\begin{equation}
    \texttt{formula = y ~ a + b + a:b + c*d}
    \label{bglm:eq:formulaspec}
\end{equation}
where \texttt{y} is the response and \texttt{a}, \texttt{b},
\texttt{c} and \texttt{d} are covariates.  Table
\ref{bglm:tab:lmsymbols} display the meaning of the symbols
\texttt{+}, \texttt{:} and \texttt{*} used in
(\ref{bglm:eq:formulaspec}).

\begin{table}[ht!]
    \centering
    \begin{tabular}{|c|l|}
        \hline
        + & to combine main effects, as in a + b \\
        \hline
        : & for interaction terms, as in a:b; \\
        \hline
        * & for both main effects and interaction terms, so c*d = c+d+c:d \\
        \hline
    \end{tabular}
    \caption{Meaning of the symbols used in the formula for specification of fixed effects.}
    \label{bglm:tab:lmsymbols}
\end{table}

Instead of using the formula specification
(\ref{bglm:eq:formulaspec}), it is also possible to use the design
matrix $\bs{X}$ explicitly as in
\begin{equation}
    \texttt{formula = y ~ X}
    \label{bglm:eq:designspec}
\end{equation}
where in this case the design matrix $\bs{X}$ is used as input for the 
\texttt{inla} function together with the response vector $\bs{y}$ in 
a list object of the form
\begin{center}
    \texttt{inla(..., data = list(y=y, X=X), ...)}
\end{center}

More details about the two forms (\ref{bglm:eq:formulaspec}) and
(\ref{bglm:eq:designspec}) of specifying fixed effects in INLA can be
found on the examples of Section \ref{bglm:sec:examples}.

\subsection{Prior for the fixed effects}

As mentioned in Section \ref{bglm:sec:intro}, INLA attributes
independent Gaussian priors for each one of the fixed effects of the
model. The mean and precision of each of the priors can be controlled
using the argument \texttt{control.fixed} of the \texttt{inla}
function.  The argument \texttt{control.fixed} needs to be a list, and
the arguments relevant for the prior specification of the fixed
effects are:
\begin{itemize}
\item \texttt{mean.intercept}: prior mean for the intercept. Default value is 0.
\item \texttt{prec.intercept}: precision for the intercept. Default value is 0.
\item \texttt{mean}: prior mean for all fixed effects except the intercept.
    Alternatively, a named list with specific means where \texttt{name = default} 
    applies to unmatched names. For example 
\texttt{control.fixed = list(mean = list(a = 1, b = 2, default = 0))} assign
\texttt{mean = 1} to fixed effect \texttt{a}, \texttt{mean = 2} to effect 
\texttt{b} and \texttt{mean = 0} to all others.
\item \texttt{prec}: Similar to the \texttt{mean} argument above, \texttt{prec} set the 
    precision for all fixed effects except the intercept. Alternatively, a named list with 
    specific precisions where \texttt{name = default} applies to unmatched names.
\end{itemize}

As an example, assume we have a model with an intercept and four fixed
effects \tv{a}, \tv{b}, \tv{c} and \tv{d} and we want the intercept to
have mean $0$ and precision $10$, the fixed effect \tv{b} to have mean
$3$ and precision $5$ and the remaining to have mean $10$ and
precision $20$. Then the prior specification in this case would be

\begin{verbatim}
control.fixed = list(mean.intercept = 0,
                     prec.intercept = 10,
                     mean = list(b = 3, default = 10),
                     prec = list(b = 5, default = 20))

result = inla(..., control.fixed = control.fixed, ...)
\end{verbatim}

\section{Likelihood function}

For BGLMs, the likelihood function belong to the exponential family,
as for example the Gaussian and Poisson case. When defining a
likelihood function for use with INLA, some aspects deserves
attention.
\begin{enumerate}
\item \textbf{Parametrization}: For example, in the Gaussian case, the
    parametrization used by INLA is mean $\mu$, precision $\tau$, and
    a possible (known) scale parameter $s$ for each observation, so
    that for data point $y_i$, we have a mean $\mu_i$ and variance
    $\sigma_i = \frac{1}{s_i\tau}$

\item \textbf{Link function}: Check how the linear predictor is
    connected to a specific parameter of the likelihood function. In
    the Gaussian case, the linear predictor $\eta$ is connected to the
    mean $\mu$ through an identity link function, while for the
    Poisson case, the linear predictor is connected to the mean
    $\lambda$ through $\eta = \log (\lambda)$.

\item \label{bglm:enum:hyper} \textbf{Hyperparameters}: Every unknown
    likelihood parameter that is not connected to the linear predictor
    is considered to be a hyperparameter. For the Gaussian likelihood,
    the precision parameter $\tau$ is considered a
    hyperparameter. Every hyperparameter has an internal
    parametrization. For example, the internal parametrization of the
    precision parameter $\tau$ in the Gaussian case is $\theta = \log
    (\tau)$, and the prior specification is done using the
    $\theta$-parametrization.

\item \textbf{Prior for hyperparameters}: As mentioned on item
    \ref{bglm:enum:hyper}, the priors for the hyperparameters are
    defined using the internal parametrization. For example, in the
    Gaussian case the prior is defined for $\theta$, where $\theta =
    \log (\tau)$.

    The prior specification for the hyperparameters of the likelihood
    function should be defined using the \texttt{control.data}
    argument of the \tv{inla} function, using the following format

    \begin{center}
        \tv{inla(..., control.data = list(hyper = hyper.lik), ...)} 
    \end{center}
    
    where \texttt{hyper.lik} is a list with one component for each
    hyperparameter in the likelihood, and each component on this list
    should be a list with information about the prior distribution for
    that specific hyperparameter.
 
    For the Gaussian case, we would have:
    
\begin{verbatim}
  hyper.lik = list(prec = prec) # There is only one
                                # hyperparameter

  prec = list(prior = "loggamma", # name of the prior
              param = c(2,0.01),  # parameters of the prior
              initial = log(200)) # initial value for 
                                  # optmization algorithm.
\end{verbatim}
    
    This might seem a little complicated now but will become clearer
    when we go through several examples later in Section
    \ref{bglm:sec:examples}.
    
\end{enumerate}

\section{Goodness of fit and predictive measures}

\section{\tv{inla()} function and the resulting object}

Up to this point we have learned how to include fixed effects in the
linear predictor, how to chose the family of the likelihood function,
and how to set up priors for the fixed effects and the hyperparameters
in the model. Now we just need to join all this together and use it as
arguments for the \tv{inla()} function:

\begin{verbatim}
## calling the inla function
result = inla(# formula for the linear predictor
              formula,
              # prior for the fixed effects
              control.fixed,
              # dataset
              data,
              # family distribution
              family,
              # prior for hyperparameters in the likelihood function
              control.data)
\end{verbatim}

After you have runned your model with INLA using the above function,
the resulting object will be stored in the \tv{result} variable. This
will be a S3 object with several attributes and we will now highlight
the most important ones.

With \tv{result\$summary.fixed} one can access the summary matrix for
the fixed effects which contains by default the mean, standard
deviation, and the $(0.025, 0.5, 0.975)$ quantiles. Similarly, we use
\tv{result\$summary.hyper} to obtain the summary matrix for the
hyperparameters in the model.

In some application we need to go beyond point estimates and have
access to the posterior distributions of the fixed effects and
hyperparameters.  We can access those with
\tv{result\$marginals.fixed} and \tv{result\$marginals.hyper},
respectively. Those objects are a named list with one element for each
parameter, and each of these elements are a matrix with two columns
where the first column hold the x-axis and the second column hold the
y-axis of the marginal posterior distribution. As an example, if we
fit a model where the linear predictor is defined by
\begin{center}
\tv{formula = y ~ 1 + a + b}
\end{center}
we can expect something similar to the following structure for 
\tv{result\$marginals.fixed}:

\small{
\begin{verbatim}
$`(Intercept)`
                x            y
 [1,] -35.2578230 1.672335e-08
 [2,] -31.2495132 6.823312e-08
         ...         ...
[80,] 108.5801057 2.072500e-08
[81,] 112.7888309 4.417685e-09

$a
                  x            y
 [1,] -0.7176915644 1.999485e-06
 [2,] -0.6841669015 8.158126e-06
          ...          ...
[80,]  0.4853387317 2.477937e-06
[81,]  0.5205396278 5.281901e-07

$b
                 x            y
 [1,] -0.688182689 2.624530e-06
 [2,] -0.662642127 1.070838e-05
           ...         ...
[80,]  0.228338654 3.252553e-06
[81,]  0.255156244 6.933052e-07
\end{verbatim}
}
\noindent That is, in this example, \tv{result\$marginals.fixed}
returns a named list with elements \tv{(Intercept)}, \tv{a} and
\tv{b}, each of which is a matrix with two columns, as expected.


\section{Examples}\label{bglm:sec:examples}

\subsection{Carbohydrate data - Normal linear model}

\subsubsection*{Example description}

The data displayed in table \ref{tab:bayesglm:carbdata} was taken from
\cite{dobson1983introduction}.  The data show responses, percentages
of total calories obtained from complex carbohydrates, for twenty male
insulin-dependent diabetics who had been on a high-carbohydrate diet
for six months. Compliance with the regime was thought to be related
to age (in years), body weight (relative to 'ideal' weight for height)
and other components of the diet, such as the percentage of calories
as protein. These other variables are treated as explanatory
variables.

\begin{table}[ht!]
    \centering
    \begin{tabular}{c|c|c|c}
        \hline
        Carbohydrate & Age & Weight & Protein \\
        \hline
        33 & 33 & 100 & 14 \\
        40 & 47 & 92  & 15 \\
        37 & 49 & 135 & 18 \\
        27 & 35 & 144 & 12 \\
        30 & 46 & 140 & 15 \\
        43 & 52 & 101 & 15 \\ 
        34 & 62 & 95  & 14 \\
        48 & 23 & 101 & 17 \\ 
        30 & 32 & 98  & 15 \\
        38 & 42 & 105 & 14 \\
        50 & 31 & 108 & 17 \\
        51 & 61 & 85  & 19 \\
        30 & 63 & 130 & 19 \\
        36 & 40 & 127 & 20 \\
        41 & 50 & 109 & 15 \\
        42 & 64 & 107 & 16 \\
        46 & 56 & 117 & 18 \\
        24 & 61 & 100 & 13 \\
        35 & 48 & 118 & 18 \\
        37 & 28 & 102 & 14 \\
        \hline
    \end{tabular}
    \caption{Carbohydrate, age, relative weight and protein for twenty 
        male insulin-dependent diabetics; for units, see text.}
    \label{tab:bayesglm:carbdata}
\end{table}

\subsubsection*{Model definition}

A possible model to analyze this data is

\begin{enumerate}
\item $y_i|\eta_i, \tau \sim N(\eta_i, \tau^{-1})$, $i=1,...,20$. 
\item $\eta_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3}$
\item $\tau \sim Gamma(2, 0.01)$, $p(\beta_j) \propto 1$, $j=0,1,2,3$
\end{enumerate}
in which carbohydrate $y$ is linearly related to age $x_1$, relative weight $x_2$
and protein $x_3$. 


\subsubsection*{INLA code}


{\small\bibliography{thiago,hrue}}

\end{document}


There is a dedicated web-site for \tv{R-INLA}
\begin{verbatim}
    http://www.r-inla.org
\end{verbatim}
and for the source-code itself
\begin{verbatim}
    http://inla.googlecode.com
\end{verbatim}
