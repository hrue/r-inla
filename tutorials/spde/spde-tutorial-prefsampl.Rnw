\chapter{Log-Cox point process and preferential sampling}\label{ch:lcox}

This content is part of the book available at
\url{http://www.r-inla.org/spde-book},
whose Gitbook version is freely available 
along all the code and datasets.

<<sett,echo=F,results='hide',message=FALSE,warning=FALSE>>=
library(knitr)
opts_chunk$set(
fig.path='figs/prefsampl',
message=FALSE, warning=FALSE
)
options(width=75, prompt = " ", continue = "   ")
library(INLA)
lcall <- inla.getOption('inla.call')
inla.setOption(inla.call='remote')
inla.setOption(num.threads=7)
library(splancs)
library(fields)
source('R/spde-tutorial-functions.R')
@ 

\section{The \cite{simpsonetal:2016} approach example}

Under the log-Cox model assumption, there is a 
latent Gaussian Random Field (LGRF) and the inference 
can be done using \textbf{\textsf{INLA}}, 
\cite{mollerSW:1998}.
A common aproach to fit the log-Cox process is to 
divide the study region into cells, that forms a lattice, 
and count the number of points into each one, 
\cite{mollerW:2003}. 
One can consider this counts and a Poisson likelihood 
conditional to a LGRF and use INLA to fit the model,
\cite{illianetal:2012}. 

An approach considering the SPDE approach 
was considerd in \cite{simpsonetal:2016}. 
This approach has a nice theoretical justification 
and considers a direct approximation of the log-Cox 
point process model likelihood. 
The data is modeled considering its exact location 
instead of binning it into cells. 
Along with the flexibility for defining a mesh, 
it can handle non-rectangular areas 
avoiding to waste computational effort. 

\subsection{Data simulation} 

We will use the data simulated here later in 
Section~\ref{sec:lcoxcov} and in Section~\ref{sec:prefsampl}. 
To sample from a log-Cox point process we will use the 
\texttt{rLGCP()} function from the 
\textbf{\textsf{spatstat}} \textbf{\textsf{R}} package. 
By default that function do simulation on window 
over the $(0,1) \times (0,1)$ square. 
We choose to do simulation 
over the $(0,3) \times (0,3)$ square.
<<window>>=
library(spatstat)
win <- owin(c(0,3), c(0,3))
@ 

This function uses the \texttt{GaussRF()} function 
from the \textsf{\textbf{RandomFields}} package. 
The \texttt{rLGCP} uses the \texttt{GaussRF()} function 
to do simulation of the LGRF over a grid on the provided 
window and use it to do the point process simulation. 

There is an internal parameter to control the 
resolution of the grid. We change it to
<<gridres>>=
spatstat.options(npixel=300)
@ 

First we define the model parameter for 
the model is the mean of the LGRF. 
This is directly related to expected number 
of points of the spatial pattern. 
The expected number of points is its 
exponential times the area fo the window. 
We use 
<<>>=
beta0 <- 3
@ 
So, the expected number of points is 
<<n-exp>>=
exp(beta0) * diff(range(win$x)) * diff(range(win$y))
@ 
Is also possible to use a functional, 
see Shapter~\ref{sec:lcoxcov}.

In the estimation process we use the Matern 
covariance function with $\nu=1$. 
So, here we just fix it on this value. 
The other parameters are the variance and scale
<<>>=
sigma2x <- 0.2;      kappa <- 2
@ 

Doing the simulation
<<simulapp,eval=TRUE>>=
library(RandomFields)
set.seed(1)
lg.s <- rLGCP('matern', beta0, 
              var=sigma2x, scale=1/kappa, nu=1, win=win)
@ 

Both the LGRF and the point pattern are returned. 
We collect the point pattern locations coordinates with 
<<xy>>=
xy <- cbind(lg.s$x, lg.s$y)[,2:1]
@ 
and the number of points is 
<<nxy>>=
(n <- nrow(xy))
@ 

The exponential of simulated values of the LGRF 
are returned as the \texttt{Lambda} attribute of the object. 
We extract the $\Lambda$ and see a summary of the 
$log(\Lambda)$ below
<<>>=
Lam <- attr(lg.s, 'Lambda')
summary(as.vector(rf.s <- log(Lam$v)))
@ 

We can see the simulated LGRF over the grid and the point pattern simulated 
in Figure~\ref{fig:lgrfpp} produced with the following commands 
<<lgrfpp,eval=F>>=
par(mfrow=c(1,1))
library(fields)
image.plot(list(x=Lam$yrow, y=Lam$xcol, z=rf.s), main='log-Lambda', asp=1)
points(xy, pch=19) 
@ 
\setkeys{Gin}{width=0.55\textwidth}
\begin{figure}\centering
<<echo=F,results='hide',fig.width=5,fig.height=5>>=
<<lgrfpp>>
@ 
\caption{Simulated intensity of the point process (left), 
  simulated point pattern (right).}
\end{figure}\label{fig:lgrfpp}


\subsection{Inference} 

Following \cite{simpsonetal:2016} we can estimate the parameters 
of the log-Cox point process model using few command lines. 

\subsubsection{The mesh and the weights}

To do inference for the log-Cox point process model 
we also need some care on building the mesh and on using it. 

To do inference for the log Cox process, it is not necessarly better 
to have any location points as any of the mesh nodes, as on the 
geostatistical analysis where it helps a bit for the estimation 
of the nugget effect, see~\ref{sec:meshcompare}. 
We just need a mesh that covers the study region. 
So, we use the \code{loc.domain} argument to build the mesh. 

An additional thing is that we ignore the second outer extension and 
we use a small first outer extension. 
This is because it is not necessary to have nodes out of the study region 
when it recieves zero weights (see weight computation below). 
<<mesh>>=
loc.d <- 3*t(matrix(c(0,0,1,0,1,1,0,1,0,0), 2))
(nv <- (mesh <- inla.mesh.2d(
            loc.domain=loc.d, offset=c(.3, 1), 
            max.edge=c(.3, 0.7), cutoff=.05))$n) 
@ 
which is visualized at Figure~\ref{fig:ppmesh} with following commands 
<<meshplot,eval=FALSE>>=
par(mar=c(0,0,0,0))
plot(mesh, asp=1, main='')
points(xy, col=4, pch=19); lines(loc.d, col=3)
@ 

\setkeys{Gin}{width=0.5\textwidth}
\begin{figure}\centering
<<echo=F,results='hide',fig.width=5,fig.height=5>>=
<<meshplot>>
@ 
\caption{Mesh used to inference for the log-Cox process.}
\end{figure}\label{fig:ppmesh}

Defining the SPDE model considering the PC-prior 
derived in \cite{fuglstadetal:2017} for the 
model parameters as the practical range, $\sqrt{8\nu}/\kappa$, 
and the marginal standard deviation.  
<<spde>>=
spde <- inla.spde2.pcmatern(
    mesh=mesh, alpha=2, ### mesh and smoothness parameter
    prior.range=c(0.05, 0.01), ### P(practic.range<0.05)=0.01
    prior.sigma=c(1, 0.01)) ### P(sigma>1)=0.01
@ 

The SPDE approach defines the model on the nodes of the mesh. 
To fit the the log-Cox point process model 
these points are considered the integration points. 
The method in \cite{simpsonetal:2016} defines the 
expected number of events to be proportional to the 
polygons volume of the dual mesh. 
It means that at the node on the mesh with has the larger edges 
we have larger expected value. 
The \code{diag(spde\$param.inla\$M0)} 
gives this value for every mesh node. 
However, the mesh has nodes out of the domain implying 
<<>>=
sum(diag(spde$param.inla$M0))
@ 
to be larger than the domain area. 

We can use these values for the nodes on the inner domain 
and compute the intersection between the dual mesh polygons 
and the study domain polygon. 
To do that we have the function \texttt{inla.mesh.dual()}, 
available in 
\url{http://inla.r-inla-download.org/r-inla.org/tutorials/spde/R/spde-tutorial-functions.R}
This function considers a mesh and return the dual mesh 
in the \texttt{SpatialPolygons} class.
<<dualmesh>>=
dmesh <- inla.mesh.dual(mesh)
@ 
We can see it in Figure~\ref{fig:dualmesh}. 

We can convert the domain polygon into a 
\texttt{SpatialPolygons} class with 
<<splocd>>=
domainSP <- SpatialPolygons(list(Polygons(
    list(Polygon(loc.d)), '0')))
@ 
Them, we can have the intersection with each 
polygon from the mesh dual using the functions 
\texttt{gIntersection()} from the 
\textbf{\textsf{rgeos}} package
<<pols>>=
library(rgeos)
sum(w <- sapply(1:length(dmesh), function(i) {
    if (gIntersects(dmesh[i,], domainSP))
        return(gArea(gIntersection(dmesh[i,], domainSP)))
    else return(0)
}))
@ 
And we chan check that there are 
integration points with zero weight. 
<<wsummary>>=
table(w>0)
@ 
These are identified in red in Figure~\ref{fig:dualmesh}. 
<<dualmeshcode,eval=FALSE>>=
par(mar=c(2,2,1,1), mgp=2:0)
plot(mesh$loc, asp=1, col=(w==0)+1, pch=19, xlab='', ylab='') 
plot(dmesh, add=TRUE)
lines(loc.d, col=3)
@ 

\setkeys{Gin}{width=0.5\textwidth}
\begin{figure}\centering
<<echo=F,results='hide',fig.width=5,fig.height=5>>=
<<dualmeshcode>>
@ 
\caption{Voronoy polygons for the mesh used to inference for the log-Cox process.}
\end{figure}\label{fig:dualmesh}

\subsubsection{The data and projector matrices}

This vector is just what we need to use as the exposure (expected) 
for the Poisson likelihood and is related to the augumented data that we need 
to fit using the Poisson likelihood. 
We can specify that the first observations (number of nodes)
are zero and the last are ones (number of events). 
<<y01>>=
y.pp <- rep(0:1, c(nv, n))
@ 
So, the expected vector can be defined by 
<<expected>>=
e.pp <- c(w, rep(0, n)) 
@ 

We must have to define the projector matrix 
to do inference using the SPDE approach, \cite{lindgren:2012}. 
For the observed points locations we have 
<<Aloc>>=
lmat <- inla.spde.make.A(mesh, xy)
@ 
We need also a projector matrix for 
the integration points and this is 
just a diagonal matrix because 
this locations are just the mesh vertices. 
<<pp-proj>>=
imat <- Diagonal(nv, rep(1, nv))
@
So, the entire projector matrix is 
<<App>>=
A.pp <- rBind(imat, lmat)
@ 

The data stack can be made by 
<<stkpp>>=
stk.pp <- inla.stack(data=list(y=y.pp, e=e.pp), 
                     A=list(1,A.pp), tag='pp',
                     effects=list(list(b0=rep(1,nv+n)), list(i=1:nv))) 
@ 

\subsubsection{Posterior marginals}

The posterior marginals for the parameters 
are obtained with 
<<ppest>>=
pp.res <- inla(y ~ 0 + b0 + f(i, model=spde), 
               family='poisson', data=inla.stack.data(stk.pp), 
               control.predictor=list(A=inla.stack.A(stk.pp)), 
               E=inla.stack.data(stk.pp)$e)
@ 

We can see the summary for the practical range and 
standard deviation of the lateng Gaussian random field with 
<<pppars,eval=TRUE>>=
round(pp.res$summary.hyperpar, 4)
@ 

The posterior distribution of the log-Cox model 
parameters are visualized on the Figure~\ref{fig:pppost}. 
<<pp-viz,eval=F>>=
par(mfrow=c(1,3), mar=c(3,3,1,0.3), mgp=c(2,1,0)) 
plot(pp.res$marginals.fix[[1]], type='l', 
     xlab=expression(beta[0]), ylab='Density')
abline(v=beta0, col=2)
plot(pp.res$marginals.hyperpar[[2]], type='l', 
     xlab=expression(sigma^2), ylab='Density')
abline(v=sigma2x, col=2)
plot(pp.res$marginals.hyperpar[[1]], type='l', 
     xlab='Nominal range', ylab='Density')
abline(v=sqrt(8*1)/kappa, col=2)
@ 
\setkeys{Gin}{width=0.99\textwidth}
\begin{figure}\centering
<<echo=F,results='hide',fig.width=10,fig.height=5>>=
<<pp-viz>>
@ 
\caption{Posterior distribution for the parameters of 
  the log-Cox model $\sigma^2$ (left), $\kappa$ (mid) 
  and the nominal range  (right)} 
\end{figure}\label{fig:pppost}


\section{Including a covariate on the log-Cox process}\label{sec:lcoxcov}

In the previous example we have done simulation considering 
the underline intensity as just the exponential of a realization 
of a Gaussian random field. 
In this chapter we consider that we have an additional effect, 
which is treated as a covariate. 
In order to fit the model, it is needed the covariate value everywhere, 
at the location points and at the integration points. 

\subsection{Covariate everywhere}

The simulation is done considering that the covariate effect is available 
at the same grid points where the Gaussian process is simulated. 
So, first we create an artificial covariate at the grid 
<<gridcov>>=
y0 <- x0 <- seq(win$xrange[1], win$xrange[2], 
                length=spatstat.options()$npixel)
gridcov <- outer(x0, y0, function(x,y) cos(x) - sin(y-2))
@ 
Now, the expected number of points is function of the covariate 
<<n-exp-cov>>=
beta1 <- -0.5
sum(exp(beta0 + beta1*gridcov) * diff(x0[1:2])*diff(y0[1:2]))
@ 

Doing the simulation
<<simulappc>>=
set.seed(1)
lg.s.c <- rLGCP('matern', im(beta0 + beta1*gridcov, xcol=x0, yrow=y0), 
              var=sigma2x, scale=1/kappa, nu=1, win=win)
@ 

Both, the LGRF and the point pattern, are returned. 
The point pattern locations are 
<<xyc>>=
(n.c <- nrow(xy.c <- cbind(lg.s.c$x, lg.s.c$y)[,2:1]))
@ 

We can see the covariate values and the simulated LGRF over the grid 
in Figure~\ref{fig:lgrfppc} with the following commands
<<lgrfppc,eval=FALSE>>=
library(fields)
par(mfrow=c(1,2), mar=c(2,2,1,1), mgp=c(1,0.5,0))
image.plot(list(x=x0, y=y0, z=gridcov), main='Covariate', asp=1)
image.plot(list(x=x0, y=y0, z=log(attr(lg.s.c, 'Lambda')$v)), 
           main='log-Lambda', asp=1)
points(xy.c, pch=19)
@ 

\setkeys{Gin}{width=0.99\textwidth}
\begin{figure}\centering
<<echo=F,results='hide',fig.width=10,fig.height=5>>=
<<lgrfppc>>
@ 
\caption{Covariate (left), simulated intensity of the point process (mid), 
  simulated point pattern (right).}
\end{figure}\label{fig:lgrfppc}

\subsection{Inference} 

We have to include the covariate values to do the inference. 
We need to collect it at the point pattern locations and at the 
mesh nodes from the grid. 

We collect the covariate with the command below
<<collcovar>>=
covariate = gridcov[Reduce('cbind', nearest.pixel(
    c(mesh$loc[,1], xy.c[,1]), c(mesh$loc[,2], xy.c[,2]), 
    im(gridcov, x0, y0)))]
@ 

The augumented response data is created in same way as before. 
<<datc>>=
y.pp.c <- rep(0:1, c(nv, n.c))
e.pp.c <- c(w, rep(0, n.c))
@ 

The projector matrix for the observed points locations 
<<A.c>>=
lmat.c <- inla.spde.make.A(mesh, xy.c)
@ 
The entire projector matrix, using the previous for the 
integration points, is
<<App.c>>=
A.pp.c <- rBind(imat, lmat.c)
@ 

The data stack is 
<<stkpp.c>>=
stk.pp.c <- inla.stack(data=list(y=y.pp.c, e=e.pp.c), 
                       A=list(1, A.pp.c), tag='pp.c',
                       effects=list(list(b0=1, covariate=covariate), 
                           list(i=1:nv)))
@ 

The model is fitted by 
<<ppest.c>>=
pp.c.res <- inla(y ~ 0 + b0 + covariate + f(i, model=spde), 
                 family='poisson', data=inla.stack.data(stk.pp.c), 
                 control.predictor=list(A=inla.stack.A(stk.pp.c)), 
                 E=inla.stack.data(stk.pp.c)$e)
@ 

Summary of the model parameters 
<<insp-u.c,eval=TRUE>>=
round(pp.c.res$summary.hyperpar, 4)
@ 

The posterior distribution of the log-Cox model 
parameters are visualized on the Figure~\ref{fig:pppostc}. 

<<ppc-viz,eval=F>>=
par(mfrow=c(2,2), mar=c(3,3,1,0.3), mgp=c(2,1,0)) 
plot(pp.c.res$marginals.fix[[1]], type='l', ylab='Density', 
     xlab=expression(beta[0])); abline(v=beta0, col=2)
plot(pp.c.res$marginals.fix[[2]], type='l', ylab='Density', 
     xlab=expression(beta[1])); abline(v=beta1, col=2)
plot(pp.c.res$marginals.hyperpar[[2]], type='l', ylab='Density', 
     xlab=expression(sigma)); abline(v=sigma2x^0.5, col=2)
plot(pp.c.res$marginals.hyperpar[[1]], type='l', ylab='Density',
     xlab=expression(sqrt(8)/kappa)); abline(v=kappa, col=2)
@ 
\setkeys{Gin}{width=0.75\textwidth}
\begin{figure}\centering
<<echo=F,results='hide',fig.width=7.5,fig.height=7.5>>=
<<ppc-viz>>
@ 
\caption{Posterior distribution for the intercept (top left), 
  coefficient of the covariate (top right) and the parameters of 
  the log-Cox model $\sigma^2$ (bottom left), $\kappa$ (bottom right)} 
\end{figure}\label{fig:pppostc}

\section{Geostatistical inference under 
  preferential sampling}\label{sec:prefsampl}

In some cases the effort on sampling depends on the response. 
For example, is more common to have stations collecting data 
about pollution on industrial area than on rural ones. 
To make inference in this case, we can test if we 
have a preferential sampling problem in our data. 
One approach is to build a joint model considering a 
log-Cox model for the point pattern (the locations) 
and the response, \cite{diggleetal:2010}. 
So, we need also to make inference for a 
point process model jointly. 

This approach assumes a linear predictor for the point process as 
\[ \beta_0 + \textrm{random field} \]
and for the observations a linear predictor which is function 
of the latent Gaussian random field for the point process
\[ \beta_0^y + \beta \textrm{random field} \]
where $\beta_0^y$ is an intercept for the observations and 
$\beta$ is a sharing parameter. 

An illustration of the use \textbf{\textsf{INLA}} for the 
preferential sampling problem is on the case studies section 
of the \textbf{\textsf{INLA}} web page, precisely on 
\url{http://www.r-inla.org/examples/case-studies/diggle09}. 
This example uses the two dimentional random walk model 
for the latent random field. 
Here, we show geoestatistical inference under preferencial 
sampling using SPDE. 

We now will use the values of the latent GRF 
considered in the simulation of the point process 
to define an outcome at the location points. 
We just take the values of closest grid centers 
to each location of the point pattern. 
The values of the LGRF is collected (and a summary) 
at closest grid centers with 
<<simulaz>>=
summary(z <- log(t(Lam$v)[Reduce(
  'cbind', nearest.pixel(xy[,1], xy[,2], Lam))]))
@ 

These values are the latent field with zero mean plus the defined intercept.
We define the response as a different intercept $\beta_y$ and multiply the 
zero mean random field with a $1/\beta$, where $\beta$ is the parameter 
as the sharing parameter between the intensity 
of the point process locations and the response. 
<<resp>>=
beta0.y <- 10;   beta <- -2;   prec.y <- 16
set.seed(2)
summary(resp <- beta0.y + (z-beta0)/beta + 
        rnorm(length(z), 0, sqrt(1/prec.y)))
@ 
Considering $\beta<0$, it means that the response values is inversly 
proportional to the point process density.  


\subsection{Fitting the usual model}

Here, we just fit the geoestatistical model using the 
usual approach. In this approach we just use the locations 
as fixed. We use the mesh of the previous Chapter. 

<<rresp>>=
stk.u <- inla.stack(data=list(y=resp), A=list(lmat, 1), 
                    effects=list(i=1:nv, b0=rep(1,length(resp))))
u.res <- inla(y ~ 0 + b0 + f(i, model=spde), 
              data=inla.stack.data(stk.u), 
              control.predictor=list(A=inla.stack.A(stk.u)))
round(cbind(True=c(beta0y=beta0.y, prec.y=prec.y), 
            rbind(u.res$summary.fix[, 1:6], u.res$summary.hy[1,])), 4)
@ 

We can see the posterior distribution marginals 
for the model parameters in the Figure~\ref{fig:upost}, 
produced with the following code
<<insp-u,eval=F>>=
par(mfrow=c(1,3), mar=c(3, 3, 0.3, 0.3), mgp=c(2,1,0))
plot(inla.tmarginal(function(x) sqrt(1/x), 
                    u.res$marginals.hyperpar[[1]]), 
     type='l', ylab='Density', xlab=expression(sigma)) 
abline(v=prec.y^-0.5, col=2)
plot(u.res$marginals.hyperpar[[2]], type='l', ylab='Density', 
     xlab=expression(sqrt(8)/kappa)); abline(v=sqrt(8)/kappa, col=2)
plot(u.res$marginals.hyperpar[[3]], type='l', 
     xlab=expression(sqrt(sigma^2[x])), ylab='Density')
abline(v=sigma2x^0.5, col=2)
@ 
\setkeys{Gin}{width=0.99\textwidth}
\begin{figure}\centering
<<echo=F,results='hide',fig.width=6,fig.height=2.1>>=
<<insp-u>>
@ 
\caption{Posterior distribution for $\sigma^2$, 
  $\kappa$ and the nominal range just using the response.}
\end{figure}\label{fig:upost}

\subsection{Model fitting under preferential sampling}

In this situation we fit the model where a LGRF is 
considered to model both point pattern and the response. 
Using \textbf{\textsf{INLA}} it can be done using two likelihoods, 
one for the point pattern and another for the response. 
To do it we need a matrix response and a new index set 
to specify the model for the LGRF. 
It is more easy by using the \texttt{inla.stack()} 
following previous examples for two likelihood models. 

We consider the point pattern 'observation' on the first 
column and the response values on the second column. 
So, we just redefine the stack for the 
response and also for the point process. 
We put the response on the first column 
and the Poisson data for the point process 
as the second column. 
Also, to avoid the expected number of cases as NA 
for the Poisson likelihood, we set it as zero 
on the response data stack. 
For the SPDE effect on the point process part 
we have to model it as a copy of the SPDE 
effect at response part. 
We do it by defining a index set with different name 
and use it on the copy feature later. 
<<ppstk>>=
stk2.y <- inla.stack(data=list(y=cbind(resp,NA), e=rep(0,n)), 
                     A=list(lmat, 1), tag='resp2',
                     effects=list(i=1:nv, b0.y=rep(1,n)))
stk2.pp <- inla.stack(data=list(y=cbind(NA,y.pp), e=e.pp), 
                      A=list(A.pp, 1), tag='pp2',
                      effects=list(j=1:nv, b0.pp=rep(1,nv+n))) 
j.stk <- inla.stack(stk2.y, stk2.pp)
@ 

Now, we fit the geostatistical model under preferential sampling. 
To put the LGRF on both likelihood, we have to use the copy 
strategy. 
<<j-res>>=
jform <- y ~ 0 + b0.pp + b0.y + 
    f(i, model=spde) + f(j, copy='i', fixed=FALSE)
j.res <- inla(jform, family=c('gaussian', 'poisson'), 
              data=inla.stack.data(j.stk), E=inla.stack.data(j.stk)$e,              
              control.predictor=list(A=inla.stack.A(j.stk)))
round(cbind(True=c(beta0, beta0.y), 
            j.res$summary.fix), 4)
@ 

We can visualize the posterior marginal distributions 
for the model parameters from the result considering only 
the point process (PP), only the observations/marks 
($\mathbf{Y}$) and jointly in Figure~\ref{fig:jpars}. 
Notice that for the $\beta_0$ parameter we only have 
results considering only the PP and joint, 
for $\beta_o^y$ we only have results considering only 
$\mathbf{Y}$ and joint, and  
for $\beta$ (fitted using copy) we only have result 
from the joint model. 
Only for the random field parameters we do have 
results from the three results. 

\setkeys{Gin}{width=0.7\textwidth}
\begin{figure}\centering
<<pppost,echo=F,results='hide',fig.width=5.5,fig.height=4>>=
par(mfrow=c(2,3), mar=c(3,3,0.5,0.5), mgp=c(2,0.5,0))
plot(j.res$marginals.fix[[1]], type='l', ylab='Density', 
     xlab=expression(beta[0]), lwd=2) 
lines(pp.res$marginals.fix[[1]], lty=2, lwd=2)
abline(v=beta0, col=2)
plot(j.res$marginals.fix[[2]], type='l', ylab='Density', 
     xlab=expression(beta[0]^y), lwd=2) 
lines(u.res$marginals.fix[[1]], lty=3, lwd=5)
abline(v=beta0.y, col=2)
plot(inla.tmarginal(function(x) 1/x, 
                    j.res$marginals.hy[[1]]), lwd=2, 
     type='l', ylab='Density', xlab=expression(sigma[y]^2)) 
lines(inla.tmarginal(function(x) 1/x, 
                     u.res$marginals.hy[[1]]), lwd=5, lty=3)
abline(v=1/prec.y, col=2)
plot(j.res$marginals.hyperpar[[2]], type='l', xlim=c(0,10),
     xlab=expression(sqrt(8)/kappa), ylab='Density', lwd=2)
lines(pp.res$marginals.hyperpar[[1]], lty=2, lwd=2)
lines(u.res$marginals.hyperpar[[2]], lty=3, lwd=5)
abline(v=sqrt(8)/kappa, col=2)
plot(j.res$marginals.hyperpar[[3]], type='l', lwd=2, xlim=c(0,1),
     xlab=expression(sqrt(sigma[x]^2)), ylab='Density')
lines(pp.res$marginals.hyperpar[[2]], lty=2, lwd=2)
lines(u.res$marginals.hyperpar[[3]], lty=3, lwd=5)
abline(v=sigma2x^0.5, col=2)
plot(j.res$marginals.hy[[4]], type='l', 
     xlab=expression(beta), ylab='Density', lwd=2)
abline(v=beta, col=2)
legend('topright', c('True value', 'Joint', 'Only PP', 'Only Y'), 
       col=c(2,1,1,1), lty=c(1,1,2,3), lwd=c(2,2,3,5), bty='n')
@ 
\caption{Posterior marginal distribution for: 
  intercept for the point process $\beta_0$, 
  intercept for the observations $\beta_0^y$, 
  noise variance in the observations $\sigma^2_y$, 
  the practical range parametrized as $\sqrt{8}/\kappa$, 
  the marginal standard deviation for the random field $\sigma^2_x$ 
  and the sharing coefficient $\beta$.}
\end{figure}\label{fig:jpars}
