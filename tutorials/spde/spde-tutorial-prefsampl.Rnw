\chapter{Point process: inference for the log-Cox process}\label{ch:lcox}

\SweaveOpts{prefix.string=figs/prefsampl} 
<<sett,echo=F,results=hide>>=
options(width=75, prompt = " ", continue = "   ")
require(INLA)
require(splancs)
require(deldir)
require(fields)
inla.setOption(inla.call='remote')
@ 

Under the log-Cox model assumption, there is a 
latent Gaussian Random Field (LGRF) and the inference 
can be done using \textbf{\textsf{INLA}} 
\cite{illianetal:2012}. 

A common aproach to fit the log-Cox process is to 
divide the study region into cells, that forms a lattice, 
and count the number of points into each one. 
This counts are modeled using the Poisson likelihood. 

A good approach for inference of log-Cox model is to 
use the SPDE approach instead occurence counts on cells, 
\cite{simpsonetal:2011}. 
There are the main advantages on this approach 
the consideration of the loc-Cox likelihood directly. 

\section{Data simulation} 

The data simulated on this section is used also 
on the Chapter~\ref{ch:prefsampl}. 

In this section we use the \texttt{rLGCP()} function 
from \textbf{\textsf{spatstat}} package to do the simulation. 
By default that function do simulation on window 
over the $(0,1) \times (0,1)$ square. 
We choose to do simulation 
over the $(0,3) \times (0,3)$ square.
<<window>>=
require(spatstat)
win <- owin(c(0,3), c(0,3))
@ 

This function uses the \texttt{GaussRF()} function 
from the \textsf{\textbf{RandomFields}} package. 
The \texttt{rLGCP} uses the \texttt{GaussRF()} function 
to do simulation of the LGRF over a grid on the provided 
window and use it to do the point process simulation. 

There is an internal parameter to control the 
resolution of the grid. We change it to
<<gridres>>=
spatstat.options(npixel=300)
@ 

First we define the model parameter for 
the model is the mean of the LGRF. 
This is directly related to expected number 
of points of the spatial pattern. 
The expected number of points is its 
exponential times the area fo the window. 
We use 
<<>>=
beta0 <- 3
@ 
So, the expected number of points is 
<<n-exp>>=
exp(beta0) * diff(range(win$x)) * diff(range(win$y))
@ 
Is also possible to use a functional, see Chapter~\ref{ch:lcoxcov}.

In the estimation process we use the Matern 
covariance function with $\nu=1$. 
So, here we just fix it on this value. 
The other parameters are the variance and scale
<<>>=
sigma2x <- 0.2;      kappa <- 2
@ 

Doing the simulation
<<simulapp>>=
require(RandomFields)
set.seed(1)
lg.s <- rLGCP('matern', beta0, 
##              c(0, variance=sigma2x, nugget=0, scale=1/kappa, nu=1), win=win)
              var=sigma2x, scale=1/kappa, nu=1, win=win)
@ 

Both, the LGRF and the point pattern, are returned. 
The point pattern locations are 
<<xy>>=
(n <- nrow(xy <- cbind(lg.s$x, lg.s$y)[,2:1]))
@ 

The exponential of simulated values of the LGRF 
are returned as the \texttt{Lambda} attribute of the object. 
We extract the $\Lambda$ and see a summary of the 
$log(\Lambda)$ below
<<>>=
Lam <- attr(lg.s, 'Lambda')
summary(as.vector(rf.s <- log(Lam$v)))
@ 

We can see the simulated LGRF over the grid and the point pattern simulated 
in Figure~\ref{fig:lgrfpp} produced with the following commands 
<<lgrfpp,eval=F>>=
par(mfrow=c(1,1))
require(fields)
image.plot(list(x=Lam$yrow, y=Lam$xcol, z=rf.s), main='log-Lambda', asp=1)
points(xy, pch=19) 
@ 
\setkeys{Gin}{width=0.55\textwidth}
\begin{figure}\centering
<<echo=F,results=hide,fig=TRUE,eps=FALSE,width=5,height=5>>=
<<lgrfpp>>
@ 
\caption{Simulated intensity of the point process (left), 
  simulated point pattern (right).}
\label{fig:lgrfpp}
\end{figure}


\section{Inference} 

Following \cite{simpsonetal:2011} we can estimate the parameters 
of the log-Cox point process model using few command lines. 

\subsection{The mesh and the weights}

To do inference for the log-Cox point process model 
we also need some care on building the mesh and on using it. 

To do inference for the log Cox process, it is not necessarly better 
to have any location points as any of the mesh nodes, as on the 
geostatistical analysis where it helps a bit for the estimation 
of the nugget effect, see~\ref{sec:meshcompare}. 
We just need a mesh that covers the study region. 
So, we use the \code{loc.domain} argument to build the mesh. 

An additional thing is that we ignore the second outer extension and 
we use a small first outer extension. 
This is because it is not necessary to have nodes out of the study region 
when it recieves zero weights (see weight computation below). 
<<mesh>>=
loc.d <- 3*t(matrix(c(0,0,1,0,1,1,0,1,0,0), 2))
(nv <- (mesh <- inla.mesh.2d(loc.d=loc.d, off=.2, max.e=.5, cut=.1))$n) 
@ 
which is visualized at Figure~\ref{fig:ppmesh} with following commands 
<<meshplot,eval=FALSE>>=
par(mar=c(0,0,0,0))
plot(mesh, asp=1, main='')
points(xy, col=4, pch=19); lines(loc.d, col=3)
@ 

\setkeys{Gin}{width=0.5\textwidth}
\begin{figure}\centering
<<echo=F,results=hide,fig=TRUE,eps=FALSE,width=5,height=5>>=
<<meshplot>>
@ 
\caption{Mesh used to inference for the log-Cox process.}
\label{fig:ppmesh}
\end{figure}

Defining the SPDE model 
<<spde>>=
spde <- inla.spde2.matern(mesh=mesh, alpha=2)
@ 

The SPDE approach defines the model on the nodes of the mesh. 
To fit the the log-Cox point process model 
these points are considered the integration points. 
Them, the *relative* area of these poins are considered 
proportional to the expected number of events. 
It means that at the node on the mesh with has the larger edges 
we have larger expected value. 
The \code{diag(spde\$param.inla\$M0)} 
gives this value. 

But, the mesh has nodes out of the domain and we have area 
larger than our domain when we have nodes outer the domain: 
<<>>=
sum(diag(spde$param.inla$M0))
@ 
We can use these values for the nodes on the inner domain and 
with its neighbours also inside the domain. 
For the nodes near the boundary it becomes hard to deal with. 
If it happens to be any node out of the study region and not 
connected to any inside the istudy region, it recieve zero. 
So, this is way we build a mesh without a large outer extension. 

First, we get the Voronoi triangulation for the mesh nodes. 
We can use the \code{deldir} function from package \pkg{deldir}, 
\cite{deldir} to compute it. 
<<deldir,results=hide>>=
require(deldir)
dd <- deldir(mesh$loc[,1], mesh$loc[,2])
@ 

Second, we get the polygons (around each mesh nodes) with
<<tiles>>=
tiles <- tile.list(dd)
@ 
These polygons are defined in a special way. 
For each of the reference points (the mesh nodes in our case), 
the polygon around is defined in a way that any location 
inside this polygon is closer to the respective reference point, 
rather another of the reference points. 
We can see these polygons on Figure~\ref{fig:vppmesh}. 

Third, we get the interection polygons between these polygons 
and the study region polygon, using functions from the 
\pkg{rgeos} package. First, we define a function to convert 
from simple coordinates defining a polygon to the 
\texttt{SpatialPolygons} object 
<<coo2sp>>=
require(rgeos)
coo2sp <- function(coo) {
    n <- nrow(coo)
    if (any(coo[1,]!=coo[n,]))
        coo <- coo[c(1:n,1),]
    SpatialPolygons(list(Polygons(list(Polygon(coo)), '0')))
}
@ 
As an example, we apply it to coordinates that defines the domain area 
and compute its area
<<gArea>>=
gArea(pl.study <- coo2sp(loc.d))
@ 
and them compute the intersection areas with 
<<pols>>=
sum(w <- sapply(tiles, function(p) {
    pl <- coo2sp(cbind(p$x, p$y))
    if (gIntersects(pl, pl.study))
        return(gArea(gIntersection(pl, pl.study)))
    else return(0)
}))
@ 
and we have some points without any weights (the red ones on Figure~\ref{fig:vppmesh}). 
<<wsummary>>=
table(w>0)
@ 

At Figure~\ref{fig:vppmesh} we can see the Voronoi polygons for the mesh nodes. 
We can see that the polygons around some mesh does not intersect the domain study area.
<<vppmesh>>=
par(mar=c(2,2,1,1), mgp=2:0)
plot(mesh$loc, asp=1, col=(w==0)+1, pch=19, xlab='', ylab='') 
for (i in 1:length(tiles)) 
    lines(c(tiles[[i]]$x, tiles[[i]]$x[1]), c(tiles[[i]]$y, tiles[[i]]$y[1]))
lines(loc.d, col=3)
@ 

\setkeys{Gin}{width=0.5\textwidth}
\begin{figure}\centering
<<echo=F,results=hide,fig=TRUE,eps=FALSE,width=5,height=5>>=
<<vppmesh>>
@ 
\caption{Voronoy polygons for the mesh used to inference for the log-Cox process.}
\label{fig:vppmesh}
\end{figure}

\subsection{The data and projector matrices}

This vector is just what we need to use as the exposure (expected) 
for the Poisson likelihood and is related to the augumented data that we need 
to fit using the Poisson likelihood. 
We can specify that the first observations (number of nodes)
are zero and the last are ones (number of events). 
<<y01>>=
y.pp <- rep(0:1, c(nv, n))
@ 
So, the expected vector can be defined by 
<<expected>>=
e.pp <- c(w, rep(0, n)) 
@ 

We must have to define the projector matrix 
to do inference using the SPDE approach, \cite{lindgren:2012}. 
For the observed points locations we have 
<<Aloc>>=
lmat <- inla.spde.make.A(mesh, xy)
@ 
We need also a projector matrix for 
the integration points and this is 
just a diagonal matrix because 
this locations are just the mesh vertices. 
<<pp-proj>>=
imat <- Diagonal(nv, rep(1, nv))
@
So, the entire projector matrix is 
<<App>>=
A.pp <- rBind(imat, lmat)
@ 

The data stack can be made by 
<<stkpp>>=
stk.pp <- inla.stack(data=list(y=y.pp, e=e.pp), 
                     A=list(1,A.pp), tag='pp',
                     effects=list(list(b0=rep(1,nv+n)), list(i=1:nv))) 
@ 

\subsection{Posterior marginals}

The posterior marginals for the parameters 
of the log-Cox model (on the SPDE approach scale) 
and for the latent random field at integration 
and location points are obtained by 
<<ppest>>=
pp.res <- inla(y ~ 0 + b0 + f(i, model=spde), 
               family='poisson', data=inla.stack.data(stk.pp), 
               control.predictor=list(A=inla.stack.A(stk.pp)), 
               E=inla.stack.data(stk.pp)$e)
@ 

To get the model parameters on the user-scale paramenters
such as the scale $\kappa$, nominal variance $\sigma^2_x$ 
and nominal range we use the \texttt{inla.spde2.result()} function 
<<insp-u,eval=TRUE>>=
pp.rf <- inla.spde2.result(pp.res, 'i', spde)
@ 

The posterior distribution of the log-Cox model 
parameters are visualized on the Figure~\ref{fig:pppost}. 
<<pp-viz,eval=F>>=
par(mfrow=c(2,2), mar=c(3,3,1,0.3), mgp=c(2,1,0)) 
plot(pp.res$marginals.fix[[1]], type='l', 
     xlab=expression(beta[0]), ylab='Density')
abline(v=beta0, col=2)
plot(pp.rf$marginals.variance.nominal[[1]], type='l', 
     xlab=expression(sigma^2), ylab='Density')
abline(v=sigma2x, col=2)
plot(pp.rf$marginals.kappa[[1]], type='l', 
     xlab=expression(kappa), ylab='Density')
abline(v=kappa, col=2)
plot(pp.rf$marginals.range.nominal[[1]], type='l', 
     xlab='Nominal range', ylab='Density')
abline(v=sqrt(8*1)/kappa, col=2)
@ 
\setkeys{Gin}{width=0.75\textwidth}
\begin{figure}\centering
<<echo=F,results=hide,fig=TRUE,eps=FALSE,width=7.5,height=7.5>>=
<<pp-viz>>
@ 
\caption{Posterior distribution for the parameters of 
  the log-Cox model $\sigma^2$ (left), $\kappa$ (mid) 
  and the nominal range  (right)} 
\label{fig:pppost}
\end{figure}


\chapter{Including a covariate on the log-Cox process}\label{ch:lcoxcov}

In Chapter~\ref{ch:lcox} we have done simulation considering 
the underline intensity as just the exponential of a realization 
of a Gaussian random field. 
In this chapter we consider that we have an additional effect, 
which is treated as a covariate. 
In order to fit the model, it is needed the covariate value everywhere, 
at the location points and at the integration points. 

\section{Covariate everywhere}

The simulation is done considering that the covariate effect is available 
at the same grid points where the Gaussian process is simulated. 
So, first we create an artificial covariate at the grid 
<<gridcov>>=
y0 <- x0 <- seq(win$xrange[1], win$xrange[2], 
                length=spatstat.options()$npixel)
gridcov <- outer(x0, y0, function(x,y) cos(x) - sin(y-2))
@ 
Now, the expected number of points is function of the covariate 
<<n-exp-cov>>=
beta1 <- -0.5
sum(exp(beta0 + beta1*gridcov) * diff(x0[1:2])*diff(y0[1:2]))
@ 

Doing the simulation
<<simulappc>>=
set.seed(1)
lg.s.c <- rLGCP('matern', im(beta0 + beta1*gridcov, xcol=x0, yrow=y0), 
##                c(0, variance=sigma2x, nugget=0, scale=1/kappa, nu=1), win=win)
              var=sigma2x, scale=1/kappa, nu=1, win=win)
@ 

Both, the LGRF and the point pattern, are returned. 
The point pattern locations are 
<<xyc>>=
(n.c <- nrow(xy.c <- cbind(lg.s.c$x, lg.s.c$y)[,2:1]))
@ 

We can see the covariate values and the simulated LGRF over the grid 
in Figure~\ref{fig:lgrfppc} with the following commands
<<lgrfppc,eval=FALSE>>=
require(fields)
par(mfrow=c(1,2), mar=c(2,2,1,1), mgp=c(1,0.5,0))
image.plot(list(x=x0, y=y0, z=gridcov), main='Covariate', asp=1)
image.plot(list(x=x0, y=y0, z=log(attr(lg.s.c, 'Lambda')$v)), 
           main='log-Lambda', asp=1)
points(xy.c, pch=19)
@ 

\setkeys{Gin}{width=0.99\textwidth}
\begin{figure}\centering
<<echo=F,results=hide,fig=TRUE,eps=FALSE,width=10,height=5>>=
<<lgrfppc>>
@ 
\caption{Covariate (left), simulated intensity of the point process (mid), 
  simulated point pattern (right).}
\label{fig:lgrfppc}
\end{figure}


\section{Inference} 

We have to include the covariate values to do the inference. 
We need to collect it at the point pattern locations and at the 
mesh nodes from the grid. 

We collect the covariate with the command below
<<collcovar>>=
covariate = gridcov[Reduce('cbind', nearest.pixel(
    c(mesh$loc[,1], xy.c[,1]), c(mesh$loc[,2], xy.c[,2]), 
    im(gridcov, x0, y0)))]
@ 

The augumented response data is created in same way as before. 
<<datc>>=
y.pp.c <- rep(0:1, c(nv, n.c))
e.pp.c <- c(w, rep(0, n.c))
@ 

The projector matrix for the observed points locations 
<<A.c>>=
lmat.c <- inla.spde.make.A(mesh, xy.c)
@ 
The entire projector matrix, using the previous for the 
integration points, is
<<App.c>>=
A.pp.c <- rBind(imat, lmat.c)
@ 

The data stack is 
<<stkpp.c>>=
stk.pp.c <- inla.stack(data=list(y=y.pp.c, e=e.pp.c), 
                       A=list(1, A.pp.c), tag='pp.c',
                       effects=list(list(b0=1, covariate=covariate), 
                           list(i=1:nv)))
@ 

The model is fitted by 
<<ppest.c>>=
pp.c.res <- inla(y ~ 0 + b0 + covariate + f(i, model=spde), 
                 family='poisson', data=inla.stack.data(stk.pp.c), 
                 control.predictor=list(A=inla.stack.A(stk.pp.c)), 
                 E=inla.stack.data(stk.pp.c)$e)
@ 

Getting the model parameters on the user-scale 
<<insp-u.c,eval=TRUE>>=
pp.c.rf <- inla.spde2.result(pp.c.res, 'i', spde)
@ 

The posterior distribution of the log-Cox model 
parameters are visualized on the Figure~\ref{fig:pppostc}. 

<<ppc-viz,eval=F>>=
par(mfrow=c(2,2), mar=c(3,3,1,0.3), mgp=c(2,1,0)) 
plot(pp.c.res$marginals.fix[[1]], type='l', ylab='Density', 
     xlab=expression(beta[0])); abline(v=beta0, col=2)
plot(pp.c.res$marginals.fix[[2]], type='l', ylab='Density', 
     xlab=expression(beta[1])); abline(v=beta1, col=2)
plot(pp.rf$marginals.variance.nominal[[1]], type='l', ylab='Density', 
     xlab=expression(sigma^2)); abline(v=sigma2x, col=2)
plot(pp.rf$marginals.kappa[[1]], type='l', ylab='Density',
     xlab=expression(kappa)); abline(v=kappa, col=2)
@ 
\setkeys{Gin}{width=0.75\textwidth}
\begin{figure}\centering
<<echo=F,results=hide,fig=TRUE,eps=FALSE,width=7.5,height=7.5>>=
<<ppc-viz>>
@ 
\caption{Posterior distribution for the intercept (top left), 
  coefficient of the covariate (top right) and the parameters of 
  the log-Cox model $\sigma^2$ (bottom left), $\kappa$ (bottom right)} 
\label{fig:pppostc}
\end{figure}


\chapter{Geostatistical inference under 
  preferential sampling}\label{ch:prefsampl}

In some cases the effort on sampling depends on the response. 
For example, is more common to have stations collecting data 
about pollution on industrial area than on rural ones. 
To make inference in this case, we can test if we 
have a preferential sampling problem in our data. 
One approach is to build a joint model considering a 
log-Cox model for the point pattern (the locations) 
and the response, \cite{diggleetal:2010}. 
So, we need also to make inference for a 
point process model jointly. 

An illustration of the use \textbf{\textsf{INLA}} for the 
preferential sampling problem is on the case studies section 
of the \textbf{\textsf{INLA}} web page, precisely on 
\url{http://www.r-inla.org/examples/case-studies/diggle09}. 
This example uses the two dimentional random walk model 
for the latent random field. 
Here, we show geoestatistical inference under preferencial 
sampling using SPDE. 

We use the values of the LGRF simulated on the 
Chapter~\ref{ch:lcox} to define the response. 
We just take the values of closest grid centers 
to each location of the point pattern. 
The values of the LGRF is collected (and a summary) 
at closest grid centers with 
<<resp>>=
summary(z <- log(t(Lam$v)[Reduce(
  'cbind', nearest.pixel(xy[,1], xy[,2], Lam))]))
@ 

These values are the latent field with zero mean plus the defined intercept.
We define the response as a different intercept $\beta_y$ and multiply the 
zero mean random field with a $1/\beta$, where $\beta$ is the parameter 
as the sharing parameter between the intensity 
of the point process locations and the response. 
Considering $\beta<0$, it means that the response values is inversly 
proportional to the points density.  
<<resp>>=
beta0.y <- 10;   beta <- -1;   prec.y <- 16
set.seed(2)
summary(resp <- beta0.y + (z-beta0)/beta + 
        rnorm(length(z), 0, sqrt(1/prec.y)))
@ 

\section{Fitting the usual model}

Here, we just fit the geoestatistical model using the 
usual approach. In this approach we just use the locations 
as fixed. We use the mesh of the previous Chapter. 

<<rresp>>= 
stk.u <- inla.stack(data=list(y=resp), A=list(lmat, 1), 
                    effects=list(i=1:nv, b0=rep(1,length(resp))))
u.res <- inla(y ~ 0 + b0 + f(i, model=spde), 
              data=inla.stack.data(stk.u), 
              control.predictor=list(A=inla.stack.A(stk.u)))
round(cbind(True=c(beta0y=beta0.y, prec.y=prec.y), 
            rbind(u.res$summary.fix[, 1:6], u.res$summary.hy[1,])), 4)
@ 

We have to build also the posterior marginals on the
user-scale paramenters: scale, nominal variance and nominal range
<<rfuparams>>=
u.rf <- inla.spde2.result(u.res, 'i', spde)
@ 

We can see the marginals posterior distributions 
for the model parameters in the Figure~\ref{fig:upost}, 
produced with the following code
<<insp-u,eval=F>>=
par(mfrow=c(1,3), mar=c(3, 3, 0.3, 0.3), mgp=c(2,1,0))
plot(u.rf$marginals.variance.nominal[[1]], type='l', ylab='Density',
     xlab=expression(sigma^2)); abline(v=sigma2x, col=2)
plot(u.rf$marginals.kappa[[1]], type='l', ylab='Density', 
     xlab=expression(kappa)); abline(v=kappa, col=2)
plot(u.rf$marginals.range.nominal[[1]], type='l', 
     xlab='Nominal range', ylab='Density')
abline(v=sqrt(8*1)/kappa, col=2)
@ 
\setkeys{Gin}{width=0.99\textwidth}
\begin{figure}\centering
<<echo=F,results=hide,fig=TRUE,eps=FALSE,width=6,height=2.1>>=
<<insp-u>>
@ 
\caption{Posterior distribution for $\sigma^2$, 
  $\kappa$ and the nominal range just using the response.}
\label{fig:upost}
\end{figure}

\section{Model fitting under preferential sampling}

In this situation we fit the model where a LGRF is 
considered to model both point pattern and the response. 
Using \textbf{\textsf{INLA}} it can be done using two likelihoods, 
one for the point pattern and another for the response. 
To do it we need a matrix response and a new index set 
to specify the model for the LGRF. 
It is more easy by using the \texttt{inla.stack()} 
following previous examples for two likelihood models. 

We consider the point pattern 'observation' on the first 
column and the response values on the second column. 
So, we just redefine the stack for the 
response and also for the point process. 
We put the response on the first column 
and the Poisson data for the point process 
as the second column. 
Also, to avoid the expected number of cases as NA 
for the Poisson likelihood, we set it as zero 
on the response data stack. 
For the SPDE effect on the point process part 
we have to model it as a copy of the SPDE 
effect at response part. 
We do it by defining a index set with different name 
and use it on the copy feature later. 
<<ppstk>>=
stk2.y <- inla.stack(data=list(y=cbind(resp,NA), e=rep(0,n)), 
                     A=list(lmat, 1), tag='resp2',
                     effects=list(i=1:nv, b0.y=rep(1,n)))
stk2.pp <- inla.stack(data=list(y=cbind(NA,y.pp), e=e.pp), 
                      A=list(A.pp, 1), tag='pp2',
                      effects=list(j=1:nv, b0.pp=rep(1,nv+n))) 
j.stk <- inla.stack(stk2.y, stk2.pp)
@ 

Now, we fit the geostatistical model under preferential sampling. 
To put the LGRF on both likelihood, we have to use the copy 
strategy. 
<<j-res>>=
jform <- y ~ 0 + b0.pp + b0.y + 
    f(i, model=spde) + f(j, copy='i', fixed=FALSE)
j.res <- inla(jform, family=c('gaussian', 'poisson'), 
              data=inla.stack.data(j.stk), E=inla.stack.data(j.stk)$e,              
              control.predictor=list(A=inla.stack.A(j.stk)))
round(cbind(True=c(beta0, beta0.y), 
            j.res$summary.fix), 4)
@ 

Computing the marginals posterior distributions 
in user-scale random field parameters:
<<psrf>>=
j.rf <- inla.spde2.result(j.res, 'i', spde)
@ 
We can visualize these posterior marginal distributions 
in Figure~\ref{fig:jpars}. 
The negative signal to the copy parameter $\beta$ is due to the 
fact that we define the response with opposite signal of the 
latent field used to do the simulation of the point process. 

\setkeys{Gin}{width=0.7\textwidth}
\begin{figure}\centering
<<pppost,echo=F,results=hide,fig=TRUE,eps=FALSE,width=5.5,height=4>>=
par(mfrow=c(2,2), mar=c(3,3,0.5,0.5), mgp=c(2,1,0))
plot(j.res$marginals.hy[[length(j.res$marginals.hy)]], type='l', 
     xlab=expression(beta), ylab='Density', col=4)
abline(v=beta, col=2, lty=2)
plot(j.rf$marginals.variance.nominal[[1]], type='l', 
     xlim=range(j.rf$marginals.variance[[1]][,1], 
         u.rf$marginals.variance[[1]][,1]), 
     ylim=range(j.rf$marginals.variance[[1]][,2], 
         u.rf$marginals.variance[[1]][,2]), 
     xlab=expression(sigma^2), ylab='Density', col=4) 
lines(u.rf$marginals.variance.nominal[[1]], col=3) 
abline(v=sigma2x, col=2, lty=2) 
plot(j.rf$marginals.kappa[[1]], type='l', 
     xlim=range(j.rf$marginals.kappa[[1]][,1], u.rf$marginals.kappa[[1]][,1]), 
     ylim=range(j.rf$marginals.kappa[[1]][,2], u.rf$marginals.kappa[[1]][,2]), 
     xlab=expression(kappa), ylab='Density', col=4)
lines(u.rf$marginals.kappa[[1]], col=3)
abline(v=kappa, col=2, lty=2)
plot(j.rf$marginals.range.nominal[[1]], type='l', 
     xlim=range(j.rf$marginals.range.nominal[[1]][,1], u.rf$marginals.range.nominal[[1]][,1]), 
     ylim=range(j.rf$marginals.range.nominal[[1]][,2], u.rf$marginals.range.nominal[[1]][,2]), 
     xlab='Nominal range', ylab='Density', col=4)
lines(u.rf$marginals.range.nominal[[1]], col=3)
abline(v=sqrt(8*1)/kappa, col=2, lty=2)
legend('topright', c('True value', 'Usual model', 'Pref_Sampl'), 
       col=c(2,3,4), lty=1, bty='n')
@ 
\caption{Posterior marginal distribution for $\beta_0$, $\sigma^2$, 
  $\kappa$ and the nominal range under preferential sampling.}
\label{fig:jpars}
\end{figure}
