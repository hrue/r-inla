\chapter{Dynamic regression example}\label{ch:dynamic}

\SweaveOpts{prefix.string=figs/dynamic} 
<<opts,echo=F,results=hide>>=
options(width=75, prompt = " ", continue = "   ")
require(INLA) 
lcall <- inla.getOption('inla.call') 
inla.setOption(inla.call='remote')
inla.setOption(num.threads=7)
@ 

There is a large literature about dynamic models 
with also some books about it, from \cite{westH:1997} 
to \cite{PetrisPC:2009}. 
These models basically defines an hierarchical 
framework for a class of time series models. 
A particular case is the dynamic regression model, 
were the regression coefficients are modeled as time series. 
That is the case when the regression coefficients 
vary smoothly over time.

\section{Dynamic space-time regression}

The specific class of models for spatially 
structured time series was proposed by \cite{gelfandKSB:2003}, 
where the regression coefficients varies smoothly over time and space. 
For the areal data case, the use of proper Gaussian Markov random fields (PGMRF) 
over space as proposed by \cite{vivarF:2009}. 
There exists a particular class of such models called 
``spatially varying coefficient models'' 
were the regression coefficients veries over space, 
\cite{assuncaoGA:1999}, \cite{assuncaoPC:2002}, \cite{gamermanMR:2003}. 

In \cite{gelfandKSB:2003} the Gibbs sampler were used for inference and 
it was claimed that better algorithms is needed due to strong autocorrelations. 
In \cite{vivarF:2009} the use of forward information 
filtering and backward sampling (FIFBS) recursions were proposed. 
Both MCMC algorithms are computationally expensive. 

One can avoid the FFBS algorithm as a relation between the Kalman-filter 
and the Cholesky factorization is provided in \cite{knorrheldR:2002}. 
The Cholesky fator  is more general and has superior performance 
when using sparse matrix methods, \cite[p. 149]{RueHeld:2005}.  
Additionally, the restriction that the prior for the latent field has to be proper can be avoided. 

When the likelihood is Gaussian, there is no approximation needed in the inference process since 
the distribution of the latent field given the data and the hyperparameters is Gaussian. 
So, the main task is to perform inference for the hyperparameters in the model. 
For this, the mode and curvature around can be found without any sampling method. 
For the class of models in~\cite{vivarF:2009} it is natural to use INLA, 
as shown in \cite{cardenasKR:2012}, and for the models in~\cite{gelfandKSB:2003} 
we can use the SPDE approach when considering the Mat\'ern covariance for the spatial part. 

In this section we show how to fit the space-time dynamic regression model 
as in \cite{gelfandKSB:2003}, considering the Mat\'ern spatial covariance and 
the AR(1) model for time which corresponds to the exponential correlation function. 
This particular covariance choise correspond to the model in 
\cite{cameletti:2012}, where only the intercept is dynamic. 
Here, we show the case when we have a dynamic intercept and 
a dynamic regression coefficient for an harmonic over time. 

\section{Simulation from the model}

We can start on defining the spatial locations:
<<coom>>=
n <- 100;   set.seed(1);  coo <- matrix(runif(2*n), n)
@ 

To sample from a random field on a set of location, 
we use the following function 
<<rrffun>>=
rrf <- function(n, coords, kappa, variance, nu=1) {
    m <- as.matrix(dist(coords))
    m <- exp((1-nu)*log(2) + nu*log(kappa*m)-
             lgamma(nu))*besselK(m*kappa, nu)
    diag(m) <- 1
    chol(variance*m)%*%matrix(rnorm(nrow(coords)*n), ncol=n)
}
@ 

We use the above function to draw $k$ (number of time points) samples from the random field. 
Then, we make it temporally correlated considering the time autoregression 
<<sample>>=
kappa <- c(10, 8);    sigma2 <- c(1/2, 1/3)
k <- 20;  rho <- c(0.9, 0.7) 
beta0 <- rrf(k, coo, kappa[1], sigma2[1]) * (1-rho[1]^2)
beta1 <- rrf(k, coo, kappa[2], sigma2[2]) * (1-rho[2]^2)
for (j in 2:k) {
    beta0[, j] <- beta0[,j-1]*rho[1] + beta0[,j]
    beta1[, j] <- beta1[,j-1]*rho[2] + beta1[,j]
}
@ 
where the $(1-\rho_j^2)$ term is in accord to the 
parametrization of the AR($1$) model in INLA.

To get the response, we define the harmonic as a function over time, 
compute the mean and add an error term 
<<response>>=
hh <- rep(sin(2 * (2*pi) * (1:k/k)), each=n) ### just two cycles
mu.beta <- c(-5, 1);   taue <- 5
length(y <- (mu.beta[1] + beta0) + (mu.beta[2]+beta1)*hh + ### dynamic regression part
           rnorm(n*k, 0, sqrt(1/taue))) ### error in the observation
@ 

\section{Fitting the model}

We have two space-time terms on the model, 
each one with three hyperparameters: 
precision, spatial scale, temporal scale (or temporal correlation). 
So, considering the likelihood, $7$ hyperparameters in total. 
To perform fast inference, we choose to have a crude mesh with 
with small number of vertices. 
<<spde>>=
(mesh <- inla.mesh.2d(coo, max.edge=0.25, offset=0.05, cutoff=0.05))$n
spde <- inla.spde2.matern(mesh)
@  

We do need one set of index for each call of the \texttt{f()} 
function, no matter if they are the same, so:
<<idx>>=
i0 <- inla.spde.make.index('i0', spde$n.spde, n.group=k)
i1 <- inla.spde.make.index('i1', spde$n.spde, n.group=k)
@ 

In the SPDE approach, the space-time model is defined in a set of mesh nodes. 
As we have considered continuous time, it is also defined on a set of time knots. 
So, we have to deal with the projection from the model domain (nodes, knots) to 
the space-time data locations. 
For the intercept it is the same way as in the other examples. 
For the regression coefficients, we need to account for the covariate value in the projection matrix. 
It can be seen as follows 
\begin{eqnarray}
\boldeta & = & \mu_{\beta_0} + \mu_{\beta_2}\mb{h} + 
\mb{A} \mb{\beta}_0 + (\mb{A} \mb{\beta}_1) \mb{h} \nonumber \\
 & = & \mu_{\beta_0} + \mu_{\beta_1}\mb{h} + 
 \mb{A} \mb{\beta}_0 + (\mb{A} \oplus (\mb{h}\mb{1}^{'}))\mb{\beta}_1 \label{eq:dynspde}
\end{eqnarray}
where $\mb{A} \oplus (\mb{h} \mb{1}^{'})$ is the 
row-wise Kronecker product between $\mb{A}$ and 
a the vector $\mb{h}$ (with length equal the number of rows in $\mb{A}$) 
expressed as the Kronecker sum of $\mb{A}$ and $\mb{h}\mb{1}^{1}$. 
This operation can be performed usind the \texttt{inla.row.kron()} function 
and is done internally in the function \texttt{inla.spde.make.A()} 
when supplying a vector in the \texttt{weights} argument.

The space-time projector matrix $\mb{A}$ is defined as follows:
<<spdebuild>>=
A0 <- inla.spde.make.A(mesh, cbind(rep(coo[,1], k), rep(coo[,2], k)),
                       group=rep(1:k, each=n))
A1 <- inla.spde.make.A(mesh, cbind(rep(coo[,1], k), rep(coo[,2], k)),
                       group=rep(1:k, each=n), weights=hh)
@ 

The data stack is as follows
<<stky>>=
stk.y <- inla.stack(data=list(y=as.vector(y)), tag='y', 
                    A=list(A0, A1, 1), 
                    effects=list(i0, i1, 
                        data.frame(mu1=1, h=hh)))
@ 
where \code{i0} is similar to \code{i1} and 
the elements \code{mu1} and \texttt{h} in the second element of the effects \texttt{data.frame} is for $\bmu_{\xi}$.  

The formula take these things into account 
<<formula>>=
form <- y ~ 0 + mu1 + h + ### to fit mu_beta
    f(i0, model=spde, group=i0.group, control.group=list(model='ar1')) + 
        f(i1, model=spde, group=i1.group, control.group=list(model='ar1'))
@

As we have Gaussian likelihood there is no approximation in the fitting process. 
The first step of the INLA algorithm is the optimization to find the mode of the $7$ hyperparameters in the model. 
This step takes around few minutes to fit, to make around 600 posterior evaluations. 
The integration step is performed at 79 hyperparameter configurations using the CCD strategy. 
By choosing good starting values it will be needed less iteractions in this optimization process. 
Below, we define starting values for the hyperparameters in the internal scale considering the values used to simute the data
<<theta>>=
(theta.ini <- c(log(taue), 
                -log(4*pi*sigma2[1]*kappa[1]^2)/2, log(kappa[1]), 
                log((1+rho[1])/(1-rho[1])), 
                -log(4*pi*sigma2[2]*kappa[2]^2)/2, log(kappa[2]), 
                log((1+rho[2])/(1-rho[2]))))
@ 

Fitting the model considering the initial values defined above
<<fittingdyn3>>=
(res <- inla(form, family='gaussian', data=inla.stack.data(stk.y), 
            control.predictor=list(A=inla.stack.A(stk.y)),
            control.mode=list(theta=theta.ini, restart=TRUE)))$cpu
@ 

Summary of the $\mu_{\beta}$:
<<summarymux>>=
round(cbind(true=mu.beta, res$summary.fix), 4)
@ 

The results for the random fields
<<rfres>>=
rf12 <- list(rf1=inla.spde2.result(res, 'i0', spde),
             rf2=inla.spde2.result(res, 'i1', spde))
@ 

We can see the posterior marginal distributions for the 
model hyperparameters and the range for each 
spatio-temporal process in Figure~\ref{fig:hd3pmds}. 
<<hd3pmds, echo=TRUE, eval=FALSE>>=
par(mfrow=c(3, 3), mar=c(2.5,2.5,0.3,0.3), mgp=c(1.5,0.5,0)) 
for (i in 1:2) for (j in 15:17) {
        plot(rf12[[i]][[j]][[1]], type='l', ylab='Density', 
             xlab=paste('rf', i, '.', substring(names(rf12[[i]])[j],11), sep=''))
        abline(v=c(kappa[i], sigma2[i], sqrt(8*1)/kappa[i])[j-14], col=4)
    }
for (i in 1:2) {
    plot(res$marginals.hy[[c(3,6)[i]]], type='l', 
         xlab=bquote(rho[.(i)]), ylab='Density') 
    abline(v=rho[i], col=4)
}
plot(res$marginals.hy[[1]], type='l', xlab=expression(tau[E]), ylab='Density') 
abline(v=taue, col=4) 
@ 
\setkeys{Gin}{width=0.99\textwidth}
\begin{figure}\centering
<<hd3pmdsf, eval=TRUE, echo=FALSE, fig=T,eps=F, width=7.5, height=5>>=
<<hd3pmds>>
@ 
\caption{Posterior marginal distributions for the 
  dynamic continuous spatial model hyperparameters} 
\label{fig:hd3pmds}\end{figure}

%We can have a look over the posterior mean of the dynamic coefficients. 
%We compute the correlation between the simulated and the posterior mean ones by
%<<betas>>=
%c(beta0=cor(as.vector(beta0), drop(A0%*%res$summary.ran$i0$mean)), 
%  beta1=cor(as.vector(beta1), 
%      drop(A0%*%res$summary.ran$i1$mean))) ## using A0 to account only for the coeff.
%@ 
