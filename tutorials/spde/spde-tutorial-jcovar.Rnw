\chapter{Joint modeling a covariate with misalignment}\label{ch:jcovar}

\SweaveOpts{prefix.string=figs/jcovar} 
\setkeys{Gin}{width=0.99\textwidth}
<<sett,echo=F,results=hide>>=
options(width=75, prompt = " ", continue = "   ")
require(INLA)
lcall <- inla.getOption('inla.call')
inla.setOption(inla.call='remote')
inla.setOption(num.threads=7)
require(geoR)
@ 
This chapter is better presented in the
last chapter of~\cite{blangiardoC:2015}.

Here we focus on the situation when we 
have a response $y$ and a covariate $c$. 
But we have misalignment, i.e., we have 
$y$ observed at $n_y$ locations and 
$x$ observed at $n_c$ locations. 
In this example, we design a soluction that 
not depends if we have or not some common 
observed locations for $c$ and $y$. 

A restriction is the assumption that 
$c$ have spatial dependency. 
This restriction is made because 
we want a good model to predict 
$c$ at locations of $y$. 
So, the goodness of prediction is 
proportional to the spatial dependency. 

\section{The model}

Taking into acount that $c$ have spatial 
dependency, a simple approach is to define 
a model for $c$, predict it on locations 
that we have $y$ and build the model for $y$. 
But, in this two stage model, we don't 
take into account the prediction 
error of $c$ on second model. 
The measurement error models is an 
approach to solve similar problems, 
\cite{muffetal:2013}. 
But, here we are able to consider the 
spatial dependency on $c$. 
So we build a spatial model for $c$ 
and another spatial model for $y$, 
and do the estimation process jointly. 

We consider the following likelihood for $c$ 
\[
c_i \sim N(m_i, \sigma_c^2)
\]
where $m_i$ is modeled as a random field and 
$\sigma_c$ is a measurement error of $c$. 
So, when we predict $c$ at location $s$, 
we have $m(s)$. 

Let following model for $y$ 
\[
y_i \sim N(\alpha_y + \beta c_i + x_i, \sigma_y^2)
\]
where $\alpha_y$ is an intercept, $\beta$ 
is the regression coefficient on $c$, 
$c_i$ is the covariate at location of $y_i$, 
$x_i$ is an zero mean random field 
and $\sigma_y^2$ measures the error 
that remain unexplained on $Y$. 

A particular case is when we don't have 
the $x$ term in the model for $y$. 
Another case, is when $\sigma_c^2=0$ and 
we don't have white noise in the covariate, 
i. e., the covariate is considered 
just a realization of a random field. 

\subsection{Simulation from the model}

To test the approach on next section, 
we do a simulation from this model. 
First, we set the model parameters. 
<<params>>=
n.y <- 123;         n.c <- 234
alpha.y <- -5;      beta <- -3
sigma2.y <- 0.5;    sigma2.c <- 0.2
@ 

First, we do the simulation of the locations 
<<simloc>>=
set.seed(1)
loc.c <- cbind(runif(n.c), runif(n.c))
loc.y <- cbind(runif(n.y), runif(n.y))
@ 

Let the parameters of both random fields $m$ and $x$:
<<rfparms>>=
kappa.m <- 3;          sigma2.m <- 5
kappa.x <- 7;          sigma2.x <- 3
@ 
and we use the \code{grf()} function of the \pkg{geoR}, 
\cite{geoR}, package to do the simulation of both random fields. 
We need the simulation of $m$ in both set of locations
<<simula>>=
library(geoR)
set.seed(2)
mm <- grf(grid=rbind(loc.c, loc.y), messages=FALSE, 
          cov.pars=c(sigma2.m, 1/kappa.m), kappa=1)$data
xx <- grf(grid=loc.y, messages=FALSE, 
          cov.pars=c(sigma2.x, 1/kappa.x), kappa=1)$data
@ 
and simulate the covariate and the response with 
<<sim-y>>=
set.seed(3)
cc <- mm + rnorm(n.c+n.y, 0, sqrt(sigma2.c))
yy <- alpha.y + beta*mm[n.c+1:n.y] + xx + 
  rnorm(n.y, 0, sqrt(sigma2.y))
@ 

\section{Fitting the model}

First we make the mesh 
<<mesh>>=
pl01 <- matrix(c(0,1,1,0,0, 0,0,1,1,0), ncol=2)
(mesh <- inla.mesh.create.helper(, pl01, cutoff=.05, 
                                 max.edge=c(.1,.2)))$n
@ 
and use it for both the random fields. 
So, the both index set based on the mesh 
have the same values. 

We do simulations of the covariate 
on the locations of the response 
just to simulate the response. 
But, in the problem that we want 
to solve in pratice, we don't have 
the covariate on the response locations. 
The misalignment implies in different 
predictor matrix for response and covariate. 
<<Apred>>=
Ac <- inla.spde.make.A(mesh, loc=loc.c)
Ay <- inla.spde.make.A(mesh, loc=loc.y)
@ 

To predict the covariate jointly, we need 
to model it jointly and we need two likelihoods. 
So, the response is formed by two colums matrix. 
The data stack can be obtained by 
<<dat>>=
stk.c <- inla.stack(data=list(y=cbind(cc[1:n.c], NA)), 
                    A=list(Ac), tag='dat.c', 
                    effects=list(m=1:mesh$n))
stk.y <- inla.stack(data=list(y=cbind(NA, yy)), 
                    A=list(Ay, 1), 
                    effects=list(list(c.y=1:mesh$n, x=1:mesh$n), 
                      list(a.y=rep(1,length(yy)))))
stk <- inla.stack(stk.c, stk.y)
@ 

The estimation of the regression coefficient 
in this approach is treated as a hyperparameter, 
such as copy parameter of an latent field. 
In this case, we need to do a good prior specification. 
For example, is possible to know, a priori, the signal. 
We set a $N(-3, 25)$ prior to $\beta$. 
Also, we define the formulae for the model. 
<<formula>>=
form <- y ~  0 + f(m, model=spde) + 
  a.y + f(x, model=spde) + f(c.y, copy='m', fixed=FALSE, 
               hyper=list(theta=list(param=c(-3,25), initial=-3))) 
@ 
define the spde model and fit the model with 
<<fit>>=
spde <- inla.spde2.matern(mesh)
res <- inla(form, data=inla.stack.data(stk), family=rep('gaussian',2), 
            control.predictor=list(compute=TRUE, A=inla.stack.A(stk))) 
@ 

\section{The results}

The true values of the intercept and 
the summary of its posterior marginal 
<<resfix>>=
round(cbind(True=alpha.y, res$summary.fix), 4)
@ 

The true values of the precisions and 
the summary of the posterior marginals 
<<reshyl>>=
round(cbind(True=1/c(Prec.c=sigma2.c, Prec.y=sigma2.y), 
            res$summary.hy[1:2,]), 4)
@ 

The true value of the regression coefficient and 
the summary of the posterior marginal 
<<reshyb>>=
round(cbind(True=beta, res$summary.hy[7,]), 4)
@ 

The true values for the precision of the both random 
fields and the summary of the posterior marginals 
<<rf>>=
m.rf <- inla.spde2.result(res, 'm', spde)
x.rf <- inla.spde2.result(res, 'x', spde)
round(cbind(True=c(s2m=sigma2.m, s2x=sigma2.x), 
            mean=c(inla.emarginal(function(x) x, 
              m.rf$marginals.variance.n[[1]]), 
              inla.emarginal(function(x) x, 
                             x.rf$marginals.variance.n[[1]])), 
            rbind(inla.hpdmarginal(.95, m.rf$marginals.variance.n[[1]]), 
                  inla.hpdmarginal(.95, x.rf$marginals.variance.n[[1]]))), 4)
@ 

The true values for the scale parameter $\kappa$ 
the mean of the posterior marginals and the 
95\% HPD credibility interval. 
<<kappa>>=
post.k <- lapply(res$marginals.hy[c(4,6)], function(y) 
                 inla.tmarginal(function(x) exp(x), y))
round(cbind(True=c(kappa.m=kappa.m, kappa.x=kappa.x), 
            t(sapply(post.k, function(x) 
                     c(mean=inla.emarginal(function(y) y, x), 
                       CI=inla.hpdmarginal(.95, x))))), 4)
@ 

We see the posterior distribution of 
likelihood parameters on Figure~\ref{fig:likeparsjcov} 
generated with comands below 
<<likeparsjcov,eval=F>>=
par(mfcol=c(2,2), mar=c(3,3,.1,.1), mgp=c(1.5,.5,0), las=1)
plot(res$marginals.fix[[1]], type='l', 
     xlab=expression(alpha[y]), ylab='')
abline(v=alpha.y, col=4)
plot(res$marginals.hy[[7]], type='l', 
     xlab=expression(beta), ylab='')
abline(v=beta, col=4)
plot.default(inla.tmarginal(function(x) 1/x, res$marginals.hy[[1]]), 
             type='l', xlab=expression(sigma[c]^2), ylab='')
abline(v=sigma2.c, col=4)
plot.default(inla.tmarginal(function(x) 1/x, res$marginals.hy[[2]]), 
             type='l', xlab=expression(sigma[y]^2), ylab='')
abline(v=sigma2.y, col=4)
@ 
\begin{figure}\centering
<<likeparsjcov,echo=F,fig=T,eps=F,width=10,height=4>>=
<<likeparsjcov>>
@ 
\caption{Posterior distribution of the likelihood parameters.}
\label{fig:likeparsjcov}
\end{figure}
We see on the Figure~\ref{fig:likeparsjcov} that 
the posterior distribution covers the true values 
of all the parameters. 

Now we want to see the posterior distribution of 
the both random fields. 
The practical range of the process is
$sqrt(8\nu)/\kappa$, where $\nu=1$ 
on this analisys.
We see these parameters 
on Figure~\ref{fig:rfparsjcov} 
generated with comands below 
<<rfparsjcov,eval=F>>=
par(mfcol=c(2,3), mar=c(3,3,.1,.3), mgp=c(1.5,.5,0), las=1)
plot.default(post.k[[1]], type='l', xlab=expression(kappa[m]), ylab='')
abline(v=kappa.m, col=4)
plot.default(post.k[[2]], type='l', xlab=expression(kappa[x]), ylab='')
abline(v=kappa.x, col=4)
plot.default(m.rf$marginals.variance.n[[1]], type='l', 
             xlab=expression(sigma[m]^2), ylab='')
abline(v=sigma2.m, col=4)
plot.default(x.rf$marginals.variance.n[[1]], type='l', 
             xlab=expression(sigma[x]^2), ylab='')
abline(v=sigma2.x, col=4)
plot.default(m.rf$marginals.range.n[[1]], type='l', 
             xlab='practical range of m', ylab='')
abline(v=sqrt(8)/kappa.m, col=4)
plot.default(x.rf$marginals.range.n[[1]], type='l', 
             xlab='practical range of x', ylab='')
abline(v=sqrt(8)/kappa.x, col=4)
@ 
\begin{figure}\centering
<<vrfparsjcov,echo=F,fig=T,eps=F,width=12,height=4>>=
<<rfparsjcov>>
@ 
\caption{Posterior distribution of all the 
  parameters of both random field.}
\label{fig:rfparsjcov}
\end{figure}
We see on Figure~\ref{fig:rfparsjcov}  
that the posterior marginal distribution 
of the all parameters of both spatial 
process cover the true values well. 

Another intersting result is the 
prediction of the covariate on the 
locations of the response. 
We have the simulated values of $m$ 
on that locations. So, we are able 
to see if the predictions are good. 

The predictor matrix used on the estimation 
proces maps the nodes from mesh vertices 
to the data locations. 
The first lines of the predictor matrix for 
the covariate can be used to access the 
predictions on the locations of the covariate. 
Also, we have the predictor matrix 
used to the response. 
The last lines of this matrix that maps 
the mesh vertices to the response locations. 
Because we have the covariate simulated 
in the both set of locations, we use 
the correspondent parts of both predictor 
matrix to project the posterior mean and 
the posterior variance on the locations. 

We get this matrix by
<<prdmat>>=
mesh2locs <- rBind(Ac, Ay)
@ 
and the posterior mean and posterior standard deviations with 
<<predicted>>=
m.mprd <- drop(mesh2locs%*%res$summary.ran$m$mean)
sd.mprd <- drop(mesh2locs%*%res$summary.ran$m$sd)
@ 

With this aproach for this both posterior 
summary can be an aproximation to 95\% 
credibility interval, with normally supposition.
We see it this results with comands below 
<<mprdplot,eval=F>>=
plot(m.mprd, mm, asp=1, type='n', 
     xlab='Predicted', ylab='Simulated')
segments(m.mprd-2*sd.mprd, mm, m.mprd+2*sd.mprd, mm, 
         lty=2, col=gray(.75))
abline(c(0,1), col=4); points(m.mprd, mm, pch=3, cex=.5)
@ 
\begin{figure}\centering
<<vismprdplot,echo=F,fig=T,eps=F,width=7,height=5>>=
<<mprdplot>>
@ 
\caption{Simulated versus predicted values of $m$ ($+$) 
  and the approximated credibility intervals.}
\label{fig:mprdplot}
\end{figure}
on the Figure~\ref{fig:mprdplot}. 
The blue line represents the situation where 
predicted is equal to simulated. 
We see that the prediction are wery well. 

