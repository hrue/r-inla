\chapter{Measurement error model}\label{ch:me}

\SweaveOpts{prefix.string=figs/me}
\setkeys{Gin}{width=0.99\textwidth}
<<sett,echo=F,results=hide>>=
options(width=75, prompt = " ", continue = "   ")
require(INLA)
loc.call <- inla.getOption('inla.call')
inla.setOption(inla.call='remote')
source('spde-tutorial-functions.R')
@ 

Here we focus on a similar situation than the 
one on Chapter~\ref{ch:jcovar}. 
We also have a response $y$ and a covariate $c$ 
and the misalignment. But, now we have that 
the covariate is observed on 
both $c$ and $y$ locations and also 
has a error measure that is spatially 
structured. 

\section{The model}

We consider the following model $c$ 
\[
c_j = \alpha_c + \beta_w w_j + m_j
\]
where $w_i$ is a covariate. 
Considering a GF for $m$, 
we have here a kind of 
measurement error models, 
\cite{muffetal:2013}, 
where the error is assumed t
o have spatial structure. 

For $y$, we have 
\[
y_i \sim \alpha_y + \beta_c cc_i + x_i + e_i 
\]
where $\alpha_y$ is an intercept, $\beta_c$ 
is the regression coefficient on the 
predicted value for $c$
$x_j$ is an zero mean random field 
and $e_i$ is a error that remains 
unexplained on $y$ such that 
$e_i \~ N(0, \sigma_y^2)$ measures the. 

A particular case is when we don't have 
the $x$ term in the model for $y$. 
Another case, is when $\sigma_c^2=0$ and 
we don't have white noise in the covariate, 
i. e., the covariate is considered 
just a realization of a random field. 

Differently than the joint model on 
Chapter~\ref{ch:jcovar}, here we need 
to define a term that express the linear predictor 
of $c$ and then copy it on the response 
linear predictor. 
We need an extra equation to do it
\[
\omega_j = \alpha_c + \beta_w w_j + m_j
\]
To fit $\omega$ we need o use 
the faked zero observations strategy,
\cite{cardenasKR:2012}. 
So, we rewrite this as 
\[
0 = \alpha_c + \beta_w w_j + m_j - \omega_j
\]
and define a Gaussian likelihood 
with fixed high precision to fit it. 

\section{Simulation from the model}

In this section 
we do a simulation from this model.

First, we do the simulation of the locations 
<<simloc>>=
n.y <- 123;         n.c <- 234
set.seed(1)
loc.c <- cbind(runif(n.c), runif(n.c))
loc.y <- cbind(runif(n.y), runif(n.y))
@ 

Let the parameters of both random fields $m$ and $x$:
<<rfparms>>=
kappa.m <- 7;          sigma2.m <- 3
kappa.x <- 10;         sigma2.x <- 2
@ 
and we use the \code{rspde} function from Chapter~\ref{ch:spde}. 
This function is available in the file at                      
\url{http://www.math.ntnu.no/inla/r-inla.org/tutorials/spde/spde-tutorial-functions.R}.
This function need a mesh to do simulation. 
We can provide the mesh or it is builded automatically. 
We build a mesh to provide taking into account the 
true value of the smaller range process 
<<mesh>>=
maxk <- max(kappa.m, kappa.x)
(mesh <- inla.mesh.2d(rbind(loc.c, loc.y), max.edge=c(1,2)/maxk, 
                      cutoff=0.1/maxk, offset=c(1,2)/maxk))$n
@ 

<<oldcall,echo=F,results=hide>>=
inla.setOption(inla.call=loc.call)
@ 
We need the simulation of $m$ in both set of locations. 
<<simula>>=
set.seed(2)
mm <- rspde(coords=rbind(loc.c, loc.y), 
            kappa=kappa.m, variance=sigma2.m, mesh=mesh)
xx <- rspde(coords=loc.y, kappa=kappa.x, 
            variance=sigma2.x, mesh=mesh)
@ 
and with the following parameters 
<<params>>=
alpha.c <- -5;      beta.w <- 0.5
alpha.y <- 3;       beta.c <- 2
sigma2.y <- 0.3
@ 
we do simulation of the covariate and response with 
<<sim-y>>=
set.seed(3)
w <- runif(n.c + n.y) 
cc <- alpha.c + beta.w * w + mm 
yy <- alpha.y + beta.c*cc[n.c+1:n.y] + xx + 
    rnorm(n.y, 0, sqrt(sigma2.y))
@ 

\section{Fitting the model}

Here we use the same mesh for both 
random fields on the model that is 
the same one used on simulation.
So, the both index set based on the mesh 
have the same values. 

We do simulations of the covariate 
on the locations of the response 
just to simulate the response. 
But, in the problem that we want 
to solve in pratice, we don't have 
the covariate on the response locations. 
The misalignment implies in different 
predictor matrix for response and covariate. 
<<Apred>>=
Ac <- inla.spde.make.A(mesh, loc=loc.c)
Ay <- inla.spde.make.A(mesh, loc=loc.y)
@ 

We have to use three likelihoods. 
One for the response, one for the 
covariate $c$ and one for the 
faked zero observations. 
It is easier to buil one stack 
for each one and join they to fit the model
<<dat>>=
stk.y <- inla.stack(data=list(y=cbind(yy, NA, NA)), 
                    A=list(Ay, 1), 
                    effects=list(x=1:mesh$n, 
                        data.frame(
                            a.y=1, o.c=(n.c+1):(n.c+n.y)))) 
stk.c <- inla.stack(data=list(y=cbind(NA, cc[1:n.c], NA)), 
                    A=list(Ac, 1), tag='dat.c', 
                    effects=list(m=1:mesh$n, 
                        data.frame(a.c=1, w=w[1:n.c]))) 
stk.0 <- inla.stack(data=list(y=cbind(NA, NA, rep(0, n.c + n.y))), 
                    A=list(rBind(Ac,Ay), 1), tag='dat.0', 
                    effects=list(m=1:mesh$n, 
                        data.frame(a.c=1, w=w[1:(n.c+n.y)], 
                                   o=1:(n.c+n.y), 
                                   o.weig=rep(-1,n.c+n.y)))) 
stk <- inla.stack(stk.c, stk.y, stk.0) 
@ 

Define the spde model 
<<spde>>=
spde <- inla.spde2.matern(mesh)
@ 

The estimation of the regression coefficient 
of $c$ on $y$ we use the copy feature. 
In this case, we need to do a good prior specification. 
For example, is possible to know, a priori, the signal. 
We set a $N(2, 25)$ prior to $\beta_c$. 
Also, we define the formulae for the model. 
<<formula>>=
form <- y ~  0 + a.c + a.y + w + 
    f(m, model=spde) + f(x, model=spde) + 
    f(o, o.weig, model='iid', 
      hyper=list(theta=list(initial=-20, fixed=TRUE))) + 
    f(o.c, copy='o',  fixed=FALSE, 
      hyper=list(theta=list(param=c(2,25), initial=2))) 
@ 
and fit the model with 
<<callremote,echo=F,results=hide>>=
inla.setOption(inla.call='remote')
@ 
<<fit>>=
res <- inla(form, data=inla.stack.data(stk), family=rep('gaussian',3), 
            control.predictor=list(compute=TRUE, A=inla.stack.A(stk)), 
            control.family=list(list(), 
                list(hyper=list(theta=list(initial=20, fixed=TRUE))), 
                list(hyper=list(theta=list(initial=20, fixed=TRUE))))) 
@ 

\section{The results}

The true values of the intercepts and 
the regression coefficient of $w$ on $c$ and 
the summary of its posterior marginal distributions 
<<resfix>>=
round(cbind(True=c(alpha.c, alpha.y, beta.w), res$summary.fix), 4)
@ 

The true values of the precision of $y$ and 
the summary of the posterior marginal distribution  
<<reshyl>>=
round(cbind(True=1/sigma2.y, res$summary.hy[1,]), 4)
@ 

The true value of the regression coefficient 
of $c$ on $y$ and the summary of its 
posterior marginal distribution 
<<reshyb>>=
round(cbind(True=beta.c, res$summary.hy[6,]), 4)
@ 

The true values for the scale parameter $\kappa$ 
the mean of the posterior marginals and the 
95\% HPD credibility interval. 
<<kappa>>=
post.k <- lapply(res$marginals.hy[c(3,5)], function(y) 
                 inla.tmarginal(function(x) exp(x), y))
round(cbind(True=c(kappa.m=kappa.m, kappa.x=kappa.x), 
            t(sapply(post.k, function(x) 
                     c(mean=inla.emarginal(function(y) y, x), 
                       CI=inla.hpdmarginal(.95, x))))), 4)
@ 

The true values for the precision of the both random 
fields and the summary of the posterior marginals 
<<rf>>=
m.rf <- inla.spde2.result(res, 'm', spde)
x.rf <- inla.spde2.result(res, 'x', spde)
round(cbind(True=c(s2m=sigma2.m, s2x=sigma2.x), 
            mean=c(inla.emarginal(function(x) x, 
              m.rf$marginals.variance.n[[1]]), 
              inla.emarginal(function(x) x, 
                             x.rf$marginals.variance.n[[1]])), 
            rbind(inla.hpdmarginal(.95, m.rf$marginals.variance.n[[1]]), 
                  inla.hpdmarginal(.95, x.rf$marginals.variance.n[[1]]))), 4)
@ 

We see the posterior distribution of 
regression parameters on Figure~\ref{fig:regparsme} 
generated with comands below 
<<likeparsme,eval=F>>=
par(mfcol=c(2,2), mar=c(3,3,.1,.1), mgp=c(1.5,.5,0), las=1)
plot(res$marginals.fix[[1]], type='l', 
     xlab=expression(alpha[c]), ylab='')
abline(v=alpha.c, col=4)
plot(res$marginals.fix[[2]], type='l', 
     xlab=expression(alpha[y]), ylab='')
abline(v=alpha.y, col=4)
plot(res$marginals.fix[[3]], type='l', 
     xlab=expression(beta[w]), ylab='')
abline(v=beta.w, col=4)
plot(res$marginals.hy[[6]], type='l', 
     xlab=expression(beta[c]), ylab='')
abline(v=beta.c, col=4)
@ 
\setkeys{Gin}{width=0.75\textwidth}
\begin{figure}\centering
<<vlikeparsme,echo=F,fig=T,eps=F,width=7,height=3.5>>=
<<likeparsme>>
@ 
\caption{Posterior distribution of the likelihood parameters.}
\label{fig:regparsme}
\end{figure}
We see on the Figure~\ref{fig:regparsme} that 
the posterior distribution covers the true values 
of all the parameters. 

Now we want to see the posterior distribution of 
the both random fields. 
The practical range of the process is
$sqrt(8\nu)/\kappa$, where $\nu=1$ 
on this analisys.
We see these parameters 
on Figure~\ref{fig:rfparsme} 
generated with comands below 
<<rfparsme,eval=F>>=
par(mfcol=c(2,3), mar=c(3,3,.1,.3), mgp=c(1.5,.5,0), las=1)
plot.default(post.k[[1]], type='l', xlab=expression(kappa[m]), ylab='')
abline(v=kappa.m, col=4)
plot.default(post.k[[2]], type='l', xlab=expression(kappa[x]), ylab='')
abline(v=kappa.x, col=4)
plot.default(m.rf$marginals.variance.n[[1]], type='l', 
             xlab=expression(sigma[m]^2), ylab='')
abline(v=sigma2.m, col=4)
plot.default(x.rf$marginals.variance.n[[1]], type='l', 
             xlab=expression(sigma[x]^2), ylab='')
abline(v=sigma2.x, col=4)
plot.default(m.rf$marginals.range.n[[1]], type='l', 
             xlab='practical range of m', ylab='')
abline(v=sqrt(8)/kappa.m, col=4)
plot.default(x.rf$marginals.range.n[[1]], type='l', 
             xlab='practical range of x', ylab='')
abline(v=sqrt(8)/kappa.x, col=4)
@ 
\setkeys{Gin}{width=0.99\textwidth}
\begin{figure}\centering
<<vrfparsme,echo=F,fig=T,eps=F,width=12,height=4>>=
<<rfparsme>>
@ 
\caption{Posterior distribution of all the 
  parameters of both random field.}
\label{fig:rfparsme}
\end{figure}
We see on Figure~\ref{fig:rfparsme}  
that the posterior marginal distribution 
of the all parameters of both spatial 
process cover the true values well. 

Another interesting result is the 
prediction of the covariate on the 
response locations. 
We have the simulated values of $m$ 
on that locations. So, we are able 
to see if the predictions are good. 

The predictor matrix used on the estimation 
proces maps the nodes from mesh vertices 
to the data locations. 
The first lines of the predictor matrix for 
the covariate can be used to access the 
predictions on the locations of the covariate. 
Also, we have the predictor matrix 
used to the response. 
The last lines of this matrix that maps 
the mesh vertices to the response locations. 
Because we have the covariate simulated 
in the both set of locations, we use 
the correspondent parts of both predictor 
matrix to project the posterior mean and 
the posterior variance on the locations. 

We get this matrix by
<<prdmat>>=
mesh2locs <- rBind(Ac, Ay)
@ 
and the posterior mean and posterior standard deviations with 
<<predicted>>=
m.mprd <- drop(mesh2locs%*%res$summary.ran$m$mean)
sd.mprd <- drop(mesh2locs%*%res$summary.ran$m$sd)
@ 

With this aproach for this both posterior 
summary can be an aproximation to 95\% 
credibility interval, with normally supposition.
We see it this results with comands below 
<<prdplotme,eval=F>>=
plot(m.mprd, mm, asp=1, type='n', 
     xlab='Predicted', ylab='Simulated')
segments(m.mprd-2*sd.mprd, mm, m.mprd+2*sd.mprd, mm, 
         lty=2, col=gray(.75))
abline(c(0,1), col=4); points(m.mprd, mm, pch=3, cex=.5)
@ 
\setkeys{Gin}{width=0.5\textwidth}
\begin{figure}\centering
<<visprdplotme,echo=F,fig=T,eps=F,width=7,height=5>>=
<<prdplotme>>
@ 
\caption{Simulated versus predicted values of $m$ ($+$) 
  and the approximated credibility intervals.}
\label{fig:prdplotme}
\end{figure}
on the Figure~\ref{fig:prdplotme}. 
The blue line represents the situation where 
predicted is equal to simulated. 

