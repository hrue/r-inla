\section{A toy example}\label{sec:toyexample} 

This content is part of the book available at 
\url{http://www.r-inla.org/spde-book},
whose Gitbook version is freely available
along all the code and datasets.

<<sett,echo=F,results='hide',message=FALSE,warning=FALSE>>=
library(knitr)
opts_chunk$set(
fig.path='figs/toy',
message=FALSE, warning=FALSE
)
options(width=75, prompt = " ", continue = "   ")
library(lattice) 
library(INLA)
lcall <- inla.getOption('inla.call')
##inla.setOption(inla.call='remote')
##inla.setOption(num.threads=4)
@ 

In this example we will fit a simple geostatistical model
for the toy dataset simulated in Section~\ref{sec:simulatoy}. 
<<datatoy>>=
data(SPDEtoy)
@ 
This dataset is a three-column \code{data.frame}. 
Coordinates are in the first two clomuns and the 
response is in the third column. 
<<strdata>>=
str(SPDEtoy)
@ 

\subsection{SPDE model definition}

We have $n$ observations $y_i$, $i=1,...,n$, 
at locations $s_i$ and we define the following model 
\begin{equation}\begin{array}{c}
\mathbf{y}|\beta_0,\mathbf{u},\sigma_e^2 \sim 
N(\beta_0 + \mathbf{A} \mathbf{u}, \sigma_e^2) \\
\mathbf{u} \sim GF(0, \Sigma)
\end{array}\end{equation}
where $\beta_0$ is the intercept, 
$\bA$ is the projector matrix and 
$x$ is a spatial Gaussian random field. 

The first step in working with SPDE is to build a mesh. 
We will use as a mesh a collection of triangles, 
formed by $m$ vertices and the edges connecting them. 
This mesh has to cover the entire spatial domain of interest. 
More details on the mesh building are given in 
section~\ref{sec:mesh}. 
Here we will use the fifth mesh built in Section~\ref{sec:meshestoy} 
using the following code 
<<buildmesh5>>=
pl.dom <- cbind(c(0,1,1,0.7,0), c(0,0,0.7,1,1))
mesh5 <- inla.mesh.2d(, pl.dom, max.e=c(0.092, 0.2))
@

The SPDE model in the original parameterization can be built 
using the function \code{inla.spde2.matern()} 
<<spde2matern>>=
args(inla.spde2.matern)
@ 
The principal arguments are the mesh 
object and the $\alpha$ parameter, related 
to the smoothness parameter of the process.  

We will choose our parameters based instead on  
the marginal variance and the practical range, 
$\sqrt{8\nu}/\kappa$. 
For details on this parameterization see
\cite{lindgren:2012}. 
When defining the SPDE model we will need also to 
set priors for both the parameters.  
The smoothness parameter $\nu$ is fixed  
as $\alpha=\nu+d/2 \in [1,2]$. 
The \texttt{inla.spde2.pcmatern()} 
uses this parameterization to set the 
Penealized Complexity prior, PC-prior, as 
derived in \cite{fuglstadetal:2017}. 

Our example domain is the $[0,1]\times [0,1]$ square. 
We set the prior median to $0.3$. 
We set the probability that the 
marginal standard deviation exceeds 1 to 1\%. 
The toy dataset was simulated with $\alpha=2$ 
and it is considered to fit the model as well. 
<<spde5def>>=
spde5 <- inla.spde2.pcmatern(
    mesh=mesh5, alpha=2, ### mesh and smoothness parameter
    prior.range=c(0.3, 0.5), ### P(practic.range<0.3)=0.5
    prior.sigma=c(1, 0.01)) ### P(sigma>1)=0.01
@ 

\subsection{Projector matrix}

The second step in preparing to use SPDE 
is to build a projector matrix. 
This will project the random field modeled at the 
mesh nodes. For details, see section~\ref{sec:secres}. 
The projector matrix can be built with 
the \code{inla.spde.make.A} function. 
Considering that each mesh vertex has a weight, 
the value for one point within one triangle is 
the projection of the plane (formed by these 
three weights) at this point location. 
The projection is a weighted average 
using weights computed by the 
\code{inla.spde.make.A()} function. 
Using the toy data set and example mesh number five, we have
<<proj2>>=
coords <- as.matrix(SPDEtoy[,1:2])
A5 <- inla.spde.make.A(mesh5, loc=coords)
@ 

This matrix has dimension equal to the number of data 
locations times the number of vertices in the mesh 
<<dima1>>=
dim(A5)
@ 
Because each point location is inside one of the triangles 
there are exactly three non-zero elements on each line 
<<a5lines>>=
table(rowSums(A5>0))
@ 
because each point location is inside one of the triangles. 
These three elements on each line sum to one.
<<rsum>>=
table(rowSums(A5))
@ 
The reason they sum to one is that multiplication with 
any vector of weights at mesh nodes times this matrix 
is the projection of these weights at the point locations. 

There are some columns in the projector 
matrix all of whose elements equal zero. 
<<colsA>>=
table(colSums(A5)>0)
@ 
These columns correspond to triangles 
with no point location inside.
These columns can be dropped. 
The \texttt{inla.stack()} function (section~\ref{sec:stack}) 
does this automatically. 

When we have a mesh where every pont location is on 
a mesh vertex, each line on the projector matrix has 
exacty one nonzero element. 
This is the case for the \code{mesh1} built in 
section \ref{sec:meshestoy}. 
<<loadmeshes, echo=FALSE>>=
for (i in 1:6)
  load(paste('mesh', i, '.RData', sep=''))
@
<<eacha1>>=
A1 <- inla.spde.make.A(mesh1, loc=coords)
table(rowSums(A1>0))
@
and all these elements are equal to one 
<<summarya1>>=
table(rowSums(A1))
@ 
Each element equals one because in this 
instance the projections to the location points 
are equal to the weight at the corresponding node 
(at the same location) of the mesh. 

\subsection{The data stack}\label{sec:stack}

The \texttt{inla.stack()} function is usefull for 
organizing data, covariates, indices and projector matrices. 
All are relevant to SPDE models. 
\texttt{inla.stack()} helps to control the way 
effects are projected in the linear predictor. 
Detailed examples including one dimensional, 
replicated random field and spacetime models
are presented in \cite{lindgren:2012}. 

In the toy example we have a linear predictor 
that can be written as 
\[
\mathbf{\eta}^{*} = \mathbf{1}\beta_0 + \mathbf{A}\mathbf{u} \;.
\]
The first half of the right side represents the intercept. 
The term on the far right represents the spatial effect. 
Each component is represented as a product of a projector 
matrix and an effect. 

The Finite Element Method solution considered for 
implementing the SPDE models build the model over 
the mesh nodes. 
Usually the number of nodes is not equal to the number 
of locations for which we have observations. 
The \code{inla.stack} function allows us to 
work with predictors that includes terms with 
different dimensions. 
The three main \code{inla.stack()} arguments 
are the \code{data} vectors list, a list of 
projector matrices (each related to one 
block effect) and the effects. 

We need two projector matrices: the projector matrix 
for the latent field and a matrix that is a 
one-to-one map of the covariate and the response. 
The latter matrix can simply be a constant 
rather than a diagonal matrix. Thus we have 
<<stackdata1b>>=
stk5 <- inla.stack(data=list(resp=SPDEtoy$y), A=list(A5,1), 
                   effects=list(i=1:spde5$n.spde, 
                     m=rep(1,nrow(SPDEtoy))), tag='est')
@ 

The \code{inla.stack()} function automatically eliminates 
the any column in a projector matrix that has a zero sum. 
It generates a new and simplified matrix. 
The \code{inla.stack.A()} extracts 
a simplified predictor matrix to use 
with the \code{inla()} function, while 
the \code{inla.stack.data()} function 
extracts the corresponding data. 

The simplified projector matrix from the stack 
consists of the simplified projector matrices, 
where each column holds one effect block. 
<<dimA>>=
dim(inla.stack.A(stk5))
@ 
In the toy example we have one column more than the number 
of columns with non-null elements in the projector matrix. 

\subsection{Model fitting and some results}

To fit the model, we need to remove the intercept from 
the formula and add it as a covariate term, so that 
all the formula's covariate terms can be captured in a 
projector matrix. 
Them the matrix of predictors is passed to the \code{inla()} 
function in its \code{control.predictor} argument 
<<modelfit>>=
res5 <- inla(resp ~ 0 + m + f(i, model=spde5), 
             data=inla.stack.data(stk5), 
             control.predictor=list(A=inla.stack.A(stk5)))
@ 

The \code{inla()} function returns an 
object that is a set of several results. 
It includes summaries, marginal posterior densities of each 
parameter in the model, the regression parameters, each 
element that is a latent field, and all the hyperparameters. 

The summary of $\beta_0$ is obtained by 
<<beta0summary>>=
res5$summary.fix
@ 
and the summary of $1/\sigma_e^2$ by 
<<invnuggetsummary>>=
res5$summary.hy[1,]
@ 

A marginal distribution in \code{inla()} output 
consists of two vectors. 
One is a set of values on the range of the parameter 
space with posterior marginal density bigger than zero 
and another is the posterior marginal density 
at each one of these values. 
Any posterior marginal can be transformed.
If we want the posterior marginal for 
$\sigma_e$, the square root of $\sigma_e^2$, 
for example, we use 
<<postnugget>>=
post.se <- inla.tmarginal(function(x) sqrt(1/x), 
                          res5$marginals.hy[[1]])
@ 
Now we are able to summarize this distribution.
<<summarypostnu>>=
inla.emarginal(function(x) x, post.se)
inla.qmarginal(c(0.025, 0.5, 0.975), post.se)
inla.hpdmarginal(0.95, post.se)
inla.pmarginal(c(0.5, 0.7), post.se)
@ 
and, of course, we can visualize it.

The parameters of the latent field is parametrized 
as $log(\kappa)$ and $log(\tau)$, where $\tau$ 
is the local variance parameter. 
We have the posterior marginals for $\kappa$, 
$\sigma^2_x$ and for the nominal range (the 
distance that we have correlation equals 0.1). 
This can be done with the \code{inla.spde2.result} 
function
<<variancepost>>=
res5.field <- inla.spde2.result(res5, 'i', spde5, do.transf=TRUE)
@ 
and we get the posterior mean of each of these parameters by
<<erandf>>=
inla.emarginal(function(x) x, res5.field$marginals.kappa[[1]])
inla.emarginal(function(x) x, res5.field$marginals.variance.nominal[[1]])
inla.emarginal(function(x) x, res5.field$marginals.range.nominal[[1]])
@ 
also we can get other summary statistics, 
HPD interval and visualize it. 

\subsection{Prediction of the random field}

A very common objective when we have spatial 
data collected on some locations is the prediction 
on a fine grid to get hight resolution maps. 
In this subsection we show two approaches to make 
prediction of the random field, one is after 
the estimation process 
and other is jointly on estimation process. 
To compare both approaches, we predict the random field 
on three target locations: (0.1,0.1), (0.5,0.55), (0.7,0.9). 
<<pts3>>=
pts3 <- rbind(c(.1,.1), c(.5,.55), c(.7,.9))
@ 

\subsubsection{Jointly with the estimation process} 

The prediction of the random field joint 
the parameter estimation process in 
Bayesian inference is the common approach. 
This approach is made by the computation of 
the marginal posterior distribution of 
the random field at target locations. 
If the target points are on the mesh, so 
we have automatically this distribution. 
If the target points are not on the mesh, 
we must define the projector matrix 
for the target points. 

The predictor matrix for the target locations is 
<<A5pts3>>=
dim(A5pts3 <- inla.spde.make.A(mesh5, loc=pts3))
@ 
We can show the columns with non-zero elements 
of this matrix 
<<a5pts3c>>=
(jj3 <- which(colSums(A5pts3)>0))
round(A5pts3[, jj3],3)
@ 

We have to define a data stack for the prediction 
and join it with the data stack of the observations. 
The prediction data stack contais the effect set, 
predictor matrices and assign NA to response 
<<stk3prd>>=
stk5p.rf <- inla.stack(data=list(resp=NA), A=list(A5pts3), 
                       effects=list(i=1:spde5$n.spde), tag='prd5r')
@ 
Also, we join both stacks by 
<<stakfull>>=
stk5.jp <- inla.stack(stk5, stk5p.rf)
@ 
and fit the model again with the full stack setting 
\code{compute=TRUE} on \code{control.predictor} 
<<refit>>=
res5p <- inla(resp ~ 0 + m + f(i, model=spde5), 
              data=inla.stack.data(stk5.jp), 
              control.predictor=list(A=inla.stack.A(stk5.jp), compute=TRUE))
@ 

To access the posterior marginal distribution 
of the random field at the target locations, 
we extract the index from the full stack 
using the adequate \code{tag}.
<<inddp>>=
(indd5p <- inla.stack.index(stk5.jp, tag='prd5r')$data)
@ 
The summary of the posterior distributions 
of the random field on the target locations is 
<<postp>>=
round(res5p$summary.linear.pred[indd5p,], 4)
@ 
that includes the posterior mean, 
standard deviation, quantiles and mode. 

Because it is a full bayesian analysis, we 
also we have the marginal distributions. 
We extract the marginals posterior distributions with
<<marg3p,results='hide'>>=
marg3 <- res5p$marginals.linear[indd5p]
@ 
and get the 95\% HPD interval for the random 
field at the second target location by 
<<hpdp3>>=
inla.hpdmarginal(0.95, marg3[[2]])
@ 
and see that around the point (0.5,0.5) the random field 
has positive values, see Figure~\ref{fig:pgrid}. 

\subsubsection{After the estimation process}

If we need just the prediction we can do 
the prediction after the estimation process 
with a very small computational cost. 
It is just a matrix operation in way 
that we just project the posterior mean of 
the the random field on mesh nodes to 
target locations, using the correspondent 
projector matrix. 

So, we 'project' the posterior mean of the 
latend random field to the target locations by
<<meanproj3>>=
drop(A5pts3%*%res5$summary.random$i$mean)
@ 
or using the \code{inla.mesh.projector()} function
<<projector>>=
inla.mesh.project(inla.mesh.projector(mesh5, loc=pts3), 
                  res5$summary.random$i$mean)
@ 
and see that for the mean we have similar values 
than those on previous subsection. 

Also, we can get the standard deviation 
<<sdproj3>>=
drop(A5pts3%*%res5$summary.random$i$sd)
@  
and we have a little difference. 
<<sdproj3c>>=
sqrt(drop((A5pts3^2)%*%(res5$summary.random$i$sd^2)))
@  

\subsubsection{Projection on a grid}

The approach by the projection of the posterior 
mean random field is computationaly cheap. 
So, it can be used to get the map 
of the random field on a fine grid. 
The \code{inla.mesh.projector()} function 
get the projector matrix automatically for a grid 
of points over a square that contains the mesh. 

To get projection on a grid at the domain 
$(0,1)\times (0,1)$ we just inform these limits
<<grid0>>=
pgrid0 <- inla.mesh.projector(mesh5, xlim=0:1, ylim=0:1, dims=c(101,101))
@ 
and we project the posterior mean and the posterior 
standard deviation on the both grid with 
<<projg>>=
prd0.m <- inla.mesh.project(pgrid0,  res5$summary.ran$i$mean)
prd0.s <- inla.mesh.project(pgrid0,  res5$summary.ran$i$s)
@ 
We visualize this values projected on the grid on 
Figure~\ref{fig:pgrid}. 

\subsection{Prediction of the response}

Another commom result that we want on 
spatially continuous modelling is the 
prediction of the response on a target 
locations that we don't have data observed. 
In similar way that on past subsection, it is possible 
to find the marginal distribution or to make 
a projection of some functional of the response. 

\subsubsection{By the posterior distribution}

In this case, we want to define a adequate 
predictor of the response and build the model again. 
This is similar to the 
stack to predict the random field, 
but here we add the intercept on the list 
of predictor matrix and on the list of effects 
<<stackpresp>>=
stk5.presp <- inla.stack(data=list(resp=NA), A=list(A5pts3,1), 
                         effects=list(i=1:spde5$n.spde, m=rep(1,3)), 
                         tag='prd5.resp')
@ 
and join with the data stack to build the model again 
<<rresp>>=
stk5.full <- inla.stack(stk5, stk5.presp)
r5presp <- inla(resp ~ 0 + m + f(i, model=spde5), 
                data=inla.stack.data(stk5.full), 
                control.predictor=list(A=inla.stack.A(stk5.full), compute=TRUE))
@

We find the index of the predictor that 
corresponds the predicted values of 
the response on the target locations. 
We extract the index from the full stack by
<<indd>>=
(indd3r <- inla.stack.index(stk5.full, 'prd5.resp')$data)
@ 

To get the summary of the posterior distributions 
of the response on target locations we do
<<postd>>=
round(r5presp$summary.fitted.values[indd3r,], 3)
@ 
Also, we extract the marginals posterior 
distributions with
<<margp,results='hide'>>=
marg3r <- r5presp$marginals.fitted.values[indd3r]
@ 
and get the 95\% HPD interval for the response 
at second target location by 
<<hpdp>>=
inla.hpdmarginal(0.95, marg3r[[2]])
@ 
and see that around the point (0.5,0.5) we have the 
values of the response significantly larger 
than $\beta_0$, see Figure~\ref{fig:pgrid}. 

\subsubsection{By sum of linear predictor components}

A computational cheap approach is to (naively) 
sum the projected posterior mean to the 
regression term. 
In this toy example we just sum the posterior mean 
of the intercept to the posterior mean of the random 
field to get the posterior mean of the response. 

If there are covariates, the prediction also can be 
made in similar way, see~\citation{camelettietal:2012}. 
That approach can be used here considering just 
the intercept 
<<prdmean>>=
res5$summary.fix[1,1] + drop(A5pts3%*%res5$summary.random$i$mean)
@ 
For the standard error, 
we need to take into account the 
error of the covariate values and 
regression coefficients. 
<<optlcall,echo=F>>=
inla.setOption(inla.call=lcall)
@ 
<<cov,results='hide',echo=F>>=
q <- inla.spde2.precision(spde5, theta=res5$summary.hyper[2:3,1])
rf.cov <- inla.qinv(q)
dim(rf.cov)
diag(cov3 <- A5pts3%*%rf.cov%*%t(A5pts3))
cov3
@ 
<<interpmean>>=
summary(rvar <- res5$summary.random$i$sd^2)
sqrt(1^2+res5$summary.fix[1,2]^2 + drop(A5pts3%*%rvar))
@ 

\subsubsection{Response on a grid}

The computation of all marginal posterior 
distributions on a grid is computationally expensive. 
But, we usually not uses the marginal distributions. 
We usually uses just the mean and standard deviation. 
So, we don't need the storage of all the marginal 
distributions! Also, we don't need the quantiles 
of the marginal distributions. 

On the code below, we build the model again 
but we disable the storage of the marginal 
posterior distributions to random effects and 
to posterior predictor values. 
Also, we disable the computation of the quantiles. 
Only the mean and standard defiation are stored. 

We use the projector matrix on the projector object 
that we use to project the posterior mean on the grid 
<<prespgrid>>=
stkgrid <- inla.stack(data=list(resp=NA), A=list(pgrid0$proj$A,1), 
                      effects=list(i=1:spde5$n.spde,
                        m=rep(1,101*101)), tag='prd.gr')
stk.all <- inla.stack(stk5, stkgrid)
res5g <- inla(resp ~ 0 + m + f(i, model=spde5), 
              data=inla.stack.data(stk.all), 
              control.predictor=list(A=inla.stack.A(stk.all), 
                compute=TRUE), quantiles=NULL, 
              control.results=list(return.marginals.random=FALSE, 
                return.marginals.predictor=FALSE))
res5g$cpu
@ 

We get the indexes 
<<indgr>>=
igr <- inla.stack.index(stk.all, 'prd.gr')$data
@ 
and use it to visualize, together the prediction  
of the random field on previous subsection,
on Figure~\ref{fig:pgrid} with the commands bellow
<<pgrid,eval=F>>=
library(gridExtra)
grid.arrange(levelplot(prd0.m, col.regions=topo.colors(99), main='latent field mean',
                       xlab='', ylab='', scales=list(draw=FALSE)), 
             levelplot(matrix(res5g$summary.fitt[igr,1], 101), 
                       xlab='', ylab='', main='response mean',
                       col.regions=topo.colors(99), scales=list(draw=FALSE)), 
             levelplot(prd0.s, col.regions=topo.colors(99), main='latent field SD',
                       xlab='', ylab='', scales=list(draw=FALSE)), 
             levelplot(matrix(res5g$summary.fitt[igr,2], 101), 
                       xlab='', ylab='', main='response SD',
                       col.regions=topo.colors(99), scales=list(draw=FALSE)), 
             nrow=2)
@ 
\begin{figure}\centering 
<<echo=FALSE,fig.width=7.5,fig.height=7,out.width='0.97\\textwidth'>>=
<<pgrid>>
@ 
\caption{The mean and standard deviation of the random field 
  (top left and bottom left) 
  and the mean and standard variation of the 
  response (top right and bottom right)}
\end{figure}\label{fig:pgrid}

We see on Figure~\ref{fig:pgrid} that we have a 
variation from -4 to 4 on the spatial effect. 
Considering also that we have standard 
deviations around 0.8 to 1.6, the spatial 
dependence is significantly. 

Another thing is that the standard deviation 
of both, random field and the response, 
are less near the corner (0, 0) and greater 
near the corner (1,1). 
This is just proportional to the locations density. 

\subsection{Results from different meshes}\label{sec:meshcompare}

In this subsection we compare six results for 
the toy dataset based on the six different 
meshs builded on Section~\ref{sec:mesh}. 
To do this comparison, we just plot the posterior 
marginal distributions of the model parameters. 
We evaluate the meshes by the addiction of 
the true values used on the simulation 
of the toy dataset. Also, we add the 
maximum likelihood estimates from 
\pkg{geoR} package, \cite{geoR}. 


We fit the model, using each one of the six meshes, 
and put the results in a list with the code bellow 
<<meshes>>=
lrf <- lres <- l.dat <- l.spde <- l.a <- list()
for (k in 1:6) {
  l.a[[k]] <- inla.spde.make.A(get(paste('mesh', k, sep='')), loc=coords)
  l.spde[[k]] <- inla.spde2.matern(get(paste('mesh', k, sep='')), alpha=2)
  l.dat[[k]] <- list(y=SPDEtoy[,3], i=1:ncol(l.a[[k]]), 
                     m=rep(1, ncol(l.a[[k]])))
  lres[[k]] <- inla(y ~ 0 + m + f(i, model=l.spde[[k]]), 
                    data=l.dat[[k]], control.predictor=list(A=l.a[[k]]))
  lrf[[k]] <- inla.spde2.result(lres[[k]], 'i', l.spde[[k]], do.transf=TRUE)
}
@ 

The mesh size influences the computational 
time needed to fit the model. 
More nodes on the mesh need more computational time.
The time running inla for these six meshes are
<<time>>=
round(sapply(lres, function(x) x$cpu[2]), 2)
@ 

We compute the distribution for $\sigma_e^2$ 
for each fitted model
<<s2marg>>=
s2.marg <- lapply(lres, function(m) 
                  inla.tmarginal(function(x) 1/x, m$marginals.hy[[1]]))
@ 

The true values are: $\beta_0=10$, $\sigma_e^2=0.3$, 
$\sigma_x^2=5$, $\kappa=7$ and $\nu=1$. 
The $\nu$ parameter is fixed on the true value when 
we define $\alpha=2$ on definition of the SPDE model. 
<<truepars>>=
beta0 <- 10; sigma2e <- 0.3; sigma2u <- 5; kappa <- 7; nu <- 1
@ 
and the maximum likelihood one can have using the 
\textbf{\textsf{geoR}} package, \cite{geoR}, are 
<<lkv>>=
lk.est <- c(beta=9.54, s2e=0.374, s2u=3.32, range=0.336)
@ 

We want to visualize the posterior marginal 
distributions for $\beta_0$, $\sigma_e^2$, 
$\sigma_x^2$, $\kappa$, nominal range and 
the local variance $\tau$. 
This can be done with the code bellow 
<<compare,eval=F>>=
rcols <- rainbow(6)##c(rgb(4:1/4,0:3/5,0), c(rgb(0,0:3/5,4:1/4)))
par(mfrow=c(2,3), mar=c(2.5,2.5,1,.5), mgp=c(1.5,.5,0), las=1)

xrange <- range(sapply(lres, function(x) range(x$marginals.fix[[1]][,1])))
yrange <- range(sapply(lres, function(x) range(x$marginals.fix[[1]][,2])))
plot(lres[[1]]$marginals.fix[[1]], type='l', xlim=xrange, ylim=yrange, 
     xlab=expression(beta[0]), ylab='Density')
for (k in 1:6)
  lines(lres[[k]]$marginals.fix[[1]], col=rcols[k], lwd=2)
abline(v=beta0, lty=2, lwd=2, col=3) 
abline(v=lk.est[1], lty=3, lwd=2, col=3)

xrange <- range(sapply(s2.marg, function(x) range(x[,1])))
yrange <- range(sapply(s2.marg, function(x) range(x[,2])))
plot.default(s2.marg[[1]], type='l', xlim=xrange, ylim=yrange, 
             xlab=expression(sigma[e]^2), ylab='Density')
for (k in 1:6) 
  lines(s2.marg[[k]], col=rcols[k], lwd=2)
abline(v=sigma2e, lty=2, lwd=2, col=3) 
abline(v=lk.est[2], lty=3, lwd=2, col=3)

xrange <- range(sapply(lrf, function(r) range(r$marginals.variance.nominal[[1]][,1])))
yrange <- range(sapply(lrf, function(r) range(r$marginals.variance.nominal[[1]][,2])))
plot(lrf[[1]]$marginals.variance.nominal[[1]], type='l', 
     xlim=xrange, ylim=yrange, xlab=expression(sigma[x]^2), ylab='Density')
for (k in 1:6)
  lines(lrf[[k]]$marginals.variance.nominal[[1]], col=rcols[k], lwd=2)
abline(v=sigma2u, lty=2, lwd=2, col=3) 
abline(v=lk.est[3], lty=3, lwd=2, col=3)

xrange <- range(sapply(lrf, function(r) range(r$marginals.kappa[[1]][,1])))
yrange <- range(sapply(lrf, function(r) range(r$marginals.kappa[[1]][,2])))
plot(lrf[[1]]$marginals.kappa[[1]], type='l', 
     xlim=xrange, ylim=yrange, xlab=expression(kappa), ylab='Density')
for (k in 1:6)
  lines(lrf[[k]]$marginals.kappa[[1]], col=rcols[k], lwd=2)
abline(v=kappa, lty=2, lwd=2, col=3) 
abline(v=lk.est[4], lty=3, lwd=2, col=3)

xrange <- range(sapply(lrf, function(r) range(r$marginals.range.nominal[[1]][,1])))
yrange <- range(sapply(lrf, function(r) range(r$marginals.range.nominal[[1]][,2])))
plot(lrf[[1]]$marginals.range.nominal[[1]], type='l', 
     xlim=xrange, ylim=yrange, xlab='nominal range', ylab='Density')
for (k in 1:6)
  lines(lrf[[k]]$marginals.range.nominal[[1]], col=rcols[k], lwd=2)
abline(v=sqrt(8)/kappa, lty=2, lwd=2, col=3)
abline(v=sqrt(8)/lk.est[4], lty=3, lwd=2, col=3)

xrange <- range(sapply(lrf, function(r) range(r$marginals.tau[[1]][,1])))
yrange <- range(sapply(lrf, function(r) range(r$marginals.tau[[1]][,2])))
plot(lrf[[1]]$marginals.tau[[1]], type='l', 
     xlim=xrange, ylim=yrange, xlab=expression(tau), ylab='Density')
for (k in 1:6)
  lines(lrf[[k]]$marginals.tau[[1]], col=rcols[k], lwd=2)

legend('topright', c(paste('mesh', 1:6, sep=''), 'True', 'Likelihood'), 
       lty=c(rep(1,6), 2, 3), lwd=rep(2, 6), col=c(rcols,3,3), bty='n')
@ 

At the Figure~\ref{fig:margposttoy} we can see 
that the posterior marginal distribution for the 
intercept has mode on the likelihood estimate, 
considering the results from all six meshes. 
<<kappai>>=
1/kappa
@ 

The main differences are on the noise 
variance $\sigma_e^2$ (the nugget effect). 
The result from the mesh based on the points 
and with small triangles mode less than the 
likelihood estimate, the second has mode near 
likelihood estimate and the third large. 
Considering the other meshes, the mesh 
four has mode around likelihood 
estimate and the other two litle larger, 
similar to the third mesh, such is 
based on points but with some freedom 
(\code{cutoff} greather than zero). 

For the marginal variance of the latent field, 
$\sigma_x^2$, the results with all meshes had 
mode near the likelihood estimate. 
For the scale parameter $\kappa$ all meshes 
has mode less than the likelihood estimate. 
The posterior distribution from the meshes 
based on points are that ones with less mode 
and that the mode from third mesh are the less.
For the practical range the opposite happens. 

\begin{figure}\centering
<<echo=F,fig.width=7.5,fig.height=5>>=
<<compare>>
@ 
\caption{Marginal posterior distribution for 
$\beta_0$ (top left), $\sigma_e^2$ (top mid), 
 $\sigma_x^2$ (top right), $\kappa$ (bottom left), 
 nominal range (bottom mid) and $\tau$ (bottom right).}
\end{figure}\label{fig:margposttoy}

These results are not conclusive, but a general 
comment is that is good to have a mesh with some 
tune on the points locations, to access noise variance, 
but with some flexibility to avoid many variability 
on the triangles size and shape, to get good 
latent field parameters estimation.
