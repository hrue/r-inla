\chapter{A toy example} 

\SweaveOpts{prefix.string=figs/toy}
<<sett,echo=F,results=hide>>=
options(width=75, prompt = " ", continue = "   ")
require(INLA)
noremote <- inla.getOption('inla.call')
inla.setOption(inla.call='remote')
@ 

In this section we start to show the fitting process 
using the SPDE approach, \cite{lindgrenRL:2011}. 
This starter is by fitting a toy geostatistical model: 
Gaussian response without covariate. 
We use the Bayesian approach and found the posterior 
marginal distributions using the Integrated Nested Laplace 
Approximation - INLA, \cite{rueMC:2009} implemented 
on the \pkg{INLA} \pkg{R} package. 
The ideas for application of the SPDE approach 
using the \pkg{INLA} package are well described on 
\cite{lindgren:2012} and on \cite{lindgrenR:2013}. 

The dataset used are a tree column \code{data.frame} 
simulated on the previous section and 
provided on \pkg{INLA} package. 
It can be called by 
<<callSPDEtoy>>=
data(SPDEtoy)
@ 
this is a \code{data.frame} where the two first columns 
are the coordinates and the third is the response 
simulated at this locations 
<<strdata>>=
str(SPDEtoy)
@ 

We consider the $n$ observations $y_i$ on 
locations the $s_i$, $i=1,...,n$, and we 
define the model 
\begin{equation}\begin{array}{c}
y_i|\beta_0,x_i,\sigma_e^2 \sim N(\beta_0 + x_i, \sigma_e^2) \\
\bbx \sim GF(0, \Sigma)
\end{array}\end{equation}
We consider that $x$ is a realization of 
a Gaussian Field, with Mat\'ern correlation 
function parametrized by the smoothness 
parameter $\nu$ and the scale $\kappa$, 
such the parametrization 
in~\cite{lindgrenRL:2011}. 

With this toy example we show with details 
how we make a good triangulation, 
prepare the data, fit the model, 
extract the results from output and 
make predictions on locations where 
we don't have observed the response. 
In this section we use the default 
priors for all the parameters. 

<<likfit,echo=F,results=hide>>=
require(geoR)
lk <- likfit(as.geodata(SPDEtoy), ini=c(5,.1), nugget=.1, kappa=1)
lk.est <- c(lk$beta, lk$nugget, lk$cov.pars)
lk.est[4] <- 1/lk.est[4]
@ 

To the estimation mesh we need to define the mesh. 
We show it on Chapter~\ref{ch:mesh} and here 
we use the fift mesh builded 
for the toy example on Chapter~\ref{ch:mesh}
<<loadmeshes,echo=F>>=
for (i in 1:6)
  load(paste('mesh', i, '.RData', sep=''))
@ 

With that mesh, we define the SPDE model 
using the function \code{inla.spde2.matern()} 
<<spde2matern>>=
args(inla.spde2.matern)
@ 
The principal arguments are the mesh 
object and the $\alpha$ parameter, related 
to the smoothness parameter of the process.  

The toy dataset was simulated with $\alpha=2$ 
and we use this value here (with the mesh five) 
<<spdedef>>=
spde5 <- inla.spde2.matern(mesh5, alpha=2)
@ 

Also, from section~\ref{sec:projmat} 
we define the projector matrix 
<<a5>>=
coords <- as.matrix(SPDEtoy[,1:2])
A5 <- inla.spde.make.A(mesh5, loc=coords)
@ 

\section{The stack functionality}\label{sec:stack}

The stack functionality is a very useful 
functionality to work with SPDE on \pkg{INLA} package. 
This allow to fit more general SPDE models, 
such as replicated or correlated ones, 
or more general models that includes 
more than one projector matrix. 
Using it we avoid errors in the index 
construction, \cite{lindgren:2012}. 
Examples on more complex models, with the details, 
can be found on \cite{lindgren:2012}, 
\cite{cameletti:2012} and \cite{lindgrenR:2013}. 

In a section on the previous Chapter we define the 
projector matrix to project the latent field on the 
response locations. 
If we have have covariates measured 
at same locations (with same dimention of the 
response), we need a more general definition 
of the linear predictor. 
On the toy example, we have to include the 
effect of the random field and the intercept 
that is treated as a covariate. 
So, we define the new intercept $\eta^{*}$ as 
\[
\eta^{*} = \bA\bbx + \b1\beta_0
\]
that is a sum of two components each one 
can be represented as a product of a projector 
matrix and an effect. 

The \code{inla.stack} function allow us to 
work 'easily' with more than one responses.
The three main \code{inla.stack()} arguments 
are the \code{data} vectors list, a list of 
projector matrices (each one related to one 
block effect) and the effects. 

We need two projector matrices, the projector matrix 
for the latend field and a matrix to map one-to-one 
the 'covariate' and the response. This last one can 
be just a constant instead a diagonal matrix. 
So, we have 
<<stackdata1b>>=
stk5 <- inla.stack(data=list(resp=SPDEtoy$y), A=list(A5,1), 
                   effects=list(i=1:spde5$n.spde, 
                     m=rep(1,nrow(SPDEtoy))), tag='est')
@ 

The \code{inla.stack()} function automatically 
eliminates the elements when any column of 
each projector matrix has zero sum, generating 
a correspondent simplified projector matrix. 
The \code{inla.stack.A()} extracts 
a simplified predictor matrix to use 
on the \code{inla()} function and 
the \code{inla.stack.data()} function 
extract the correspondent organized data. 

The simplified projector matrix from the stack 
is the binded by column the simplified 
projectors matrices from each effect block. 
So, in this case we have 
<<dimA>>=
dim(inla.stack.A(stk5))
@ 
one columns more than the number of columns with 
non null elements of the projector matrix.

\section{Model fitting and some results}

To fit the model need to remove the intercept from 
the formulae and add it as a covariate term, 
because we a have projector matrix 
that allows it as a covariate effect. 
Of course, we must have to pass these predictor 
matrix on \code{control.predictor} argument of the 
\code{inla} function
<<modelfit>>=
res5 <- inla(resp ~ 0 + m + f(i, model=spde5), 
             data=inla.stack.data(stk5), 
             control.predictor=list(A=inla.stack.A(stk5)))
@ 

An object from \code{inla()} function 
has a set of several results. 
These includes summaries, marginal posterior densities of 
each parameter on the model: the regression parameters, 
each element of the latent field and all the hyperparameters. 

The summary of $\beta_0$ is obtained by 
<<beta0summary>>= 
res5$summary.fix
@ 

The summary of $1/\sigma_e^2$ is obtained by 
<<invnuggetsummary>>= 
res5$summary.hy[1,]
@ 

A marginal distribution on \code{inla()} output 
is just two vector, where one represents the 
parameter values and another the density. 
Any posterior marginal can be transformed.
If we want the posterior marginal for 
$\sigma_e$, the square root of the $\sigma_e^2$, 
we use 
<<postnugget>>=
post.se <- inla.tmarginal(function(x) sqrt(1/x), 
                          res5$marginals.hy[[1]])
@ 
and now we are able to summarize this distribution
<<summarypostnu>>=
inla.emarginal(function(x) x, post.se)
inla.qmarginal(c(0.025, 0.5, 0.975), post.se)
inla.hpdmarginal(0.95, post.se)
inla.pmarginal(c(0.5, 0.7), post.se)
@ 
and, of course, we can visualize it.

The parameters of the latent field is parametrized 
as $log(\kappa)$ and $log(\tau)$, where $\tau$ 
is the local variance parameter. 
We have the posterior marginals for $\kappa$, 
$\sigma^2_x$ and for the nominal range (the 
distance that we have correlation equals 0.1). 
This can be done with the \code{inla.spde2.result} 
function
<<variancepost>>=
res5.field <- inla.spde2.result(res5, 'i', spde5, do.transf=TRUE)
@ 
and we get the posterior mean of each of these parameters by
<<erandf>>=
inla.emarginal(function(x) x, res5.field$marginals.kappa[[1]])
inla.emarginal(function(x) x, res5.field$marginals.variance.nominal[[1]])
inla.emarginal(function(x) x, res5.field$marginals.range.nominal[[1]])
@ 
also we can get other summary statistics, 
HPD interval and visualize it. 

\section{Prediction of the random field}

A very common objective when we have spatial 
data collected on some locations is the prediction 
on a fine grid to get hight resolution maps. 
In this section we show two approaches to make 
prediction of the random field, one is after 
the estimation process 
and other is jointly on estimation process. 
To compare both approaches, we predict the random field 
on three target locations: (0.1,0.1), (0.5,0.55), (0.7,0.9). 
<<pts3>>=
pts3 <- rbind(c(.1,.1), c(.5,.55), c(.7,.9))
@ 

\subsection{Jointly with the estimation process} 

The prediction of the random field joint 
the parameter estimation process in 
Bayesian inference is the common approach. 
This approach is made by the computation of 
the marginal posterior distribution of 
the random field at target locations. 
If the target points are on the mesh, so 
we have automatically this distribution. 
If the target points are not on the mesh, 
we must define the projector matrix 
for the target points. 

The predictor matrix for the target locations is 
<<A5pts3>>=
dim(A5pts3 <- inla.spde.make.A(mesh5, loc=pts3))
@ 
We can show the columns with non-zero elements 
of this matrix 
<<a5pts3c>>=
(jj3 <- which(colSums(A5pts3)>0))
round(A5pts3[, jj3],3)
@ 

We have to define a data stack for the prediction 
and join it with the data stack of the observations. 
The prediction data stack contais the effect set, 
predictor matrices and assign NA to response 
<<stk3prd>>=
stk5p.rf <- inla.stack(data=list(resp=NA), A=list(A5pts3), 
                       effects=list(i=1:spde5$n.spde), tag='prd5r')
@ 
Also, we join both stacks by 
<<stakfull>>=
stk5.jp <- inla.stack(stk5, stk5p.rf)
@ 
and fit the model again with the full stack setting 
\code{compute=TRUE} on \code{control.predictor} 
<<refit>>=
res5p <- inla(resp ~ 0 + m + f(i, model=spde5), 
              data=inla.stack.data(stk5.jp), 
              control.predictor=list(A=inla.stack.A(stk5.jp), compute=TRUE))
@ 

To access the posterior marginal distribution 
of the random field at the target locations, 
we extract the index from the full stack 
using the adequate \code{tag}.
<<indd>>=
(indd5p <- inla.stack.index(stk5.jp, tag='prd5r')$data)
@ 
The summary of the posterior distributions 
of the random field on the target locations is 
<<postd>>=
round(res5p$summary.linear.pred[indd5p,], 4)
@ 
that includes the posterior mean, 
standard deviation, quantiles and mode. 

Because it is a full bayesian analysis, we 
also we have the marginal distributions. 
We extract the marginals posterior distributions with
<<margp,results=hide>>=
marg3 <- res5p$marginals.linear[indd5p]
@ 
and get the 95\% HPD interval for the random 
field at the second target location by 
<<hpdp>>=
inla.hpdmarginal(0.95, marg3[[2]])
@ 
and see that around the point (0.5,0.5) the random field 
has positive values, see Figure~\ref{fig:pgrid}. 

\subsection{After the estimation process}

If we need just the prediction we can do 
the prediction after the estimation process 
with a very small computational cost. 
It is just a matrix operation in way 
that we just project the posterior mean of 
the the random field on mesh nodes to 
target locations, using the correspondent 
projector matrix. 

So, we 'project' the posterior mean of the 
latend random field to the target locations by
<<meanproj3>>=
drop(A5pts3%*%res5$summary.random$i$mean)
@ 
or using the \code{inla.mesh.projector()} function
<<projector>>=
inla.mesh.project(inla.mesh.projector(mesh5, loc=pts3), 
                  res5$summary.random$i$mean)
@ 
and see that for the mean we have similar values 
than those on previous section. 

Also, we can get the standard deviation 
<<sdproj3>>=
drop(A5pts3%*%res5$summary.random$i$sd)
@  
and we have a little difference. 
<<sdproj3c>>=
sqrt(drop((A5pts3^2)%*%(res5$summary.random$i$sd^2)))
@  

\subsection{Projection on a grid}

The approach by the projection of the posterior 
mean random field is computationaly cheap. 
So, it can be used to get the map 
of the random field on a fine grid. 
The \code{inla.mesh.projector()} function 
get the projector matrix automatically for a grid 
of points over a square that contains the mesh. 

To get projection on a grid at the domain 
$(0,1)\times (0,1)$ we just inform these limits
<<grid0>>=
pgrid0 <- inla.mesh.projector(mesh5, xlim=0:1, ylim=0:1, dims=c(101,101))
@ 
and we project the posterior mean and the posterior 
standard deviation on the both grid with 
<<projg>>=
prd0.m <- inla.mesh.project(pgrid0,  res5$summary.ran$i$mean)
prd0.s <- inla.mesh.project(pgrid0,  res5$summary.ran$i$s)
@ 
We visualize this values projected on the grid on 
Figure~\ref{fig:pgrid}. 

\section{Prediction of the response}

Another commom result that we want on 
spatially continuous modelling is the 
prediction of the response on a target 
locations that we don't have data observed. 
In similar way that on past section, it is possible 
to find the marginal distribution or to make 
a projection of some functional of the response. 

\subsection{By the posterior distribution}

In this case, we want to define a adequate 
predictor of the response and build the model again. 
This is similar to the 
stack to predict the random field, 
but here we add the intercept on the list 
of predictor matrix and on the list of effects 
<<stackpresp>>=
stk5.presp <- inla.stack(data=list(resp=NA), A=list(A5pts3,1), 
                         effects=list(i=1:spde5$n.spde, m=rep(1,3)), 
                         tag='prd5.resp')
@ 
and join with the data stack to build the model again 
<<rresp>>=
stk5.full <- inla.stack(stk5, stk5.presp)
r5presp <- inla(resp ~ 0 + m + f(i, model=spde5), 
                data=inla.stack.data(stk5.full), 
                control.predictor=list(A=inla.stack.A(stk5.full), compute=TRUE))
@

We find the index of the predictor that 
corresponds the predicted values of 
the response on the target locations. 
We extract the index from the full stack by
<<indd>>=
(indd3r <- inla.stack.index(stk5.full, 'prd5.resp')$data)
@ 

To get the summary of the posterior distributions 
of the response on target locations we do
<<postd>>=
round(r5presp$summary.fitted.values[indd3r,], 3)
@ 
Also, we extract the marginals posterior 
distributions with
<<margp,results=hide>>=
marg3r <- r5presp$marginals.fitted.values[indd3r]
@ 
and get the 95\% HPD interval for the response 
at second target location by 
<<hpdp>>=
inla.hpdmarginal(0.95, marg3r[[2]])
@ 
and see that around the point (0.5,0.5) we have the 
values of the response significantly larger 
than $\beta_0$, see Figure~\ref{fig:pgrid}. 

\subsection{By sum of linear predictor components}

A computational cheap approach is to (naively) 
sum the projected posterior mean to the 
regression term. 
In this toy example we just sum the posterior mean 
of the intercept to the posterior mean of the random 
field to get the posterior mean of the response. 

If there are covariates, the prediction also can be 
made in similar way, see~\citation{cameletti:2012}. 
That approach can be used here considering just 
the intercept 
<<prdmean>>=
res5$summary.fix[1,1] + drop(A5pts3%*%res5$summary.random$i$mean)
@ 
For the standard error, 
we need to take into account the 
error of the covariate values and 
regression coefficients. 
<<optnoremote,echo=F>>=
inla.setOption(inla.call=noremote)
@ 
<<cov,results=hide,echo=F>>=
q <- inla.spde2.precision(spde5, theta=res5$summary.hyper[2:3,1])
rf.cov <- inla.qinv(q)
dim(rf.cov)
diag(cov3 <- A5pts3%*%rf.cov%*%t(A5pts3))
cov3
@ 
<<interpmean>>=
summary(rvar <- res5$summary.random$i$sd^2)
sqrt(1^2+res5$summary.fix[1,2]^2 + drop(A5pts3%*%rvar))
@ 

\subsection{Response on a grid}

The computation of all marginal posterior 
distributions on a grid is computationally expensive. 
But, we usually not uses the marginal distributions. 
We usually uses just the mean and standard deviation. 
So, we don't need the storage of all the marginal 
distributions! Also, we don't need the quantiles 
of the marginal distributions. 

On the code below, we build the model again 
but we disable the storage of the marginal 
posterior distributions to random effects and 
to posterior predictor values. 
Also, we disable the computation of the quantiles. 
Only the mean and standard defiation are stored. 

We use the projector matrix on the projector object 
that we use to project the posterior mean on the grid 
<<prespgrid>>=
stkgrid <- inla.stack(data=list(resp=NA), A=list(pgrid0$proj$A,1), 
                      effects=list(i=1:spde5$n.spde,
                        m=rep(1,101*101)), tag='prd.gr')
stk.all <- inla.stack(stk5, stkgrid)
res5g <- inla(resp ~ 0 + m + f(i, model=spde5), 
              data=inla.stack.data(stk.all), 
              control.predictor=list(A=inla.stack.A(stk.all), 
                compute=TRUE), quantiles=NULL, 
              control.results=list(return.marginals.random=FALSE, 
                return.marginals.predictor=FALSE))
res5g$cpu
@ 

We get the indexes 
<<indgr>>=
igr <- inla.stack.index(stk.all, 'prd.gr')$data
@ 
and use it to visualize, together the prediction  
of the random field on previous section,
on Figure~\ref{fig:pgrid} with the commands bellow
<<pgrid,eval=F>>=
require(gridExtra)
grid.arrange(levelplot(prd0.m, col.regions=topo.colors(99), main='latent field mean',
                       xlab='', ylab='', scales=list(draw=FALSE)), 
             levelplot(matrix(res5g$summary.fitt[igr,1], 101), 
                       xlab='', ylab='', main='response mean',
                       col.regions=topo.colors(99), scales=list(draw=FALSE)), 
             levelplot(prd0.s, col.regions=topo.colors(99), main='latent field SD',
                       xlab='', ylab='', scales=list(draw=FALSE)), 
             levelplot(matrix(res5g$summary.fitt[igr,2], 101), 
                       xlab='', ylab='', main='response SD',
                       col.regions=topo.colors(99), scales=list(draw=FALSE)), 
             nrow=2)
@ 
\setkeys{Gin}{width=0.99\textwidth}
\begin{figure}\centering 
<<echo=F,fig=TRUE,eps=F,width=7.5,height=7>>=
<<pgrid>>
@ 
\caption{The mean and standard deviation of the random field 
  (top left and bottom left) 
  and the mean and standard variation of the 
  response (top right and bottom right)}
\label{fig:pgrid}
\end{figure}

We see on Figure~\ref{fig:pgrid} that we have a 
variation from -4 to 4 on the spatial effect. 
Considering also that we have standard 
deviations around 0.8 to 1.6, the spatial 
dependence is significantly. 

Another thing is that the standard deviation 
of both, random field and the response, 
are less near the corner (0, 0) and greater 
near the corner (1,1). 
This is just proportional to the locations density. 

\section{Results from different meshes}\label{sec:meshcompare}

In this section we compare six results for 
the toy dataset based on the six different 
meshs builded on section~\ref{ch:mesh}. 
To do this comparison, we just plot the posterior 
marginal distributions of the model parameters. 
We evaluate the meshes by the addiction of 
the true values used on the simulation 
of the toy dataset. Also, we add the 
maximum likelihood estimates from 
\pkg{geoR} package, \cite{geoR}. 


We fit the model, using each one of the six meshes, 
and put the results in a list with the code bellow 
<<meshes,echo=F,results=hide>>=
lrf <- lres <- l.dat <- l.spde <- l.a <- list()
for (k in 1:6) {
  l.a[[k]] <- inla.spde.make.A(get(paste('mesh', k, sep='')), loc=coords)
  l.spde[[k]] <- inla.spde2.matern(get(paste('mesh', k, sep='')), alpha=2)
  l.dat[[k]] <- list(y=SPDEtoy[,3], i=1:ncol(l.a[[k]]), 
                     m=rep(1, ncol(l.a[[k]])))
  lres[[k]] <- inla(y ~ 0 + m + f(i, model=l.spde[[k]]), 
                    data=l.dat[[k]], control.predictor=list(A=l.a[[k]]))
  lrf[[k]] <- inla.spde2.result(lres[[k]], 'i', l.spde[[k]], do.transf=TRUE)
}
@ 

The mesh size influences the computational 
time needed to fit the model. 
More nodes on the mesh need more computational time.
The time running inla for these six meshes are
<<time>>=
round(sapply(lres, function(x) x$cpu[2]), 2)
@ 

We compute the distribution for $\sigma_e^2$ 
for each fitted model
<<s2marg>>=
s2.marg <- lapply(lres, function(m) 
                  inla.tmarginal(function(x) 1/x, m$marginals.hy[[1]]))
@ 

The true values are: $\beta_0=10$, $\sigma_e^2=0.3$, 
$\sigma_x^2=5$, $\kappa=7$ and $\nu=1$. 
The $\nu$ parameter is fixed on the true value when 
we define $\alpha=2$ on definition of the SPDE model. 
<<truepars>>=
beta0 <- 10; sigma2e <- 0.3; sigma2x <- 5; kappa <- 7; nu <- 1
@ 
and the maximum likelihood estimates are 
<<lkv>>=
lk.est
@ 

We want to visualize the posterior marginal 
distributions for $\beta_0$, $\sigma_e^2$, 
$\sigma_x^2$, $\kappa$, nominal range and 
the local variance $\tau$. 
This can be done with the code bellow 
<<compare,eval=F>>=
rcols <- rainbow(6)##c(rgb(4:1/4,0:3/5,0), c(rgb(0,0:3/5,4:1/4)))
par(mfrow=c(2,3), mar=c(2.5,2.5,1,.5), mgp=c(1.5,.5,0), las=1)

xrange <- range(sapply(lres, function(x) range(x$marginals.fix[[1]][,1])))
yrange <- range(sapply(lres, function(x) range(x$marginals.fix[[1]][,2])))
plot(lres[[1]]$marginals.fix[[1]], type='l', xlim=xrange, ylim=yrange, 
     xlab=expression(beta[0]), ylab='Density')
for (k in 1:6)
  lines(lres[[k]]$marginals.fix[[1]], col=rcols[k], lwd=2)
abline(v=beta0, lty=2, lwd=2, col=3) 
abline(v=lk.est[1], lty=3, lwd=2, col=3)

xrange <- range(sapply(s2.marg, function(x) range(x[,1])))
yrange <- range(sapply(s2.marg, function(x) range(x[,2])))
plot.default(s2.marg[[1]], type='l', xlim=xrange, ylim=yrange, 
             xlab=expression(sigma[e]^2), ylab='Density')
for (k in 1:6) 
  lines(s2.marg[[k]], col=rcols[k], lwd=2)
abline(v=sigma2e, lty=2, lwd=2, col=3) 
abline(v=lk.est[2], lty=3, lwd=2, col=3)

xrange <- range(sapply(lrf, function(r) range(r$marginals.variance.nominal[[1]][,1])))
yrange <- range(sapply(lrf, function(r) range(r$marginals.variance.nominal[[1]][,2])))
plot(lrf[[1]]$marginals.variance.nominal[[1]], type='l', 
     xlim=xrange, ylim=yrange, xlab=expression(sigma[x]^2), ylab='Density')
for (k in 1:6)
  lines(lrf[[k]]$marginals.variance.nominal[[1]], col=rcols[k], lwd=2)
abline(v=sigma2x, lty=2, lwd=2, col=3) 
abline(v=lk.est[3], lty=3, lwd=2, col=3)

xrange <- range(sapply(lrf, function(r) range(r$marginals.kappa[[1]][,1])))
yrange <- range(sapply(lrf, function(r) range(r$marginals.kappa[[1]][,2])))
plot(lrf[[1]]$marginals.kappa[[1]], type='l', 
     xlim=xrange, ylim=yrange, xlab=expression(kappa), ylab='Density')
for (k in 1:6)
  lines(lrf[[k]]$marginals.kappa[[1]], col=rcols[k], lwd=2)
abline(v=kappa, lty=2, lwd=2, col=3) 
abline(v=lk.est[4], lty=3, lwd=2, col=3)

xrange <- range(sapply(lrf, function(r) range(r$marginals.range.nominal[[1]][,1])))
yrange <- range(sapply(lrf, function(r) range(r$marginals.range.nominal[[1]][,2])))
plot(lrf[[1]]$marginals.range.nominal[[1]], type='l', 
     xlim=xrange, ylim=yrange, xlab='nominal range', ylab='Density')
for (k in 1:6)
  lines(lrf[[k]]$marginals.range.nominal[[1]], col=rcols[k], lwd=2)
abline(v=sqrt(8)/kappa, lty=2, lwd=2, col=3)
abline(v=sqrt(8)/lk.est[4], lty=3, lwd=2, col=3)

xrange <- range(sapply(lrf, function(r) range(r$marginals.tau[[1]][,1])))
yrange <- range(sapply(lrf, function(r) range(r$marginals.tau[[1]][,2])))
plot(lrf[[1]]$marginals.tau[[1]], type='l', 
     xlim=xrange, ylim=yrange, xlab=expression(tau), ylab='Density')
for (k in 1:6)
  lines(lrf[[k]]$marginals.tau[[1]], col=rcols[k], lwd=2)

legend('topright', c(paste('mesh', 1:6, sep=''), 'True', 'Likelihood'), 
       lty=c(rep(1,6), 2, 3), lwd=rep(2, 6), col=c(rcols,3,3), bty='n')
@ 

At the Figure~\ref{fig:margposttoy} we can see 
that the posterior marginal distribution for the 
intercept has mode on the likelihood estimate, 
considering the results from all six meshes. 
<<kappai>>=
1/kappa
@ 

The main differences are on the noise 
variance $\sigma_e^2$ (the nugget effect). 
The result from the mesh based on the points 
and with small triangles mode less than the 
likelihood estimate, the second has mode near 
likelihood estimate and the third large. 
Considering the other meshes, the mesh 
four has mode around likelihood 
estimate and the other two litle larger, 
similar to the third mesh, such is 
based on points but with some freedom 
(\code{cutoff} greather than zero). 

For the marginal variance of the latent field, 
$\sigma_x^2$, the results with all meshes had 
mode near the likelihood estimate. 
For the scale parameter $\kappa$ all meshes 
has mode less than the likelihood estimate. 
The posterior distribution from the meshes 
based on points are that ones with less mode 
and that the mode from third mesh are the less.
For the practical range the opposite happens. 

\begin{figure}\centering
<<echo=F,fig=TRUE,eps=FALSE,width=7.5,height=5>>=
<<compare>>
@ 
\caption{Marginal posterior distribution for 
$\beta_0$ (top left), $\sigma_e^2$ (top mid), 
 $\sigma_x^2$ (top right), $\kappa$ (bottom left), 
 nominal range (bottom mid) and $\tau$ (bottom right).}
\label{fig:margposttoy}
\end{figure}

These results are not conclusive, but a general 
comment is that is good to have a mesh with some 
tune on the points locations, to access noise variance, 
but with some flexibility to avoid many variability 
on the triangles size and shape, to get good 
latent field parameters estimation.
