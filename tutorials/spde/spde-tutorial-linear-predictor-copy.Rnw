\section{Copying part or the entire linear predictor}\label{sec:copy}

This content is part of the book available at
\url{http://www.r-inla.org/spde-book},
whose Gitbook version is freely available 
along all the code and datasets.


<<sett,echo=F,results='hide',message=FALSE,warning=FALSE>>=
library(knitr)
opts_chunk$set(
fig.path='figs/copy',
message=FALSE, warning=FALSE
)
options(width=77, prompt = " ", continue = "   ")
library(INLA)
lcall <- inla.getOption('inla.call')
inla.setOption(inla.call='remote')
inla.setOption(num.threads=8)
source('R/spde-tutorial-functions.R')
set.seed(1)
@ 

In this Chapter we show how to copy part of a linear predictor. 
%%in order to deal with the spatial data fusion problem. 
That is two or more outcomes (or same outcome from different sources) are modeled jointly. 
In this case there are effects which are common in the 
linear predictor for more than one outcome. 

Suppose we have data collected at the locations $\mb{s}$, 
$\mb{y}1(\mb{s})$, $\mb{y}2(\mb{s})$ and $\mb{y}3(\mb{s})$. 
Consider a case were we have the following three observation models 
\begin{eqnarray}
\mb{y}1(\mb{s}) & = & \beta_0 + \beta_1 \mb{x}(\mb{s}) + \mb{A}(\mb{s},\mb{s}0)\mb{b}(\mb{s}0) + \mb{\epsilon} 1(\mb{s})  \label{eq:y1obs} \\
\mb{y}2(\mb{s}) & = & \beta_2(\beta_0 + \beta_1 \mb{x}(\mb{s})) + \mb{\epsilon} 2(\mb{s})   \label{eq:y2obs} \\
\mb{y}3(\mb{s}) & = & \beta_3(\beta_0 + \beta_1 \mb{x}(\mb{s}) + \mb{A}(\mb{s},\mb{s}0)\mb{b}(\mb{s}0)) + \mb{\epsilon} 3(\mb{s})   \label{eq:y3obs} 
\end{eqnarray}
where we have a SPDE model at the mesh nodes $\mb{b}(\mb{s}0)$
with $\mb{A}(\mb{s},\mb{s}0)$ being the projector matrix, 
$\mb{\epsilon} j$, j=1,2,3, are observation errors 
considered as zero mean Gaussian with variance $\sigma^2_j$. 
By this setting we have a linear model for each outcome. 
We can see that a common effect is scaled from one linear preditor into another, 
where $\beta_2$ and $\beta_3$ are the scaling parameters.

We can define the following model terms 
\begin{itemize}
 \item $\mb{\eta}0(\mb{s}) = \beta_0 + \beta_1 \mb{x}(\mb{s})$ 
 \item $\mb{\eta}1(\mb{s}) = \mb{\eta}0(\mb{s}) + \mb{A}(\mb{s},\mb{s}0)\mb{b}(\mb{s}0)$ 
 \item $\mb{\eta}2(\mb{s}) = \beta_2\mb{\eta}0(\mb{s})$ 
 \item $\mb{\eta}3(\mb{s}) = \beta_3\mb{\eta}1(\mb{s})$ 
\end{itemize}
By this, this example is about showing how to copy 
$\mb{\eta}0$ into $\mb{\eta}2$ and 
$\mb{\eta}1$ into $\mb{\eta}3$ 
in order to estimate $\beta_2$ and $\beta_3$. 

We have assumed all the three 
observation vectors, $\mb{y}1$, $\mb{y}2$ and $\mb{y}3$ 
to be observed at the same locations. 
%By this assumption, $\mb{A}(\mb{s},\mb{s}0)$ is the same in the 
%equations for $\mb{y}1$ and $\mb{y}3$. 

\subsection{Generating data}

First of all we define the set of parameters in the model. 
for $\beta_j$, j=0,2,...,3 as follows 
<<truebeta>>=
beta0 = 5
beta1 = 1
beta2 = 0.5
beta3 = 2
@ 

Then, we define the variance errors as
<<s123true>>=
s123 <- c(0.1, 0.05, 0.15)
@ 

For the $b(s)$ process, we consider a Mat\'{e}rn 
covariance function with $\kappa_b$, $\sigma^2_b$ and $\nu=1$ (fixed). 
<<kappas2>>=
kappab <- 10
sigma2b <- 1
@ 

To realize the process we consider a set of locations as follows
<<loc>>=
n <- 50
loc <- cbind(runif(n), runif(n))
@ 
and draw one realization of the Mat\'{e}rn process 
considering the \texttt{rMatern()} function available in the file at 
\url{http://inla.r-inla-download.org/r-inla.org/tutorials/spde/R/spde-tutorial-functions.R}
<<>>=
b <- rMatern(n=1, coords=loc, kappa=10, variance=1)
@ 

This sample can be visualized using
<<vizloc,eval=FALSE>>=
par(mar=c(0,0,0,0))
plot(loc, asp=1, cex=0.3+2*(b-min(b))/diff(range(b)), 
     pch=19, axes=FALSE); box()
@ 
\setkeys{Gin}{width=0.39\textwidth}
\begin{figure}\centering
<<vvizloc,echo=F,results='hide',fig.width=5.5,fig.height=5.5>>=
<<vizloc>>
@ 
\caption{Locations}
\end{figure}\label{fig:loc}

Additionally we have to define a covariate. 
We just sample it as follows
<<covariate>>=
x <- runif(n, -1, 1)*sqrt(3)
@ 

Them we build the linear predictors as follows
<<predictor>>=
eta1 <- beta0 + beta1*x + b
eta2 <- beta2*(beta0 + beta1*x)
eta3 <- beta3*eta1
@
and have the observations as follows
<<obs>>=
y1 <- rnorm(n, eta1, s123[1])
y2 <- rnorm(n, eta2, s123[2])
y3 <- rnorm(n, eta3, s123[3])
@ 
%%Just to check
%%<<check>>=
%%solve(crossprod(cbind(1,x)),
%%    crossprod(cbind(1,x),y2))
%%@ 

\subsection{Setting the way to fit the model}

There are more than one way to fit this model using 
the \texttt{R-INLA} package. 
The main point is the need to compute 
$\mb{\eta} 0(\mb{s}) = \beta_0 + \beta_1 \mb{x}(\mb{s})$ and
$\mb{\eta} 1(\mb{s}) = \beta_0 + \beta_1 \mb{x}(\mb{s}) + \mb{A}(\mb{s},\mb{s}0)\mb{b}(\mb{s})$ 
from the first observation equation 
in order to copy it to the second and 
third observation equation.
So, one has to define a model that computes 
$\mb{\eta} 0(\mb{s})$ and $\mb{\eta} 1(\mb{s})$ explicitelly.

The way we choose is to minimize the size of the 
graph generated by the model, 
\cite{Rueetal:2017}. 
First, we have that 
\begin{eqnarray}
  \mb{0}(\mb{s}) & = & \mb{A}(\mb{s},\mb{s}0)\mb{b}(\mb{s}0) + \mb{\eta} 0(\mb{s}) + \mb{\epsilon} 1(\mb{s}) - \mb{y1}(\mb{s})   \label{eq:eta0} \\
  \mb{0}(\mb{s}) & = & \mb{\eta} 1(\mb{s}) + \mb{\epsilon} 1(\mb{s}) - \mb{y}1(\mb{s})   \label{eq:eta1}
\end{eqnarray}
where only $\mb{A}(\mb{s},\mb{s}0)$, $\mb{x}(\mb{s})$ and $\mb{y}1(\mb{s})$ are known. 
For the $\mb{\eta}0(\mb{s})$ and $\mb{\eta}2(\mb{s})$ terms we consider 
an independent and identically distributed ('iid') 
model with low fixed precision.  
With this high fixed hight variance, each element in 
$\mb{\eta}0(\mb{s})$ and $\mb{\eta}1(\mb{s})$ can assume any value. 
However, it will be forced these values to be 
$\beta_0 + \beta_1 \mb{x}(\mb{s})$ and 
$\beta_0 + \beta_1 \mb{x}(\mb{s}) + \mb{A}(\mb{s},\mb{s}0)\mb{b}(\mb{s}0)$ 
by considering a Gaussian likelihood for the 'faked zero' 
observations with a hight fixed precision value 
(lower fixed likelihood variance). 
For details and examples of this approach see 
\cite{cardenasKR:2012}, \cite{martinsSLR:2013} and 
Chapter~8 of \cite{blangiardoC:2015}. 

Second, since we have only Gaussian likelihood 
for the observed data, one can can include 
$\mb{\epsilon} j$ (j=1,2,3) in the linear predictor 
and fix a high likehihood precision. 
By this setting, we have only Gaussian likelihood 
with high fixed precision: only one likelihood. 

\subsection{Fitting the model}

In order to fit the $b(\mb{s})$ term we have to set a 
SPDE Mat\'{e}rn model. 
For that, we set a mesh 
<<mesh>>=
mesh <- inla.mesh.2d(loc.domain=cbind(c(0,1,1,0), c(0,0,1,1)),
                     max.edge=c(0.1, 0.3),
                     offset=c(0.05, 0.35), cutoff=0.05)
@ 
define the projector matrix 
<<As>>=
As <- inla.spde.make.A(mesh, loc)
@ 
and the SPDE model
<<spde>>=
spde <- inla.spde2.matern(mesh, alpha=2)
@ 
to be used.

The data has to be organized using the \texttt{inla.stack()} function. 
The data stack for the first observation vector is just
<<stack1>>=
stack1 <- inla.stack(tag='y1',
                     data=list(y=y1), 
                     effects=list(
                         data.frame(beta0=1, beta1=x),
                         s=1:spde$n.spde,
                         e1=1:n),
                     A=list(1, As, 1))
@ 
where the \texttt{e1} term will be used to fit $\mb{\epsilon} 1$. 

Them the stack data for the first 'facked zero' observations
<<stack01>>=
stack01 <- inla.stack(tag='eta1',
                      data=list(y=rep(0,n), offset=-y1),
                      effects=list(
                          s=1:spde$n.spde,
                          list(e1=1:n, 
                               eta1=1:n)),
                      A=list(As, 1))
@ 
which has the negated fist observation vector as offset. 

The stack for the second 'facked zero' observation is
<<stack02>>=
stack02 <- inla.stack(tag='eta2',
                      data=list(y=rep(0,n), offset=-y1),
                      effects=list(list(e1=1:n, eta2=1:n)),
                      A=list(1))
@ 

The stack for the second observation vector now considers 
an index set to compute the $\mb{\eta}1$ copied from the 
first 'facked zero' observations. 
<<stack2>>=
stack2 <- inla.stack(tag='y2',
                     data=list(y=y2),
                     effects=list(list(eta1c=1:n, e2=1:n)), 
                     A=list(1))
@ 

In similar way, we have the third observation stack including 
an index set to compute the $\mb{\eta}2$ copied from the 
second 'facked zero' observations. 
<<stack3>>=
stack3 <- inla.stack(tag='y3',
                     data=list(y=y3), 
                     effects=list(list(eta2c=1:n, e3=1:n)),
                     A=list(1))
@ 

To fit the model we join all the data 
<<stacks>>=
stack <- inla.stack(stack1, stack01, stack02, stack2, stack3)
@ 

The prior distributions will be the default for most of the parameters. 
For the three variance errors in the observations we have set the 
PC-prior as follows
<<pcprec>>=
pcprec <- list(theta=list(prior='pcprec', param=c(0.5, 0.1)))
@ 
And consider it when defining the model formula
<<formula123>>=
formula123 <- y ~ 0 + beta0 + beta1 + 
    f(s, model=spde) + f(e1, model='iid', hyper=pcprec) +
    f(eta1, model='iid',
      hyper=list(theta=list(initial=-10, fixed=TRUE))) + 
    f(eta2, model='iid',
      hyper=list(theta=list(initial=-10, fixed=TRUE))) + 
    f(eta1c, copy='eta1', fixed=FALSE) +
    f(e2, model='iid', hyper=pcprec) +
    f(eta2c, copy='eta2', fixed=FALSE) +
    f(e3, model='iid', hyper=pcprec)
@ 

The model is them fitted with
<<res123>>=
res123 <- inla(formula123,
               data=inla.stack.data(stack),
               offset=offset,
               control.family=list(list(
                   hyper=list(theta=list(initial=10, fixed=TRUE)))),
               control.predictor=list(A=inla.stack.A(stack)))
@ 

\subsection{Model results}

We can see the fixed effects $\beta_0$ and $\beta_1$ with
<<fixed>>=
round(cbind(true=c(beta0,beta1),res123$summary.fixed), 4)
@ 

The $\beta_2$ and $\beta_3$ parameters are 
<<copy12>>=
i.b <- match(paste0('Beta for eta', 1:2, 'c'),
             rownames(res123$summary.hyper))
round(cbind(true=c(beta2, beta3), res123$summary.hy[i.b,]), 4)
@ 

The posterior marginal distribution of the 
standard deviation for $\mb{\epsilon} 1$, 
$\mb{\epsilon} 2$ and $\mb{\epsilon} 1$ 
can be visualized with
<<s123,eval=FALSE>>=
i.e123 <- match(paste0('Precision for e', 1:3),
                names(res123$marginals.hyper))
par(mfrow=c(1,3), mar=c(3,3,1,1), mgp=c(1.5, 0.5, 0))
for (j in 1:3) {
    plot(inla.tmarginal(function(x) 1/sqrt(x),
                        res123$marginals.hyperpar[[i.e123[j]]]),
         type='l', lwd=2, xlab=bquote(sigma[.(j)]^2), 
         ylab='Posterior marginal density')
    abline(v=s123[j], lwd=2)
}
@ 
\setkeys{Gin}{width=0.99\textwidth}
\begin{figure}\centering
<<vs123,echo=F,results='hide',fig.width=7.5,fig.height=3>>=
<<s123>>
@ 
\caption{Observation error standard deviations.}
\end{figure}\label{fig:s123}

Finally, the random field parameters posterior marginals 
can be visualized with
<<rfparams,eval=FALSE>>=
rfparams <- inla.spde2.result(res123, 's', spde)
par(mfrow=c(1,3), mar=c(3,3,1,1), mgp=c(1.5, 0.5, 0))
for (j in 15:17) {
    plot(rfparams[[j]][[1]], xlab=names(rfparams)[j],
         type='l', lwd=2, ylab='Posterior marginal density')
    abline(v=c(10, 1, sqrt(8)/10)[j-14], lwd=2)
}
@ 
\begin{figure}\centering
<<vrfparams,echo=F,results='hide',fig.width=7.5,fig.height=3>>=
<<rfparams>>
@ 
\caption{Posterior marginal distributions for the random field parameters} 
\end{figure}\label{fig:rfparams}

