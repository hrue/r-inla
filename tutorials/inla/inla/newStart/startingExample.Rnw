\chapter{How to use \tv{R-INLA}: A first overview}
<<echo=FALSE>>=
library(INLA)
@

There are essentially four parts to an \tv{R-INLA}-program: 1) Organise 
the data and store response, covariate, \ldots, in an object; 2) Use 
the \tv{formula} notation to specify the model; 3) Call the 
\tv{inla}-program; 4)) Extract posterior information of interest.\\[0.2cm]

Before we go into detail on how these four steps proceed, consider a simple 
example describing the world records in ski jumping from
1961 to 2011. Figure~\ref{fig:ski} shows the obtained length in meters for 
the corresponding years.
\begin{figure}
\begin{center}
<<echo=FALSE, fig.width=10, fig.height=7, out.width=".8\\textwidth">>=
a = read.table("examples/ski.txt", header=F, sep="\t")
year = sort(as.numeric(substr(a[,4], 1,4)), index.return=T)

as = a[year$ix, ]

b=cbind(as[,1], year$x)

yearu = unique(year$x)

m = matrix(0, ncol=2, nrow=length(yearu))

for(i in 1:length(yearu)){
	t =yearu[i] - 1960
	len = max(b[b[,2] ==yearu[i],1])
	m[i,] = c(t,len)
}
par(mar=c(5,5,5, 0.5), cex.lab=1.3, cex.axis=1.3, cex.main=1.4)
plot(m[,1], m[,2], xlab="year", 
	ylab="Length in meters",
	pch=19, xaxt="n")
axis(1, at=seq(1, 51, by=5), label=seq(1961, 2011, by=5))
res = lm(m[,2] ~ m[,1])
abline(res, col=2)
legend("topleft", c("Linear model"), col=2, lty=1)
@
\caption{World records in ski jumping, 1961--2011.\label{fig:ski}}
\end{center}
\end{figure}
Obviously, there seems to be a linear relationship in the data. Thus, let us 
assume a simple linear regression model with
\begin{align*}
  E(y_i) &= \mu + \beta x_i,& Var(y_i) = 1/\tau.
\end{align*}
for $i=1, \ldots, n$. Here $y_i, i=1, \ldots, n$ represents the obtained jump 
length of the record and $x_i, i=1, \ldots, n$, an index for the year in which 
the record was obtained. Fitting a linear model in R using:
<<eval=FALSE>>=
  lm(y ~ x)
@
leads to the red line indicated in Figure~\ref{fig:ski}, with intercept equal 
to $\Sexpr{format(coef(res)[1], digits=2, nsmall=2)}$ 
(SD: $\Sexpr{format(summary(res)$coefficients[1,2], digits=2, nsmall=2)}$) and
slope $\beta$ equal to $\Sexpr{format(coef(res)[2], digits=2, nsmall=2)}$ 
(SD: $\Sexpr{format(summary(res)$coefficients[2,2], digits=2, nsmall=2)}$).

Alternatively we can use a Bayesian approach, where priors are assigned to the 
parameters $\mu$, $\beta$ and $\tau$. Delaying the issue of choosing priors to 
Section TODO, assume we use the default priors of \tv{R-INLA}. Then we can
define the linear model in using

\begin{itemize}
	\item[1.]
		\textbf{Data organisation}: Make an object to store 
response and covariate information\\[0.2cm]
		\hspace{0.5cm}\verb|data = list(y = y, x = x)|
	\item[2.]
		\textbf{Use the \tv{formula}-notation} to specify the 
model (similar to \tv{lm} and \tv{glm} functions)\\[0.2cm]
		\hspace{0.5cm}\verb|formula = y~x|
	\item[3.]
		\textbf{Call the \tv{inla}-program}\\[0.2cm]
		\hspace{0.5cm}
	\verb|res = inla(formula, data=data, family="gaussian")|
	\item[4.] \textbf{Extract posterior information}, e.g.~for a 
first overview use \\[0.2cm]
		\hspace{0.5cm}\verb|summary(res)|
\end{itemize}

The obtained summary of the posterior estimates in step 4. is as follows:
<<echo=FALSE>>=
data = data.frame(y=m[,2],x=m[,1])
library(INLA)
formula = y~x
result = inla(formula, family="gaussian", data=data)
summary(result)
@

Here, various information is shown. First the inla-call is repeated and then
time need to fit the model is given. Next posterior summary information for all 
fixed effects is provided. In this example there are two fixed effects, the 
intercept $\mu$ and the regression coefficient $\beta$ belonging to the 
covariate $x$. Per default posterior mean, standard deviation, as well as the 
$2.5\%, 50\%, 97.5\%$ quantiles and the posterior mode are given. 
\textcolor{red}{Mention what KLD means}.

Next, there are comes a note that there are no random effects in the model. 
Analogously to the fixed effects we obtain posterior summary statistics for 
each hyperparameter, such as precision or correlation parameters, in the model. 
Here, we only have the precision parameter of the observation noise $\tau$. 

The expected number of effective parameters represents the number of
independent parameters included in the model and is followed by the number of 
equivalent replicates which indicates the available information (in
terms of sample size) per effective parameter. Finally an estimate of the 
marginal log-likelihood is provided which can be used for model choice, see 
Section~TODO.

Comparing the results with the estimates obtained in the classical framework we 
see that the results are very similar.

\section{Data organisation}

In order to estimate a model in \tv{R-INLA}, all data needed to 
define the model have to be collected in one single \tv{R}-object of type 
\tv{list} or \tv{data.frame}. Assume we have a response \texttt{y}, covariates 
\texttt{x1} and \texttt{x2}, and a time index \texttt{t}. Then can they be 
organized in one object using

<<eval=FALSE>>=
# Option 1: Generate a list object
data = list(y = y, x1 = x1, x2 = x2, t = t)

# Option 2: Generate a data.frame
data = data.frame(y = y, x1 = x1, x2 = x2, t = t)
@

\section{Specifying the linear predictor}

Similar to the well-know functions \tv{lm} and \tv{glm} in R, is the model 
specified using a \tv{formula} object. A \tv{formula} is a compact symbolic 
description of the model. The $\sim$ operator is basic in the formation of 
such models.  An expression of the form \tv{y} $\sim$ \tv{model} means that the 
response $\tv{y}$ is modelled by a linear predictor specified symbolically by 
$\tv{model}$. Such a model consists of a series of terms separated by \tv{+} 
operators, such as
<<eval=FALSE>>=
formula = y ~ x1 + x2 + f(t, ...)
@
Here, \tv{y} is the name of the response in the object \tv{data}. The 
fixed effects are given i.i.d Gaussian priors with mean zero and low inverse 
variance (default: $0.01$).The \tv{f} function is used to specify random 
effects, such as temporal or spatial effects or smooth effects of covariates,
see Section TODO. An intercept is included automatically and can be removed
by adding \tv{-1} to the formula.


\section{Calling \tv{inla}}

After the data are organised and the model is specified we can call the 
\tv{inla} function, which in its basic form is
<<eval=FALSE>>=
result = inla(
    # Description of linear predictor
    formula,
    # Likelihood
    family = "gaussian",
    # List or data frame with response, covariates, etc.
    data = data,
    
    ## This is all that is needed for a basic call  
    # check what happens
    verbose = TRUE,
    # keep working files
    keep = TRUE
    
    # there are also some "control statements"
    # to customize things)
@

\section{Inspecting the posterior results}

Besides inspecting the results as obtained by the \tv{summary} function, we 
can directly look at all marginal posterior densities. The marginal posterior 
densities of all fixed effects are saved in the slot \tv{marginals.fixed}. This 
is a list whose length is equal to the number of fixed effects (including the 
intercept), i.e. in our case equal to $2$.
<<>>=
  names(result$marginals.fixed)
  length(result$marginals.fixed)
@
Each marginal posterior density is represented as a matrix with two columns, 
the x-values and the correponding y-values, i.e. the density values at location 
$x$. 

<<>>=
   m  = result$marginals.fixed[[1]]
   str(m)
   head(m)
@
Thus, we can simply visualize a posterior marginal by plotting the y versus 
the x values, see Figure~\ref{fig:marg1}. 
\begin{figure}
\begin{center}
<<echo=FALSE, fig.width=10, fig.height=7, out.width=".6\\textwidth">>=
   plot(m, pch=20, cex=2)
@
\caption{Illustration of the posterior marginal density of the intercept $\mu$ 
in the ski jumping example.\label{fig:marg1}}
\end{center}
\end{figure}
The marginal is only represented by limited number of pairs. To get a more 
smooth representation we can use the function \tv{inla.smarginal} which 
interpolates values between the provided points using a spline. 
Figure~\ref{fig:marg2} shows the corresponing smoother representation.
\begin{figure}
\begin{center}
<<echo=FALSE, fig.width=10, fig.height=7, out.width=".6\\textwidth">>=
   plot(inla.smarginal(m), pch=20, cex=2)
@
\caption{Illustration of the posterior marginal density of the intercept $\mu$ 
in the ski jumping example in a higher resolution.\label{fig:marg2}}
\end{center}
\end{figure}

The summary result for the fixed effects can be acessed using
{\small
<<>>=
   print(result$summary.fixed)
@
}
which includes per defaultt posterior mean, standard deviation, as well as the 
$2.5\%, 50\%, 97.5\%$ quantiles and the posterior mode in form of a matrix, 
where each line corresponds to one fixed effect.  The mean of all fixed effects
is extracted using
{
<<>>=
   result$summary.fixed[,1]
@
}

Based on the posterior marginal distribution also other quantiles of interest 
can be computed after running the model. For example, assume interest lies in 
the $10\%$ posterior quantile for the intercept, then we can call the function 
\tv{inla.qmarginal} as follows
<<>>=
   inla.qmarginal(0.1, m)
@
Here, the first entriy is the quantile of interest and the second is the 
posterior marginal density as obtained from \tv{inla()} which is a matrix with 
two columns where the first column gives the locations and the second the 
density values at these locations.

Similar to \tv{inla.qmarginal} there exist the function \tv{inla.dmarginal} and 
\tv{inla.pmarginal} to compute the CDF and density function of a posterior 
marginal.

<<>>=
   inla.dmarginal(137, m)
   inla.pmarginal(140, m)
@

The function \tv{inla.emarginal()} can be used to compute the expected value of
a transformed quantity. Consider, for example,

<<>>=
   E=inla.emarginal(function(x){return(c(x,x^2))}, m)
   E
@

This gives us the posterior mean (which we already now) and the posterior mean 
for $x^2$. From this we can compute the posterior standard deviation as

<<>>=
   sqrt(E[2] - E[1]^2)
@

We can compare the obtained quantity to the posterior standard deviation 
returned in the summary information for the intercept

<<>>=
   result$summary.fixed[1, 1:2]
@

and see that the value fits very well. The functions \tv{inla.qmarginal}, 
\tv{inla.dmarginal}, \tv{inla.pmarginal} and \tv{inla.emarginal} can be applied
to any posterior marginal distribution. To ease computations INLA works with 
inverse variance, i.e. precision parameters, instead of variance parameters. 
However, it is easy to transform the returned summary information for the 
precision into summary information for the variance.

In our simple model we only have one hyperparameters namely the precision 
parameter for the observational noise. The corresponding postrior marginal can 
be found in
<<>>=
  tau0 = result$marginals.hyperpar$"Precision for the Gaussian observations"
@

If we are interested in the posterior mean and posterior standard deviation of 
the variance we can simply use the function \tv{inla.emarginal} as follows

<<>>=
  # compute the expected value of 1/x and 1/x^2
  E = inla.emarginal(function(x){return(c(1/x, 1/x^2))}, tau0)
  
  # compute the standard deviation
  mysd = sqrt(E[2] - E[1]^2)
  
  print(c(mean=E[1], sd=mysd))
@

In this section we focused on a short overview of \tv{R-INLA}. We have looked 
at a simple linear model and learned how to get and inspect posterior results
for fixed effects and hyperparameters. The following sections will investigate 
all things presented here in more detail. In addition, we will describe how to 
extend the model formulation by including random effects or linear combinations 
of various components in the latent field, for example. We discuss how to 
obtain predictions and different tools available for model choice. Finally we 
discuss more advanced modelling options.


% 
% Provide an example that illustrates the most central point of R-INLA
% and also points common to several examples. Thus, maybe an
% interesting/popular example including fixed and random effects.
% 
% Show data structure, model definition, inla call, output + interpretation.
% 
% Goal: After reading this section the reader should already have
% an idea about how \verb|R-INLA| works, its structure, output and some details.
% 
