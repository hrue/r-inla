\chapter{Model specification}
<<echo=FALSE>>=
library(INLA)
@

The first thing to do for running \tv{R-INLA} is to specify the model of
interest in the right way.

\section{Bayesian hierarchical models}

INLA can be used to estimate Bayesian hierarchical models defined over
three stages. The first
stage describes the distibribution of the response vector $\bs{y}=(y_1,
\ldots, y_{n_d})^\top$ and is formed by a likelihood function with conditional independence
properties given the ($n_d \times 1$) vector of linear predictors
$\bs{\eta}$ and the ($m \times 1$) vector of possible hyperparameters
$\bs{\theta}_1$
\begin{list}{\labelitemi}{\leftmargin=5em}
\item[\textbf{Stage 1.}] $\bs{y}|\bs{\eta},\bs{\theta}_1 \sim
    p(\bs{y}|\bs{\eta},\bs{\theta}) = \prod _{i=1}^{n_d} p(y_i|g^{-1}
    (\eta _i), \bs{\theta}_1)$,
\end{list}
where $g^{-1}(\cdot)$ is the inverse of the link function that connects the
conditional mean of $y_i$ to the linear predictor $\eta _i$.  The
second stage defines the distribution of the underlying unobserved
(latent) components. This stage is the systematic part of the model and states
that the covariates are linearly related to the linear predictor. This
stage can include fixed effects for covariates, unstructured random
effects, such as individual or group effects, or structured random
effects, such as an autoregressive model of order 1 or a smoothin
random effect over space. It is crucial that the second stage, the
so-called latent level, is Gaussian
distributed in order to apply INLA. In matrix notation we have,
\begin{list}{\labelitemi}{\leftmargin=5em}
\item[\textbf{Stage 2.}] $\bs{\eta} = \bs{X} \bs{\beta} + \bs{w}
  f(\bs{c}, \bs{\theta}_2) + \ldots, \quad
    \bs{\beta} \sim N(\bs{\mu}, \text{diag}(\bs{\tau}^{-1}))$,
\end{list}
where $\bs{X}$ is a ($n_d \times p$) design matrix which has the $p$
covariate vectors as its columns and $\bs{\beta}$ is a ($p \times 1$)
vector of regression coefficients, i.e. fixed effects. INLA attributes independent
Gaussian priors with mean $\mu _i$ and precision $\tau_i$ for each of
the regressor coefficients $\beta_i$, $i = 1,...,p$. The term $\bs{w}
f(\bs{c}, \bs{\theta}_2)$ specifies a non-linear/smooth Gaussian effect of
covariate $\bs{c}$  weighted with a vector of known weights $\bs{w}$
and $\bs{\theta}_2$ represent the hyperparameters for this Guassian
model. The hierarchical
model is then completed with an appropriate prior distribution for the
possible vector of hyperparameters $\bs{\theta} = (\bs{\theta}_1,
\bs{\theta}_2)$
which controls the
single model components
\begin{list}{\labelitemi}{\leftmargin=5em}
  \item[\textbf{Stage 3.}] $\bs{\theta} \sim \pi(\bs{\theta})$.
\end{list}
Here, $\bs{\theta}_1$ denots the hyperparameters of the likelihood
model which can include the variance of the observation noise, the
dispersion parameter in the negative binomial model, the probabiliy of
observing zero counts in a zero-inflated model. In contrast,
$\bs{\theta}_2$ includes all hyperparameters from the second stage,
e.g.~variance parameters of structured or unstructured random effects,
correlation parameters for multivariate effects, autocorrelation
parameters, etc.


% We start the model specification by looking at
% the linear predictor $\bm{\eta}$. For
% example, assume we have a generic model
% $y_i \stackrel{\text{iid}}{\sim} p(y_i \mid \lambda_i)$, where
% the link function $h(\cdot)$ is used to link the mean $\lambda$ to
% the linear predictor:
% \begin{equation}\label{eq:exampleLinPred}
% 	\eta_i = h(\lambda_i) = \beta_0 + \beta_1 X_{i} + \beta_2 Z_{i} +
% f(W_i) + \ldots
% \end{equation}
% Here, $\bm{\beta} = (\beta_0, \beta_1, \beta_2)$ denote ``fixed'' effects
% where $\beta_0$ is the intercept and $\beta_1$ and $\beta_2$ describe a
% linear effect of the observed covariates $\bm{X}$ and $\bm{Z}$.
% The term $f(W_i)$ specifies a non-linear/smooth effect of the covariate
% $\bm{W}$.
% The specification on the level of the linear predictor is regarded as
% the latent structure and strictly speaking this structure has
% to be Gaussian in order to be able to use \tv{R-INLA}.

 Within \tv{R-INLA} a
structure, such as the one
provided in stage 2 is specified through a $\tt{formula}$
object:
<<eval=FALSE>>=
  formula = y ~ X + f(c, w, model="...", ...)
@
A formula object is thereby composed of two different types of terms,
fixed effects just added to the model without additional specification, see
previous chapter, and random effects specified using the ${\tt f(.)}$ function.
Here, additional arguments to describe the underlying latent model are
available.
Both terms will be described in more detail in following section.


\section{Define the latent structure}

\subsection{Fixed effects}

The notation used to include fixed effects is analogous to the use in the
\tv{glm} or \tv{lm} function in \tv{R}, see also \tv{?formula} within
\tv{R}\footnote{One exception is that the specification
    ``\tv{y $\sim$ .}'' is not allowed.}.
Table~\ref{tab:fixed} list symbols and their meaning for specifying fixed
effects within a \tv{formula}.
\begin{table}
\begin{tabular}{lcl}
\toprule
Symbol & Example & Meaning\\
\midrule
\tv{+} & \tv{+ X} & include this variable\\
\tv{:} & \tv{X:Z} & include the interaction between these variables\\
\tv{*} & \tv{X*Y} & include these variables and the interaction
between them\\
\tv{1} & \tv{- 1} & intercept: delete the intercept (regress through the
origin)\\
\bottomrule
\end{tabular}
\caption{Symbols used to include effects in a \tv{formula}
object. \label{tab:fixed}}
\end{table}

Importantly, the use of \tv{+} in the \tv{formula} context is different than
its usual meaning. Within the \tv{formula} notation it is a short-hand for
which variable to include in the statistical model.  Note that an intercept
$\beta_0$ is automatically added to the model,
unless it is explicitly removed by adding term $-1$ or $0$:
<<eval=FALSE>>=
  formula = y ~ -1 + ...
  formula = y ~ 0 + ...
@
Similarly, adding term $1$ makes the intercept appear explicitly
<<eval=FALSE>>=
  formula = y ~ 1 + ...
@
There is often more than one way to specify a specific model; the notation is
not unique. For example the following formulae are all equivalent:
<<eval=FALSE>>=
  formula = y ~ 1 + X + Z + X:Z
  formula = y ~ 1 + X*Z
  formula = y ~ X*Z
@
each corresponding to
\begin{equation}\label{eq:exampleLinPred}
	\eta_i = h(\lambda_i) = \beta_0 + \beta_1 X_{i} + \beta_2 Z_{i} +
	\beta_3 X_iZ_i
\end{equation}
A small example will illustrate these features.
<<>>=
n = 100
x = rnorm(n)
z = rnorm(n)
y = 1 + x + z + x*z + rnorm(n, sd=0.5)

formula = y ~ x*z
result = inla(formula, data = data.frame(y=y, x=x, z=z))
result$summary.fixed
@
Note that we need to pass a \tv{data.frame} or a \tv{list} containing the
explanatory variables to the field \tv{data} in the functin \tv{inla()}.

It is also possible to use the design matrix $\mathbf{X}$ explicitly as in
<<eval=FALSE>>=
  formula = y ~ X
@
An alternative specification of the last example, is
<<>>=
formula = y ~ X
X = cbind(x=x, z=z, "x:z"=x*z)
result = inla(formula, data = list(y=y,X=X))
result$summary.fixed
@
%%
\tv{R-INLA} adds prefix \tv{X} to the fixed effects names, to make the
connection to the design-matrix $\mathbf{X}$ explicit.

\tv{R-INLA} attributes independent Gaussian priors for each fixed effect of the
model. The mean and precision of each of the priors can be controlled
using the argument \tv{control.fixed} of the \tv{inla}
function.  The argument \texttt{control.fixed} needs to be a list, and
the arguments relevant for the prior specification of the fixed
effects are:
\begin{itemize}
\item \texttt{mean.intercept}: prior mean for the intercept. Default value is
    \Sexpr{inla.set.control.fixed.default()$mean.intercept}
\item \texttt{prec.intercept}: precision for the intercept. Default
    value is
\Sexpr{inla.set.control.fixed.default()$prec.intercept}\footnote{Note
        that the this prior is improper!}
\item \texttt{mean}: prior mean for all fixed effects except the
    intercept.  Default is
    \Sexpr{inla.set.control.fixed.default()$mean}.  Alternatively, a
    named list with specific means where \texttt{name = default}
    applies to unmatched names. For example
    \begin{center}
        \texttt{control.fixed = list(mean = list(a = 1, b = 2, default = 0))}
    \end{center}
    assign \texttt{mean = 1} to fixed effect \texttt{a}, \texttt{mean
        = 2} to effect \texttt{b} and \texttt{mean = 0} to all others.
\item \texttt{prec}: Similar to the \texttt{mean} argument above,
    \texttt{prec} set the precision for all fixed effects except the
    intercept.  Default is
    \Sexpr{inla.set.control.fixed.default()$prec}.  Alternatively, a
    named list with specific precisions where \texttt{name = default}
    applies to unmatched names.
\end{itemize}
As an example, assume we have a model with an intercept and four fixed
effects \tv{a}, \tv{b}, \tv{c} and \tv{d} and we want the intercept to
have mean $0$ and precision $10$, the fixed effect \tv{b} to have mean
$3$ and precision $5$ and the remaining to have mean $10$ and
precision $20$. Then the prior specification in this case would be
<<eval=FALSE,echo=TRUE>>=
control.fixed = list(mean.intercept = 0,
                     prec.intercept = 10,
                     mean = list(b = 3, default = 10),
                     prec = list(b = 5, default = 20))
inla(..., control.fixed = control.fixed)
@


\subsection{Random effects}

Within a linear predictor we are usually not only interested in including
fixed effects but also random effects.
Here, we describe the specification of Gaussian random effects models of the
form
\begin{equation*}
  \mm{w} \cdot f(\mm{c}; \mm{\theta}),
\end{equation*}
where $\mm{w}$ denotes a vector of known weights for the effect and
$\mm{\theta}$ represents the hyperparameters of the Gaussian model.

A random effects model can be defined using the \tv{f()}-function.
This function allows the user to specify the latent model to be used, the prior
distributions for its hyperparameters $\mm{\theta}$, linear constraints,
and much more. In this section we will describe the most important
features of the \tv{f()}-function. More advanced topics like replicating
a latent model, or grouping several latent models will be described later in
detail.

The basic form of the \tv{f()}-function is:
<<eval=FALSE,echo=TRUE>>=
  f(c, w, model="...", ...)
@
where the first argument \tv{name} refers to the covariate to be modelled,
the second argument is n optional vector of known weights $\mm{w}$ (one for
each covariate value) and \tv{model} defines the type of the latent model.
For example, to define an independent and identically distributed (iid) random
effect for each observation, we set $\tv{c}=1,2,\ldots,n$, with $n$
denoting the total number of observations,
leading to:
<<eval=FALSE,echo=TRUE>>=
  f(c, model="iid")
@
A smooth effect, e.g.~a random walk of second order (RW2),
of a covariate $\mm{c}=(c_1, \ldots, c_n)^\top$, assuming
covariate values $c_i$ for observation $i$, can be obtained by
<<eval=FALSE,echo=TRUE>>=
  f(c, model="rw2")
@
The vector $c$ needs consequently to be added to the {\tt data.frame}
or {\tt list}, which is then passed to the field {\tt data} in the
function {\tt inla}. It is important to note that each {\tt f}-function
needs its own index/covariate. That means if we consider a time series
model for $T$ time points where we would like to have an temporal
unstructured effect, i.e.~iid, and a temporal structured effect, a RW1
say, then we have to define the temporal index twice:
<<eval=FALSE,echo=TRUE>>=
  formula = y ~ f(t, model="iid") + f(t2, model="rw1") + ...
@
We call the {\tt inla} function as
<<eval=FALSE,echo=TRUE>>=
  result = inla(formula, family="...", data=data.frame(y=y, t=1:T, t2=1:T, ...), ...)
@
This means that we cannot use the time index {\tt t} twice, once for the {\tt iid} and
once for the {\tt rw1} model.



Table~\ref{tab:latentModels} gives an overview of all implemented latent
Gaussian models.
\begin{table}[h!]
\begin{center}
\begin{tabular}{ll}
\toprule
Model & Identifier\\
\midrule
Independent random noise &  {\tt iid}\\
Linear & {\tt linear}\\
Constrained linear & {\tt clinear}\\
Random walk of order 1  &     {\tt rw1  }\\
Random walk of order 2  &    {\tt rw2 }\\
Continuous random walk of order 2 &     {\tt crw2   }\\
Model for seasonal variation &     {\tt seasonal  }\\
Model for spatial effect (know as Besag model)  &   {\tt besag   }\\
Model for spatial effect (proper version)  &    {\tt besagproper    }\\
Model for weighted spatial effects   &    {\tt besag2 }\\
Model for spatial effect + random effect (known as Besag-York-Mollie model) &
    {\tt bym }\\
Autoregressive model of order 1  &     {\tt ar1 }\\
Autoregressive model of order p &     {\tt ar}\\
The Ornstein-Uhlenbeck process  &    {\tt ou}\\
User defined structure matrix, type 0  &    {\tt generic0 }\\
User defined structure matrix, type 1   &    {\tt generic1  }\\
User defined structure matrix, type 2  &     {\tt generic2   }\\
Model for correlated effects with Wishart prior &{\tt iid1d, iid2d, iid3d}\\
  (dimension 1, 2, 3, 4 and 5) &      {\tt iid4d, iid5d}\\
Classical random effect model   &    {\tt z   }\\
Random walk of order 2 on a lattice   &    {\tt rw2d  }\\
Gaussian field with Matern covariance function  &    {\tt matern2d   }\\
Classical measurement error model &      {\tt mec     }\\
Berkson measurement error model  &    {\tt meb }\\
Spatial lag model & {\tt slm}\\
Sigmodial and reverse sigmodial & {\tt sigm}, {\tt revsigm}\\
\bottomrule
\end{tabular}
\caption{List of latent Gaussian field models implemented in
\tv{R-INLA}.}\label{tab:latentModels}
\end{center}
\end{table}
Specific documentation for each model can be found on
\url{http://www.r-inla.org/models/latent-models}. Within {\tt R} model
documentation can be requested using the function {\tt inla.doc(<identifier>)},
e.g.~{\tt inla.doc("rw1")}.
This will open a documentation pdf-file, with the following structure
\begin{enumerate}
        \item Description of the model parameterization.
        \item Definition of the hyperparameters.
        \item Model specification using R-INLA.
        \item Hyperparameter specification and default values of the model.
        \item Illustration of usage using a small simulated example.
\end{enumerate}

% \begin{example}
Consider a simple regression model including one linear covariate $\bm{X}$ and
a random effect $\mathbf{a}=(a_1,\dots,a_n)$ that follows an
autoregressive models of order 1 (AR (1)) and is defined as:
\begin{eqnarray}\nonumber
    a_1&\sim&\mathcal{N}(0,(\tau(1-\rho^2))^{-1}) \\\nonumber
    a_i&=&\rho\ a_{i-1}+\epsilon_i; \qquad
\epsilon_i\sim\mathcal{N}(0,\tau^{-1}) \qquad  i=2,\dots,n
\end{eqnarray}
where
\[
|\rho|<1
\]
Thus, the model can be written in hierarchical form as follows.
\begin{description}
  \item[Stage 1:]
    \[
      y_i|\eta_i \sim \mathcal{N}(\eta_i, \sigma_\mathrm{o}^2)
    \]
  \item[Stage 2:]
    Covariates and AR(1) component  are connected to likelihood by
    \[
      \eta_i = \beta_0 + \beta_1 x_i + a_i
    \]
  \item[Stage 3:]
  \begin{itemize}
	\item
	  \(\sigma_\mathrm{o}^2\)\,: variance of observation noise
	\item
	  \(\rho\)\,: dependence in AR(1) process
	\item
	  \(\tau^{-1}\)\,: variance of the innovations in AR(1) process
  \end{itemize}
\end{description}

Using a simulated dataset we show how to estimate this model in \tv{R-INLA}
<<setup, include=FALSE, cache=FALSE, tidy=TRUE>>=
options(tidy=TRUE, width=70)
@

<<>>=
set.seed(580258)
n = 100
t = 1:n
ar = rep(0 ,n)

# Generate the autoregressive component
for(i in 2:n)
  ar[i] = 0.8 * ar[i -1] + rnorm(n = 1, sd = 0.1)

# Simulate data with AR (1) component
x = runif(n)
y = 1 + 2 * x + ar + rnorm(n = n , sd = 0.1)

# Specify the model and run inla to estimate the parameters back
formula = y ~ 1 + x + f(t , model = "ar1" )
result = inla(formula, data = list(x = x, y = y, t = t), family ="gaussian")
@

Using the \tv{summary()} function we can inspect the parameter estimates. Of note
in the summary only estimates for the fixed effects here $\beta_0$ and
$\beta_1$ as well as the three hyperparameters are given.The posterior
mean for the intercept and regression coefficient are
$\Sexpr{format(result$summary.fixed$mean[1],digits=2,nsmall=2)}$  (SD:
$\Sexpr{format(result$summary.fixed$sd[1],digits=2,nsmall=2)}$)
and $\Sexpr{format(result$summary.fixed$mean[2],digits=2,nsmall=2)}$ (SD:
$\Sexpr{format(result$summary.fixed$sd[2],digits=2,nsmall=2)}$),
respectively, and therefore  well estimated. The posterior mean and
standard deviation for the  autocorrelation parameter $\rho$ are
$\Sexpr{format(result$summary.hyperpar$mean[3],digits=2,nsmall=2)}$  and
$\Sexpr{format(result$summary.hyperpar$sd[3],digits=2,nsmall=2)}$,
respectively. Estimates will be closer to the simulated values of
$0.8$ when increasing the number of observations. Section~TODO  disusses
how to extract output information for the random effects.
<<>>=
summary(result)
@
To  reduce the number of unique values of a covariate the
helper function {\tt inla.group()} can be used. This might help to reduce
numerical instabilities. For example,
<<eval=FALSE>>=
  inla.group(c, n=20)
@
groups the covariate values into 20 classes. For more information
type {\tt ?inla.group} within {\tt R}. The resulting {\tt f()}-function
changes to
<<eval=FALSE>>=
  f(inla.group(c, n=20), w, model="...")
@
% To define all values assumed by a covariate and
% for which an effect should be estimated,  the optional field
% {\tt values} can be used. Here, a numerical vector, a vector of
% factors or ‘NULL’ can be set:
% <<eval=FALSE>>=
%   f(c, w, model="...", values=<values assumed by the covariate>)
% @
Besides the type of latent model some general options can be specified.
All available options are
listed on the help page of the {\tt f()}-function, found via {\tt ?f}.
Some important options are

\begin{itemize}
        \item[{\tt constr}:] A boolean variable indicating whether to
        set a sum to zero constraint. By default the sum to zero constraint is
        imposed on all intrinsic models (``iid'', ``rw1'', ``rw2'', ``besag'',
        etc..). That means to all models where the corresponding
        precision matrix has not full rank.
        \item[{\tt extraconstr}] This argument defines extra linear
        constraints. The argument is a list with two elements, a matrix
        {\tt A} and a
        vector {\tt e}, which defines the extra constraint {\tt Ax =
          e} for random effect {\tt x};
        leading to {\tt extraconstr = list(A=A, e=e)}. The number of
        columns of {\tt A} must correspond to the length of this
        f-model.


        For example, assume that the random effect $j$ has length
        five and is modelled as an RW1.
        In addition to the default sum-to-zero constraing we would
        like to have two
        extra constraints, the
        first saying that the first two effects should be equal and the
        second that the last two
        elements are equal. Thus we have
<<eval=FALSE>>=
  A = matrix(1,-1,0,0,0,
             0,0,0,1,-1), nrow=2, ncol=5, byrow=TRUE)
  e = c(0,0)
  extra = list(A=A, e=e)
  formula = y ~ f(j, model="rw1", constr=TRUE, extraconstr=extra) + ....
@
         Note that this
        constraint comes additional to the
        sum-to-zero constraint defined if {\tt constr = TRUE}.
        \item[{\tt quantile}:]A vector of maximum $10$ quantiles
        to compute for each posterior marginal.
        \item[{\tt rankdef}:] A number *defining* the rank deficiency of
                the latent model.
        \item[{\tt cyclic}:]A boolean specifying wheather the model is cyclical. Only
          valid for "rw1" and "rw2" models. If cyclic=T then the sum to
          0 constraint is removed.
\end{itemize}
The following section illustrates the specification of hyperparameter settings for
a random effect to be defined within the \tv{f}-function.

\section{Define hyperpriors for random effects}

\subsection{General structure}

Each latent model depends on a certain number of hyperparameters,
e.g.~the {\tt iid} model has one hyperparameter namely the precision,
for which a prior distribution
needs to be specified.

Hyperparameters are internally often not represented in its original scale
but using a more well defined transforamtion. For example, a
precison parameter $\tau$ and a correlation parameter $\rho$ are
internally represented as
\begin{align*}
  \theta_1 &= \log(\tau)\\
  \theta_2 &= \log \Biggl( \frac{1+\rho}{1-\rho}\Biggr)
\end{align*}
That means that prior distributions as well as starting values for the
numerical optimisation must be specified on the corresponding internal
scale. Of note, posterior marginals and summary estimates are
returned on the original scale.

To set the details for the hyperparameters the field {\tt hyper} of
the {\tt f()} function is used, which takes as argument a list. This
list has as many entries as hyperparameters exist for the latent model.
For each hyperparameter again a list containing the prior settings is
defined.
The prior settings for a precision parameter, in case of the {\tt rw2}
model for example, are set via:
<<eval=FALSE>>=
 hyper=list(
   prec=list(
     prior="loggamma",
     param=c(1, 1e-04),
     initial=4,
     fixed=FALSE,
     to.theta = function(x) log(x),
     from.theta = function(x) exp(x)
   )
 )
@
{\tt prec} in line 2 is the short name identifier to indicate that
the settings specified in the subsequent list belong to the precision
parameter. Internally the precision parameter is log-transformed and
all prior settings are defined for this transformed parameter.
The transformation used can be seen in lines 7 and 8.
Here, we use a log-gamma distribution  with parameters $1$ and
$1e-04$ (lines 3 and 4). It is also possible to fix the hyperparameter setting
the field {\tt fixed = TRUE} in line 6, i.e.~to not estimate this parameter.
The value to which it should be fixed
is assigned to {\tt initial} in line 5. If {\tt fixed=FALSE} the value
set in {\tt initial} is used as starting value in the numerical
optimisation for finding the mode of $\bm{\theta} \mid \bm{y}$.

For a latent model with more than one hyperparameter several of prior lists
can be specified in {\tt hyper}. For example for an AR1 model, which
has two hyperparameters,  the corresponding specification would be:
<<eval=FALSE>>=
 hyper=list(
   prec=list(
     prior="loggamma",
     param=c(1, 1e-04),
     initial=4,
     fixed=FALSE,
     to.theta = function(x) log(x),
     from.theta = function(x) exp(x)
   ),
   rho=list(
     prior = "normal",
     param = c(0, 0.15),
     initial = 2,
     fixed = FALSE,
     to.theta = function(x) log((1+x)/(1-x)),
     from.theta = function(x) 2*exp(x)/(1+exp(x))-1
   )
 )
@
Here, a log-gamma distribution with parameters  $1$ and $1e-04$ is assigned
to the log-precision parameter and a normal distribution with mean $0$
and precision $0.15$ is assigned to the logit-transformed lag one correlation
parameter $\rho$.

The details on the hyperparameters  (number, identifier names, \ldots)
% and default values of all latent models can be requested within {\tt R}
% using the function {\tt inla.model.properties(<identifier>, "latent")}.
are given in the documentary pdf file of the latent model or
listed when typing {\tt ?inla.models}.


\subsection{List of available priors}

Provide table about what it exists separated by type of parameter:
precision parameter, correlation, covariance matrix, ...

\subsection{User-defined priors}

Possible using table and expression.

\subsection{Model scaling}

Write about Sigrunns and your paper.


\section{Define the observation model and run the model}

\subsection{The inla-call}
What can be specified.

\subsection{Data structure and available likelihoods}
\begin{itemize}
\item Say something about how data need to be organised what needs
to be provided.

\item Table about likelihoods. Comment that likelihoods cannot be
defined manually.

\item Mention how to set priors for hyperparameters in the likelihood.
\end{itemize}

\subsection{Control arguments}
Write about
<<eval=FALSE>>=
?control.fixed
?control.predictor
?control.inla
?control.compute
?control.results
@

\subsection{Multiple likelihoods}


\section{Improved hyperparameter specification}

Write about
<<eval=FALSE>>=
inla.hyperpar
@
